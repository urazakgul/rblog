[
  {
    "path": "posts/2022-09-27-post34/",
    "title": "Close-to-Close vs High-Low (Parkinson) Volatility",
    "description": "Close-to-Close and High-Low (Parkinson) volatility types are compared.",
    "author": [
      {
        "name": "Uraz Akgül",
        "url": {}
      }
    ],
    "date": "2022-09-27",
    "categories": [
      "Finance"
    ],
    "contents": "\r\nVolatility can be measured in a variety of ways, and in this post I’d\r\nlike to compare two of them.\r\nThe formulas for the two volatility methods mentioned in the title\r\nare shown below.\r\nClose-to-Close Volatility:\r\nClose-to-close historical volatility is calculated using only the\r\nclosing prices of stocks.\r\n\\(\\sigma_{cl} =\r\n\\sqrt{\\frac{1}{T-1}\\sum_{t=1}^{T}(r_t-\\bar{r})^2}\\)\r\nwhere:\r\n\\(T:\\) Number of days in the sample\r\nperiod\r\n\\(r_t:\\) Return on day t\r\n\\(\\bar{r}:\\) Mean return\r\nHigh-Low or Parkinson Volatility:\r\nParkinson volatility is a measure of volatility that takes into\r\naccount the stock’s high and low prices for the day.\r\n\\(\\sigma_{hl} =\r\n\\sqrt{\\frac{1}{4Tln2}\\sum_{t=1}^{T}ln(\\frac{H_t}{L_t})^2}\\)\r\nwhere:\r\n\\(T:\\) Number of days in the sample\r\nperiod\r\n\\(H_t:\\) High price on day t\r\n\\(L_t:\\) Low price on day t\r\nWe’ll use two methods to calculate the volatility of the stocks in\r\nthe BIST30 index. The stock list is available on PDP, which stands for\r\nPublic Disclosure Platform. The data were obtained from Reuters, which\r\nyou can access by downloading the post34.xlsx file from here.\r\n\r\n\r\nlibrary(tidyverse)\r\n\r\n\r\n\r\n\r\n\r\ndf <- readxl::read_excel(\"data.xlsx\")\r\n\r\nsampleSize <- nrow(df)\r\nstockList <- unique(gsub(\"_Open|_High|_Low|_Close\",\"\",names(df)))[-1]\r\n\r\n\r\n\r\nClose-to-Close Volatility:\r\n\r\n\r\nclose_to_close <- data.frame()\r\n\r\nfor(i in 1:31){\r\n  \r\n  cl <- df %>% \r\n    select(contains(paste0(stockList[i],\"_Close\"))) %>% \r\n    rename(\"Close\"=1) %>% \r\n    mutate(\r\n      Return = lag(log(lead(Close) / Close)),\r\n      Mean = mean(Return, na.rm = TRUE),\r\n      ReturnMean2 = (Return - Mean)^2\r\n    ) %>% \r\n    pull(ReturnMean2) %>% \r\n    sum(., na.rm = TRUE) / (sampleSize - 2) %>% # 31.12.2021 is not included & T - 1 = T - 2\r\n    sqrt(.) * 100\r\n  \r\n  tbl_cl <- data.frame(\r\n    \"Ticker\" = stockList[i],\r\n    \"ClosetoCloseVol\" = cl\r\n  )\r\n  \r\n  close_to_close <- close_to_close %>% \r\n    bind_rows(tbl_cl)\r\n    \r\n}\r\n\r\n\r\n\r\nHigh-Low or Parkinson Volatility:\r\n\r\n\r\nhigh_low <- data.frame()\r\n\r\nfor(j in 1:31){\r\n  \r\n  hl <- df %>% \r\n    select(contains(c(paste0(stockList[j],\"_High\"),paste0(stockList[j],\"_Low\")))) %>% \r\n    rename(\"High\"=1, \"Low\"=2) %>% \r\n    mutate(HL = log(High/Low)^2) %>% \r\n    pull(HL) %>% \r\n    sum(.) / (4 * (sampleSize - 1) * log(2)) %>% # 31.12.2021 is not included = T - 1\r\n    sqrt(.) * 100\r\n  \r\n  tbl <- data.frame(\r\n    \"Ticker\"= stockList[j],\r\n    \"HighLowVol\" = hl\r\n  )\r\n  \r\n  high_low <- high_low %>% \r\n    bind_rows(tbl)\r\n  \r\n}\r\n\r\n\r\n\r\n\r\n\r\nmaster <- close_to_close %>% \r\n  left_join(high_low, by = \"Ticker\")\r\n\r\n\r\n\r\n\r\n\r\nggplot(master, aes(x = ClosetoCloseVol, y = HighLowVol)) +\r\n  geom_point(size = 5, alpha = .3) + \r\n  geom_point(data = master %>% filter(Ticker == \"XU030\"), color = \"red\", size = 5) +\r\n  ggrepel::geom_text_repel(aes(label = Ticker), size = 4, nudge_x = 0.05) +\r\n  hrbrthemes::theme_ipsum() +\r\n  labs(\r\n    title = \"The Most and Least Volatile Stocks Among the BIST30 (XU030)\",\r\n    x = \"Close-to-Close Volatility\",\r\n    y = \"High and Low or Parkinson Volatility\"\r\n  )\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-09-27-post34/post34_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2022-09-27T21:48:09+03:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-09-25-post33/",
    "title": "Does History Repeat Itself in the Financial Markets?",
    "description": "Hierarchical clustering based on DTW and markov chains were used.",
    "author": [
      {
        "name": "Uraz Akgül",
        "url": {}
      }
    ],
    "date": "2022-09-25",
    "categories": [
      "Finance",
      "Machine Learning"
    ],
    "contents": "\r\nEveryone has an opinion on whether history repeats itself, with some\r\nclaiming that it does and others claiming that it does not. I agree with\r\nthose who believe history repeats itself. This study’s claim will be\r\nrevealed at the end of this post.\r\nI’d like to inform you about how the study will progress step by\r\nstep.\r\nFirst things first, it’ll be decided what the financial series will\r\nbe. I chose to use data from the Borsa Istanbul 100 index for this\r\nstudy. We’ll divide the series into the number of slices we determined\r\nin the second step once we know what the data will be. After dividing\r\nthe series, we’ll use the hierarchical clustering method based on\r\nDynamic Time Warping to find similar series. More information about DTW\r\ncan be found in my\r\nprevious post. Following the discovery of similar series, the\r\nmovements of these series in a given time interval will be examined. We\r\nexpect that the series will behave similarly in the given time interval.\r\nBecause, well, why not?\r\nThe data were obtained from CBRT’s website, which you can access by\r\ndownloading the post33.xlsx file from here.\r\n\r\n\r\n\r\n\r\n\r\n\r\nThe dataset contains 4943 rows.\r\n\r\n\r\nt\r\n\r\n\r\ndate\r\n\r\n\r\nclose\r\n\r\n\r\n4934\r\n\r\n\r\n2022-09-09\r\n\r\n\r\n3521.38\r\n\r\n\r\n4935\r\n\r\n\r\n2022-09-12\r\n\r\n\r\n3649.21\r\n\r\n\r\n4936\r\n\r\n\r\n2022-09-13\r\n\r\n\r\n3426.88\r\n\r\n\r\n4937\r\n\r\n\r\n2022-09-14\r\n\r\n\r\n3446.96\r\n\r\n\r\n4938\r\n\r\n\r\n2022-09-15\r\n\r\n\r\n3363.12\r\n\r\n\r\n4939\r\n\r\n\r\n2022-09-16\r\n\r\n\r\n3377.33\r\n\r\n\r\n4940\r\n\r\n\r\n2022-09-19\r\n\r\n\r\n3199.54\r\n\r\n\r\n4941\r\n\r\n\r\n2022-09-20\r\n\r\n\r\n3277.50\r\n\r\n\r\n4942\r\n\r\n\r\n2022-09-21\r\n\r\n\r\n3245.76\r\n\r\n\r\n4943\r\n\r\n\r\n2022-09-22\r\n\r\n\r\n3295.27\r\n\r\n\r\n\r\n\r\n\r\nThe first 4900 days of the current series will be used.\r\n\r\n\r\n\r\n\r\n\r\nt\r\n\r\n\r\ndate\r\n\r\n\r\nclose\r\n\r\n\r\n4891\r\n\r\n\r\n2022-07-06\r\n\r\n\r\n2408.15\r\n\r\n\r\n4892\r\n\r\n\r\n2022-07-07\r\n\r\n\r\n2425.56\r\n\r\n\r\n4893\r\n\r\n\r\n2022-07-08\r\n\r\n\r\n2434.02\r\n\r\n\r\n4894\r\n\r\n\r\n2022-07-13\r\n\r\n\r\n2408.08\r\n\r\n\r\n4895\r\n\r\n\r\n2022-07-14\r\n\r\n\r\n2382.44\r\n\r\n\r\n4896\r\n\r\n\r\n2022-07-18\r\n\r\n\r\n2451.27\r\n\r\n\r\n4897\r\n\r\n\r\n2022-07-19\r\n\r\n\r\n2501.96\r\n\r\n\r\n4898\r\n\r\n\r\n2022-07-20\r\n\r\n\r\n2525.20\r\n\r\n\r\n4899\r\n\r\n\r\n2022-07-21\r\n\r\n\r\n2511.37\r\n\r\n\r\n4900\r\n\r\n\r\n2022-07-22\r\n\r\n\r\n2516.56\r\n\r\n\r\nThe number of sub-series into which we’ll divide the series is\r\ncritical. If we choose a very long time interval, there may be issues\r\nwith similarity; if we choose a very short time interval, it may be time\r\nconsuming and we may not get the desired movements. The optimal number\r\ncan be determined later, but for now, I’d like to set it to 50. When the\r\nnumber is set to 50, there will be 98 sub-series.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nt\r\n\r\n\r\ndate\r\n\r\n\r\nclose\r\n\r\n\r\ngroup\r\n\r\n\r\n1\r\n\r\n\r\n2003-01-02\r\n\r\n\r\n105.9858\r\n\r\n\r\nG1\r\n\r\n\r\n2\r\n\r\n\r\n2003-01-03\r\n\r\n\r\n108.3753\r\n\r\n\r\nG1\r\n\r\n\r\n3\r\n\r\n\r\n2003-01-06\r\n\r\n\r\n103.5734\r\n\r\n\r\nG1\r\n\r\n\r\n4\r\n\r\n\r\n2003-01-07\r\n\r\n\r\n97.5286\r\n\r\n\r\nG1\r\n\r\n\r\n5\r\n\r\n\r\n2003-01-08\r\n\r\n\r\n101.6121\r\n\r\n\r\nG1\r\n\r\n\r\n4895\r\n\r\n\r\n2022-07-14\r\n\r\n\r\n2382.4400\r\n\r\n\r\nG98\r\n\r\n\r\n4896\r\n\r\n\r\n2022-07-18\r\n\r\n\r\n2451.2700\r\n\r\n\r\nG98\r\n\r\n\r\n4897\r\n\r\n\r\n2022-07-19\r\n\r\n\r\n2501.9600\r\n\r\n\r\nG98\r\n\r\n\r\n4898\r\n\r\n\r\n2022-07-20\r\n\r\n\r\n2525.2000\r\n\r\n\r\nG98\r\n\r\n\r\n4899\r\n\r\n\r\n2022-07-21\r\n\r\n\r\n2511.3700\r\n\r\n\r\nG98\r\n\r\n\r\n4900\r\n\r\n\r\n2022-07-22\r\n\r\n\r\n2516.5600\r\n\r\n\r\nG98\r\n\r\n\r\nIt’d be better if we normalize the data.\r\n\r\n\r\n\r\nThe first 10 normalized groups are displayed below.\r\n\r\n\r\nt\r\n\r\n\r\nG1\r\n\r\n\r\nG2\r\n\r\n\r\nG3\r\n\r\n\r\nG4\r\n\r\n\r\nG5\r\n\r\n\r\nG6\r\n\r\n\r\nG7\r\n\r\n\r\nG8\r\n\r\n\r\nG9\r\n\r\n\r\nG10\r\n\r\n\r\n1\r\n\r\n\r\n0.4866416\r\n\r\n\r\n0.2871891\r\n\r\n\r\n1.0000000\r\n\r\n\r\n0.0601604\r\n\r\n\r\n0.1547002\r\n\r\n\r\n0.7574900\r\n\r\n\r\n0.8913942\r\n\r\n\r\n0.3243712\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.0000000\r\n\r\n\r\n2\r\n\r\n\r\n0.5908696\r\n\r\n\r\n0.1963512\r\n\r\n\r\n0.8103680\r\n\r\n\r\n0.0833915\r\n\r\n\r\n0.2008063\r\n\r\n\r\n0.9194708\r\n\r\n\r\n0.9165304\r\n\r\n\r\n0.4303481\r\n\r\n\r\n0.0014956\r\n\r\n\r\n0.0663438\r\n\r\n\r\n3\r\n\r\n\r\n0.3814147\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.5970524\r\n\r\n\r\n0.0966980\r\n\r\n\r\n0.1433398\r\n\r\n\r\n0.8052858\r\n\r\n\r\n0.9897514\r\n\r\n\r\n0.2890309\r\n\r\n\r\n0.1536198\r\n\r\n\r\n0.1860019\r\n\r\n\r\n4\r\n\r\n\r\n0.1177456\r\n\r\n\r\n0.1127994\r\n\r\n\r\n0.6337714\r\n\r\n\r\n0.0679026\r\n\r\n\r\n0.0515875\r\n\r\n\r\n0.7746797\r\n\r\n\r\n1.0000000\r\n\r\n\r\n0.2929387\r\n\r\n\r\n0.1071586\r\n\r\n\r\n0.2776303\r\n\r\n\r\n5\r\n\r\n\r\n0.2958645\r\n\r\n\r\n0.1970771\r\n\r\n\r\n0.5063833\r\n\r\n\r\n0.0875143\r\n\r\n\r\n0.0587623\r\n\r\n\r\n0.5753909\r\n\r\n\r\n0.8275195\r\n\r\n\r\n0.3509452\r\n\r\n\r\n0.0877059\r\n\r\n\r\n0.2685981\r\n\r\n\r\nWe can now proceed to hierarchical clustering based on DTW. The\r\nsub-series will be divided into 4 classes.\r\n\r\n\r\n\r\n\r\n\r\n\r\nLet’s look at the transition probabilities between clusters.\r\n\r\n\r\n\r\n\r\n\r\n.\r\n\r\n\r\n1\r\n\r\n\r\n2\r\n\r\n\r\n3\r\n\r\n\r\n4\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n5\r\n\r\n\r\n3\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n6\r\n\r\n\r\n20\r\n\r\n\r\n6\r\n\r\n\r\n13\r\n\r\n\r\n3\r\n\r\n\r\n0\r\n\r\n\r\n6\r\n\r\n\r\n2\r\n\r\n\r\n8\r\n\r\n\r\n4\r\n\r\n\r\n2\r\n\r\n\r\n14\r\n\r\n\r\n5\r\n\r\n\r\n6\r\n\r\n\r\nWe’ll use the table above, but right now we need to focus on the last\r\nsub-series, G98.\r\n\r\n\r\ngroup\r\n\r\n\r\ncluster\r\n\r\n\r\nG98\r\n\r\n\r\n1\r\n\r\n\r\n\r\n\r\n\r\nThe sub-series G98 is in cluster 1, which indicates that the\r\nfollowing probabilities will hold true.\r\nThe probability of transitioning from cluster 1 to cluster 1 is\r\n10%\r\nThe probability of transitioning from cluster 1 to cluster 2 is\r\n50%\r\nThe probability of transitioning from cluster 1 to cluster 3 is\r\n30%\r\nThe probability of transitioning from cluster 1 to cluster 4 is\r\n0%\r\nWe then normalize the remaining 43 days that we excluded at the\r\nbeginning of the study. The averages of the groups divided into 4\r\nclusters over normalized values are shown below, where we can see four\r\ndifferent patterns.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nWe can see that the 50% chance of moving from cluster 1 to cluster 2\r\nhas occurred. So, what will the next 50-day movement look like?\r\nThe probability of transitioning from cluster 2 to cluster 1 is\r\n13%\r\nThe probability of transitioning from cluster 2 to cluster 2 is\r\n43%\r\nThe probability of transitioning from cluster 2 to cluster 3 is\r\n13%\r\nThe probability of transitioning from cluster 2 to cluster 4 is\r\n28%\r\nWe can expect that the BIST100 will continue to rise with a 43%\r\nprobability until mid-December.\r\nThe codes used in the study can be found below.\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(dtwclust)\r\nlibrary(markovchain)\r\n\r\ndf <- readxl::read_excel(\"data.xlsx\") %>% \r\n  mutate(date = lubridate::dmy(date),\r\n         t = seq(1,nrow(.),1), .before = date)\r\n\r\nggplot(df, aes(x = date, y = close)) +\r\n  geom_line() +\r\n  ggthemes::theme_fivethirtyeight() +\r\n  theme(axis.text = element_text(size = 20),\r\n        plot.title = element_text(size = 25, hjust = 0.5)) +\r\n  scale_y_continuous(labels = scales::comma) +\r\n  labs(title = \"BIST100\")\r\n\r\nbist100 <- df %>% \r\n  filter(t %in% 1:4900)\r\n\r\nggplot(bist100, aes(x = t, y = close)) +\r\n  geom_line() +\r\n  geom_vline(xintercept = seq(1,4900,50), linetype = \"dotdash\") +\r\n  ggthemes::theme_fivethirtyeight() +\r\n  theme(axis.text = element_text(size = 20),\r\n        plot.title = element_text(size = 25, hjust = 0.5)) +\r\n  scale_y_continuous(labels = scales::comma) +\r\n  labs(title = \"BIST100\")\r\n\r\nbist100 <- bist100 %>% \r\n  mutate(\r\n    group = paste0(\"G\",rep(1:98, each = 50))\r\n  )\r\n\r\nnormalized <- function(x,...){\r\n  \r\n  (x - min(x,...)) / (max(x,...) - min(x,...))\r\n  \r\n}\r\n\r\nmaster <- bist100 %>% \r\n  select(close,group) %>% \r\n  group_by(group) %>% \r\n  mutate(t = row_number()) %>% \r\n  ungroup() %>% \r\n  pivot_wider(names_from = \"group\", values_from = \"close\") %>% \r\n  mutate_at(\r\n    vars(-t), function(x) normalized(x, na.rm = TRUE)\r\n  )\r\n\r\nk <- 4L\r\n\r\ndata_cluster <- tsclust(\r\n  t(master[,-1]),\r\n  type = \"h\",\r\n  k = k,\r\n  distance = \"dtw\"\r\n)\r\n\r\ncluster <- as.data.frame(cutree(data_cluster, k=k)) %>% \r\n  rownames_to_column(., var = \"group\") %>% \r\n  rename(\"cluster\"=2)\r\n\r\nmaster2 <- bist100 %>% \r\n  arrange(group) %>% \r\n  left_join(cluster, by = \"group\") %>% \r\n  group_by(group) %>% \r\n  mutate(n = row_number()) %>% \r\n  mutate_at(vars(close), function(x) normalized(x, na.rm = TRUE)) %>% \r\n  ungroup()\r\n\r\nfor(i in 1:k){\r\n  \r\n  g <- ggplot(master2 %>% filter(cluster == i), aes(x = n, y = close)) +\r\n    geom_line(data = master2 %>% filter(cluster == i) %>% rename(group2 = group), aes(group = group2), color = \"gray\", size = 1) +\r\n    geom_line(color = \"dark blue\", size = 1) +\r\n    ggthemes::theme_fivethirtyeight() +\r\n    theme(strip.text = element_text(size = 20),\r\n          axis.text = element_blank()) +\r\n    facet_wrap(~group, scales = \"free\")\r\n  \r\n  plot(g)\r\n  \r\n}\r\n\r\nprobs <- as.data.frame(createSequenceMatrix(cluster$cluster)) %>% \r\n  rowid_to_column(\".\")\r\n\r\ng98 <- cluster %>% \r\n  filter(group == \"G98\") %>% \r\n  pull(cluster)\r\n\r\ndf43 <- df %>% \r\n  slice(4901:nrow(.)) %>% \r\n  mutate_at(vars(close), function(x) normalized(x, na.rm = TRUE)) %>% \r\n  mutate(n = seq(51,93,1), .before = date) %>% \r\n  select(-date)\r\n\r\npred <- master2 %>% \r\n  select(n,group,cluster,close) %>% \r\n  group_by(n,cluster) %>% \r\n  summarise(close_mean = mean(close)) %>% \r\n  ungroup() %>% \r\n  arrange(cluster)\r\n\r\nggplot(pred, aes(x = n, y = close_mean)) +\r\n  geom_line(size = 1) +\r\n  facet_wrap(~cluster) +\r\n  ggthemes::theme_fivethirtyeight()\r\n\r\ncluster_g98 <- pred %>% \r\n  filter(cluster == g98) %>% \r\n  select(n,close_mean) %>% \r\n  rename(\"close\"=2) %>% \r\n  bind_rows(df43)\r\n\r\nggplot(cluster_g98, aes(x = n, y = close)) +\r\n  geom_line() +\r\n  geom_vline(xintercept = 50, linetype = \"dotdash\", size = 1) +\r\n  scale_color_manual(values = c(\"red\",\"blue\")) +\r\n  ggthemes::theme_fivethirtyeight() +\r\n  theme(legend.title = element_blank(),\r\n        legend.position = \"top\")\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-09-25-post33/post33_files/figure-html5/unnamed-chunk-19-1.png",
    "last_modified": "2022-09-25T23:52:50+03:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-09-21-post32/",
    "title": "A Look at Stock Market Performance After Elections: The Case of BIST100 Indices",
    "description": "Stock market performance by elections.",
    "author": [
      {
        "name": "Uraz Akgül",
        "url": {}
      }
    ],
    "date": "2022-09-21",
    "categories": [
      "Finance"
    ],
    "contents": "\r\nTurkey held general elections on the following dates (not included\r\nbefore 2002):\r\n\r\n\r\ndate\r\n\r\n\r\nelection\r\n\r\n\r\nNov 3, 2002\r\n\r\n\r\nGeneral\r\n\r\n\r\nJul 22, 2007\r\n\r\n\r\nGeneral\r\n\r\n\r\nJun 12, 2011\r\n\r\n\r\nGeneral\r\n\r\n\r\nJun 07, 2015\r\n\r\n\r\nGeneral\r\n\r\n\r\nNov 01, 2015\r\n\r\n\r\nGeneral\r\n\r\n\r\nJun 24, 2018\r\n\r\n\r\nGeneral\r\n\r\n\r\nSince the AKP has been ruling Turkey since 2002, this work will\r\nactually be the government’s performance.\r\nYou can download the post32_1.xlsx and\r\npost32_2.xlsx files here or go\r\nto the CBRT’s\r\nwebsite.\r\n\r\n\r\nlibrary(tidyverse)\r\n\r\n\r\n\r\n\r\n\r\nbist100 <- readxl::read_excel(\"data1.xlsx\") %>% \r\n  mutate(date = lubridate::dmy(date))\r\n\r\n\r\n\r\n\r\n\r\nelection_dates <- c(\"2002-11-03\",\r\n                    \"2007-07-22\",\r\n                    \"2011-06-12\",\r\n                    \"2015-06-07\",\r\n                    \"2015-11-01\",\r\n                    \"2018-06-24\")\r\n\r\nggplot(bist100, aes(x = date, y = close)) +\r\n  geom_line() +\r\n  geom_vline(xintercept = as.Date(election_dates), linetype = \"dotdash\", color = \"red\", size = 1) +\r\n  ggthemes::theme_fivethirtyeight() +\r\n  theme(axis.text = element_text(size = 20)) +\r\n  scale_y_continuous(labels = scales::comma)\r\n\r\n\r\n\r\n\r\nWe’ll create indices starting with 100 using each election date as a\r\nstart date.\r\n\r\n\r\n# Groups\r\n\r\nbist100 <- bist100 %>% \r\n  mutate(\r\n    election = case_when(\r\n      date >= as.Date(\"2002-11-03\") & date < as.Date(\"2007-07-22\") ~ \"GE-1 (Nov 3, 2002)\",\r\n      date >= as.Date(\"2007-07-22\") & date < as.Date(\"2011-06-12\") ~ \"GE-2 (Jul 22, 2007)\",\r\n      date >= as.Date(\"2011-06-12\") & date < as.Date(\"2015-06-07\") ~ \"GE-3 (Jun 12, 2011)\",\r\n      date >= as.Date(\"2015-06-07\") & date < as.Date(\"2015-11-01\") ~ \"GE-4 (Jun 07, 2015)\",\r\n      date >= as.Date(\"2015-11-01\") & date < as.Date(\"2018-06-24\") ~ \"GE-5 (Nov 01, 2015)\",\r\n      date >= as.Date(\"2018-06-24\") ~ \"GE-6 (Jun 24, 2018)\"\r\n    )\r\n  )\r\n\r\n\r\n\r\n\r\n\r\n# Initial Values\r\n\r\ninitialValues <- bist100 %>% \r\n  group_by(election) %>% \r\n  slice_min(date) %>% \r\n  rename(\"initial\"=2) %>% \r\n  ungroup()\r\n\r\n\r\n\r\n\r\n\r\n# Indices\r\n\r\nmaster <- bist100 %>% \r\n  left_join(initialValues, by = c(\"date\",\"election\")) %>% \r\n  mutate(initial = zoo::na.locf(initial), # Replace NAs with previous value\r\n         \"indices\" = close / initial * 100) %>% \r\n  group_by(election) %>% \r\n  mutate(t = row_number()) %>% \r\n  ungroup()\r\n\r\n\r\n\r\nDone!\r\n\r\n\r\nggplot(master, aes(x = t, y = indices)) +\r\n  geom_line(data = master %>% rename(election2 = election),\r\n            aes(group = election2), color = \"gray\", size = 1) +\r\n  geom_line(color = \"dark blue\", size = 1.5) +\r\n  ggthemes::theme_fivethirtyeight() +\r\n  theme(strip.text = element_text(size = 20),\r\n        axis.text = element_text(size = 20)) +\r\n  facet_wrap(~election, ncol = 2)\r\n\r\n\r\n\r\n\r\nLet’s calculate the returns on a group basis.\r\n\r\n\r\nperform100 <- master %>% \r\n  group_by(election) %>% \r\n  filter(date == min(date) | date == max(date)) %>% \r\n  mutate(\r\n    return = log(1 + (lead(indices) - indices) / indices) * 100\r\n  ) %>% \r\n  ungroup() %>% \r\n  select(election,return) %>% \r\n  na.omit()\r\n\r\n\r\n\r\n\r\n\r\nelection\r\n\r\n\r\nreturn\r\n\r\n\r\nGE-1 (Nov 3, 2002)\r\n\r\n\r\n158.5\r\n\r\n\r\nGE-2 (Jul 22, 2007)\r\n\r\n\r\n13.6\r\n\r\n\r\nGE-3 (Jun 12, 2011)\r\n\r\n\r\n25.5\r\n\r\n\r\nGE-4 (Jun 07, 2015)\r\n\r\n\r\n2.0\r\n\r\n\r\nGE-5 (Nov 01, 2015)\r\n\r\n\r\n13.6\r\n\r\n\r\nGE-6 (Jun 24, 2018)\r\n\r\n\r\n124.9\r\n\r\n\r\nThe best performance was demonstrated between the elections in 2002\r\nand 2007 as seen in the table above. The period following the 2018\r\ngeneral elections has the second best performance. Yay!\r\nIt’s too soon to celebrate! See below for the CPI values during the\r\ntime of the ruling party!\r\n\r\n\r\ncpi <- readxl::read_excel(\"data2.xlsx\") %>% \r\n  mutate(\r\n    date = lubridate::ymd(paste0(date,\"-\",1))\r\n  )\r\n\r\n\r\n\r\n\r\n\r\nggplot(cpi, aes(x = date, y = cpi)) +\r\n  geom_line() +\r\n  geom_vline(xintercept = as.Date(election_dates), linetype = \"dotdash\", color = \"red\", size = 1) +\r\n  ggthemes::theme_fivethirtyeight() +\r\n  theme(axis.text = element_text(size = 20)) +\r\n  scale_y_continuous(labels = scales::comma)\r\n\r\n\r\n\r\n\r\nBecause CPI values are published on a monthly basis, we must convert\r\nthe stock market index to a monthly basis. The following formulas can be\r\nused to perform calculations.\r\n\\(P_t^{real} = P_t / CPI_t\\)\r\n\\(r_t^{real} = ln(1 +\r\nR_t^{real})\\)\r\n\\(R_t^{real} = (P_t^{real} -\r\nP_{t-1}^{real}) / P_{t-1}^{real}\\)\r\n\r\n\r\nbist100_monthly <- readxl::read_excel(\"data1.xlsx\") %>% \r\n  mutate(date = lubridate::dmy(date),\r\n         year = lubridate::year(date),\r\n         month = lubridate::month(date)) %>% \r\n  group_by(year,month) %>% \r\n  summarise(close = mean(close)) %>% \r\n  ungroup() %>% \r\n  mutate(date = as.Date(paste0(year,\"-\",month,\"-\",1))) %>% \r\n  select(date,close) %>% \r\n  mutate(\r\n    election = case_when(\r\n      date >= as.Date(\"2002-11-01\") & date < as.Date(\"2007-07-01\") ~ \"GE-1 (Nov 3, 2002)\",\r\n      date >= as.Date(\"2007-07-01\") & date < as.Date(\"2011-06-01\") ~ \"GE-2 (Jul 22, 2007)\",\r\n      date >= as.Date(\"2011-06-01\") & date < as.Date(\"2015-06-01\") ~ \"GE-3 (Jun 12, 2011)\",\r\n      date >= as.Date(\"2015-06-01\") & date < as.Date(\"2015-11-01\") ~ \"GE-4 (Jun 07, 2015)\",\r\n      date >= as.Date(\"2015-11-01\") & date < as.Date(\"2018-06-01\") ~ \"GE-5 (Nov 01, 2015)\",\r\n      date >= as.Date(\"2018-06-01\") ~ \"GE-6 (Jun 24, 2018)\"\r\n    )\r\n  ) %>% \r\n  left_join(cpi, by = \"date\") %>% \r\n  na.omit() %>% \r\n  mutate(\r\n    new_close = close / cpi, .before = close\r\n  ) %>% \r\n  select(-close,-cpi)\r\n\r\n\r\n\r\n\r\n\r\ninitialValues2 <- bist100_monthly %>% \r\n  group_by(election) %>% \r\n  slice_min(date) %>% \r\n  rename(\"initial\"=2) %>% \r\n  ungroup()\r\n\r\n\r\n\r\n\r\n\r\nmaster2 <- bist100_monthly %>% \r\n  left_join(initialValues2, by = c(\"date\",\"election\")) %>% \r\n  mutate(initial = zoo::na.locf(initial),\r\n         \"indices\" = new_close / initial * 100) %>% \r\n  group_by(election) %>% \r\n  mutate(t = row_number()) %>% \r\n  ungroup()\r\n\r\n\r\n\r\n\r\n\r\nggplot(master2, aes(x = t, y = indices)) +\r\n  geom_line(data = master2 %>% rename(election2 = election),\r\n            aes(group = election2), color = \"gray\", size = 1) +\r\n  geom_line(color = \"dark blue\", size = 1.5) +\r\n  ggthemes::theme_fivethirtyeight() +\r\n  theme(strip.text = element_text(size = 20),\r\n        axis.text = element_text(size = 20)) +\r\n  facet_wrap(~election, ncol = 2)\r\n\r\n\r\n\r\n\r\n\r\n\r\nperform100_2 <- master2 %>% \r\n  group_by(election) %>% \r\n  filter(date == min(date) | date == max(date)) %>% \r\n  mutate(\r\n    return = log(1 + (lead(indices)-indices) / indices) * 100\r\n  ) %>% \r\n  ungroup() %>% \r\n  select(election,return) %>% \r\n  na.omit()\r\n\r\n\r\n\r\n\r\n\r\nelection\r\n\r\n\r\nreturn\r\n\r\n\r\nGE-1 (Nov 3, 2002)\r\n\r\n\r\n108.0\r\n\r\n\r\nGE-2 (Jul 22, 2007)\r\n\r\n\r\n-7.9\r\n\r\n\r\nGE-3 (Jun 12, 2011)\r\n\r\n\r\n-2.1\r\n\r\n\r\nGE-4 (Jun 07, 2015)\r\n\r\n\r\n-6.5\r\n\r\n\r\nGE-5 (Nov 01, 2015)\r\n\r\n\r\n-1.5\r\n\r\n\r\nGE-6 (Jun 24, 2018)\r\n\r\n\r\n7.0\r\n\r\n\r\nWhen we consider inflation, we can see how the gap between the best\r\nreturn and the current period widens!\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-09-21-post32/post32_files/figure-html5/unnamed-chunk-16-1.png",
    "last_modified": "2022-09-21T22:02:48+03:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-09-20-post31/",
    "title": "Mapping of Istanbul's Districts Based on Socioeconomic Development Index Results",
    "description": "How to map data.",
    "author": [
      {
        "name": "Uraz Akgül",
        "url": {}
      }
    ],
    "date": "2022-09-20",
    "categories": [
      "Map"
    ],
    "contents": "\r\nTurkey’s most developed districts were determined by the Ministry of\r\nIndustry and Technology.\r\nAccording to the data of the Ministry, the top 10 most developed\r\ndistricts out of 973 are:\r\n\r\n\r\nrank\r\n\r\n\r\nprovince\r\n\r\n\r\ndistrict\r\n\r\n\r\nscore\r\n\r\n\r\n1\r\n\r\n\r\nİstanbul\r\n\r\n\r\nŞişli\r\n\r\n\r\n6.959\r\n\r\n\r\n2\r\n\r\n\r\nAnkara\r\n\r\n\r\nÇankaya\r\n\r\n\r\n6.901\r\n\r\n\r\n3\r\n\r\n\r\nİstanbul\r\n\r\n\r\nBeşiktaş\r\n\r\n\r\n5.940\r\n\r\n\r\n4\r\n\r\n\r\nİstanbul\r\n\r\n\r\nKadıköy\r\n\r\n\r\n4.910\r\n\r\n\r\n5\r\n\r\n\r\nAnkara\r\n\r\n\r\nYenimahalle\r\n\r\n\r\n4.481\r\n\r\n\r\n6\r\n\r\n\r\nİstanbul\r\n\r\n\r\nBakırköy\r\n\r\n\r\n4.465\r\n\r\n\r\n7\r\n\r\n\r\nİstanbul\r\n\r\n\r\nFatih\r\n\r\n\r\n4.226\r\n\r\n\r\n8\r\n\r\n\r\nBursa\r\n\r\n\r\nNilüfer\r\n\r\n\r\n4.072\r\n\r\n\r\n9\r\n\r\n\r\nİstanbul\r\n\r\n\r\nAtaşehir\r\n\r\n\r\n3.545\r\n\r\n\r\n10\r\n\r\n\r\nİstanbul\r\n\r\n\r\nBaşakşehir\r\n\r\n\r\n3.468\r\n\r\n\r\nIstanbul’s Şişli named most-developed district in Turkey.\r\nIn this study, we will map the districts of Istanbul. All data can be\r\naccessed by downloading the post31.xlsx file from here.\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(raster)\r\nlibrary(sf)\r\n\r\n\r\n\r\nFollowing is a ranking of the districts’ socioeconomic\r\ndevelopment:\r\n\r\n\r\ndf <- readxl::read_excel(\"data.xlsx\")\r\n\r\ndf %>% \r\n  filter(province == \"İstanbul\") %>% \r\n  ggplot(aes(x = reorder(district,score), y = score, fill = score)) +\r\n  geom_col() +\r\n  theme_minimal() +\r\n  theme(axis.title = element_blank(),\r\n        axis.text = element_text(size = 15),\r\n        legend.position = \"none\") +\r\n  scale_fill_gradient(low = \"red\", high = \"orange\") +\r\n  coord_flip()\r\n\r\n\r\n\r\n\r\nWe first download Turkey’s file from here in order to\r\ncreate the map. Click on the Shapefile name to download it. The zip you\r\ndownloaded can be extracted and placed in your project file. There are 3\r\nlevels: level-0, level-1, and the level we are interested in,\r\nlevel-2.\r\nAnother option is to run the code below instead of following the\r\nsteps above. The {raster} package will be used.\r\n\r\n\r\n# raster::getData()\r\n\r\ndistricts_shp <- getData(\r\n  name = \"GADM\",\r\n  country = \"TUR\",\r\n  level = 2\r\n)\r\n\r\nclass(districts_shp)\r\n\r\n\r\n[1] \"SpatialPolygonsDataFrame\"\r\nattr(,\"package\")\r\n[1] \"sp\"\r\n\r\nLet’s create a data frame from the file you’ll see in your\r\nenvironment.\r\n\r\n\r\n# ggplot2::fortify()\r\n\r\ndistricts_shp_df <- fortify(districts_shp, region = \"NAME_2\")\r\n\r\nclass(districts_shp_df)\r\n\r\n\r\n[1] \"data.frame\"\r\n\r\nMerge the istanbul and districts_shp_df dataframes.\r\n\r\n\r\nistanbul <- df %>% \r\n  filter(province == \"İstanbul\") %>% \r\n  rename(\"id\"=\"district\") %>% \r\n  mutate(\r\n    id = ifelse(id == \"Arnavutköy\", \"Arnavutkoy\", id),\r\n    id = ifelse(id == \"Ataşehir\", \"Atasehir\", id),\r\n    id = ifelse(id == \"Başakşehir\", \"Basaksehir\", id),\r\n    id = ifelse(id == \"Beylikdüzü\", \"Beylikduzu\", id),\r\n    id = ifelse(id == \"Eyüpsultan\", \"Eyüp\", id),\r\n    id = ifelse(id == \"Çekmeköy\", \"Çekmekoy\", id)\r\n  )\r\n\r\nmaster <- districts_shp_df %>% \r\n  inner_join(istanbul, by = \"id\")\r\n\r\n\r\n\r\nIt’s now time to map the data.\r\n\r\n\r\nggplot() +\r\n  geom_polygon(data = master, aes(x = long, y = lat, group = group, fill  = score)) +\r\n  scale_fill_gradient(low = \"dark red\", high = \"yellow\") +\r\n  theme_void() +\r\n  theme(\r\n    legend.title = element_blank(),\r\n    legend.position = \"bottom\",\r\n    legend.key.width = unit(2,\"cm\")\r\n  )\r\n\r\n\r\n\r\n\r\nOops! Even to someone who is familiar with Istanbul, the map appears\r\nnormal, but Silivri, one of the city’s districts, is not included.\r\n\r\n\r\nunique(master$id)\r\n\r\n\r\n [1] \"Adalar\"        \"Arnavutkoy\"    \"Atasehir\"      \"Avcılar\"      \r\n [5] \"Bağcılar\"      \"Bahçelievler\"  \"Bakırköy\"      \"Basaksehir\"   \r\n [9] \"Bayrampaşa\"    \"Beşiktaş\"      \"Beykoz\"        \"Beylikduzu\"   \r\n[13] \"Beyoğlu\"       \"Büyükçekmece\"  \"Çatalca\"       \"Çekmekoy\"     \r\n[17] \"Esenler\"       \"Esenyurt\"      \"Eyüp\"          \"Fatih\"        \r\n[21] \"Gaziosmanpaşa\" \"Güngören\"      \"Kadıköy\"       \"Kağıthane\"    \r\n[25] \"Kartal\"        \"Küçükçekmece\"  \"Maltepe\"       \"Pendik\"       \r\n[29] \"Sancaktepe\"    \"Sarıyer\"       \"Sultanbeyli\"   \"Sultangazi\"   \r\n[33] \"Şile\"          \"Şişli\"         \"Tuzla\"         \"Ümraniye\"     \r\n[37] \"Üsküdar\"       \"Zeytinburnu\"  \r\n\r\n\r\n\r\nistanbul$id\r\n\r\n\r\n [1] \"Şişli\"         \"Beşiktaş\"      \"Kadıköy\"       \"Bakırköy\"     \r\n [5] \"Fatih\"         \"Atasehir\"      \"Basaksehir\"    \"Beyoğlu\"      \r\n [9] \"Ümraniye\"      \"Sarıyer\"       \"Üsküdar\"       \"Tuzla\"        \r\n[13] \"Maltepe\"       \"Beylikduzu\"    \"Pendik\"        \"Esenyurt\"     \r\n[17] \"Bahçelievler\"  \"Zeytinburnu\"   \"Bağcılar\"      \"Kartal\"       \r\n[21] \"Bayrampaşa\"    \"Kağıthane\"     \"Küçükçekmece\"  \"Güngören\"     \r\n[25] \"Büyükçekmece\"  \"Eyüp\"          \"Adalar\"        \"Beykoz\"       \r\n[29] \"Avcılar\"       \"Gaziosmanpaşa\" \"Çekmekoy\"      \"Esenler\"      \r\n[33] \"Silivri\"       \"Sancaktepe\"    \"Sultangazi\"    \"Arnavutkoy\"   \r\n[37] \"Çatalca\"       \"Şile\"          \"Sultanbeyli\"  \r\n\r\nWe leave everything behind and move on!\r\nWe went with the second option, but now we’re back to the first. With\r\nthe help of the {sf} package, we’ll read a file with the .shp extension\r\nfrom the file we saved earlier.\r\n\r\n\r\n# sf::read_sf()\r\n\r\ndistricts_shp2 <- read_sf(\"gadm41_TUR_shp/gadm41_TUR_2.shp\") %>% \r\n  filter(\r\n    NAME_1 == \"Istanbul\"\r\n  )\r\n\r\n\r\n\r\nI won’t change anything from what I previously told you.\r\n\r\n\r\nistanbul2 <- istanbul %>% \r\n  rename(\"NAME_2\"=\"id\")\r\n\r\nmaster2 <- districts_shp2 %>% \r\n  inner_join(istanbul2, by = \"NAME_2\")\r\n\r\n\r\n\r\nIt’s now time to remap the data.\r\n\r\n\r\nmaster2 %>% \r\n  ggplot(aes(fill = score)) +\r\n  geom_sf() +\r\n  scale_fill_gradient(low = \"dark red\", high = \"yellow\") +\r\n  theme_void() +\r\n  theme(\r\n    legend.title = element_blank(),\r\n    legend.position = \"bottom\",\r\n    legend.key.width = unit(2,\"cm\"),\r\n    plot.title = element_text(size = 15, face = \"bold\", hjust = 0.5)\r\n  ) +\r\n  labs(\r\n    title = \"Socioeconomic Development Ranking of Istanbul's Districts, 2022\"\r\n  )\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-09-20-post31/post31_files/figure-html5/unnamed-chunk-12-1.png",
    "last_modified": "2022-09-20T20:15:27+03:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-09-18-post29/",
    "title": "Clustering of Currency Exchange Rates Using Hierarchical Methods Based on Dynamic Time Warping",
    "description": "Clustering the daily spot exchange rates of the 30 currencies.",
    "author": [
      {
        "name": "Uraz Akgül",
        "url": {}
      }
    ],
    "date": "2022-09-18",
    "categories": [
      "Machine Learning",
      "Finance"
    ],
    "contents": "\r\nIn this study, we collect and analyze historical exchange rate data\r\nfor the 30 currencies listed in the table below, considering the US\r\ndollar as reference. Reuters is the source of the data and you can\r\naccess it by downloading post29.xlsx file here.\r\n\r\n\r\ncountry\r\n\r\n\r\ncurrency\r\n\r\n\r\nArgentine\r\n\r\n\r\nUSDARS\r\n\r\n\r\nBrazil\r\n\r\n\r\nUSDBRL\r\n\r\n\r\nBulgaria\r\n\r\n\r\nUSDBGN\r\n\r\n\r\nChile\r\n\r\n\r\nUSDCLP\r\n\r\n\r\nColombia\r\n\r\n\r\nUSDCOP\r\n\r\n\r\nCzech Republic\r\n\r\n\r\nUSDCZK\r\n\r\n\r\nEgypt\r\n\r\n\r\nUSDEGP\r\n\r\n\r\nHungary\r\n\r\n\r\nUSDHUF\r\n\r\n\r\nIndia\r\n\r\n\r\nUSDINR\r\n\r\n\r\nIndonesia\r\n\r\n\r\nUSDIDR\r\n\r\n\r\nKazakhstan\r\n\r\n\r\nUSDKZT\r\n\r\n\r\nKenya\r\n\r\n\r\nUSDKES\r\n\r\n\r\nMalaysia\r\n\r\n\r\nUSDMYR\r\n\r\n\r\nMexico\r\n\r\n\r\nUSDMXN\r\n\r\n\r\nNigeria\r\n\r\n\r\nUSDNGN\r\n\r\n\r\nPakistan\r\n\r\n\r\nUSDPKR\r\n\r\n\r\nPeru\r\n\r\n\r\nUSDPEN\r\n\r\n\r\nPhilippines\r\n\r\n\r\nUSDPHP\r\n\r\n\r\nPoland\r\n\r\n\r\nUSDPLN\r\n\r\n\r\nRomania\r\n\r\n\r\nUSDRON\r\n\r\n\r\nRussia\r\n\r\n\r\nUSDRUB\r\n\r\n\r\nSingapore\r\n\r\n\r\nUSDSGD\r\n\r\n\r\nSouth Africa\r\n\r\n\r\nUSDZAR\r\n\r\n\r\nSouth Korea\r\n\r\n\r\nUSDKRW\r\n\r\n\r\nSri Lanka\r\n\r\n\r\nUSDLKR\r\n\r\n\r\nTaiwan\r\n\r\n\r\nUSDTWD\r\n\r\n\r\nThailand\r\n\r\n\r\nUSDTHB\r\n\r\n\r\nTurkey\r\n\r\n\r\nUSDTRY\r\n\r\n\r\nUkranie\r\n\r\n\r\nUSDUAH\r\n\r\n\r\nVietnam\r\n\r\n\r\nUSDVND\r\n\r\n\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(dtwclust)\r\n\r\n\r\n\r\nBy calculating the average, we can convert the daily frequency data\r\nto monthly data.\r\n\r\n\r\ndf <- readxl::read_excel(\"data.xlsx\") %>% \r\n  na.omit() %>% \r\n  pivot_longer(!DATE, names_to = \"Vars\", values_to = \"Vals\") %>% \r\n  mutate(\r\n    Months = format(DATE,\"%m\"),\r\n    Years = format(DATE,\"%Y\")\r\n  ) %>% \r\n  group_by(Vars,Months,Years) %>% \r\n  summarise(\r\n    MeanVals = mean(Vals)\r\n  ) %>% \r\n  ungroup() %>% \r\n  arrange(Vars,Years) %>% \r\n  mutate(\r\n    DATE = as.Date(paste0(Years,\"-\",Months,\"-\",1))\r\n  ) %>% \r\n  select(DATE,Vars,MeanVals) %>% \r\n  pivot_wider(names_from = \"Vars\", values_from = \"MeanVals\")\r\n\r\n\r\n\r\nIt would be better if we standardize the data. The following formula\r\ncan be used to standardize the values in a dataset.\r\n\\(z = \\frac{(x - \\mu)}{\\sigma}\\)\r\n\\(z\\): Standardised value or\r\nZ-score\r\n\\(x\\): Original value\r\n\\(\\mu\\): Sample mean\r\n\\(\\sigma\\): Sample standard\r\ndeviation\r\n\r\n\r\nstandardize <- function(x){\r\n  \r\n  (x - mean(x)) / sd(x)\r\n  \r\n}\r\n\r\ndf_standardize <- df %>% \r\n  mutate_at(\r\n    vars(-DATE), function(x) standardize(x)\r\n  )\r\n\r\n\r\n\r\n\r\n\r\ndf_standardize %>% \r\n  pivot_longer(!DATE, names_to = \"Vars\", values_to = \"Vals\") %>% \r\n  ggplot(aes(x = DATE, y = Vals)) +\r\n  geom_line() +\r\n  facet_wrap(~Vars, ncol = 5) +\r\n  theme_minimal() +\r\n  theme(axis.title = element_blank(),\r\n        axis.text.x = element_text(size = 15),\r\n        axis.text.y = element_blank(),\r\n        strip.text = element_text(size = 20))\r\n\r\n\r\n\r\n\r\nWhat happens when the data is standardised? An example is provided\r\nbelow. I’d like to draw your attention to the y-axis.\r\n\r\n\r\n\r\nType: h or Hierarchical\r\nAn algorithm called hierarchical clustering divides objects into\r\nclusters based on how similar they are. The result is a collection of\r\nclusters, each of which differs from the others while having things that\r\nare generally similar to one another.\r\nDistance: dtw or Dynamic Time Warping\r\nWe can explain DTW by comparing it to the Euclidean distance. The\r\ndistance between two points in Euclidean space is known as the Euclidean\r\ndistance.\r\n\r\n\r\n\r\nFigure 1: https://rtavenar.github.io/blog/dtw.html\r\n\r\n\r\n\r\nDynamic Time Warping uses temporal distortions to create the best\r\npossible alignment, whereas Euclidean only allows one-to-one point\r\ncomparison.\r\nAs a result of various trials, I decided to create 3 different\r\nclusters.\r\n\r\n\r\nk <- 3L\r\n\r\ndata_cluster <- tsclust(\r\n  t(df_standardize[,-1]), # data\r\n  type = \"h\", # What type of clustering method to use\r\n  k = k, # Number of desired clusters\r\n  distance = \"dtw\" # Dynamic time warping\r\n)\r\ncluster <- as.data.frame(cutree(data_cluster, k=k)) %>% \r\n  rownames_to_column(., var = \"Vars\") %>% \r\n  rename(\"Cluster\"=2)\r\n\r\n\r\n\r\n\r\n\r\nVars\r\n\r\n\r\nCluster\r\n\r\n\r\nARS\r\n\r\n\r\n1\r\n\r\n\r\nEGP\r\n\r\n\r\n1\r\n\r\n\r\nHUF\r\n\r\n\r\n1\r\n\r\n\r\nIDR\r\n\r\n\r\n1\r\n\r\n\r\nINR\r\n\r\n\r\n1\r\n\r\n\r\nKES\r\n\r\n\r\n1\r\n\r\n\r\nKZT\r\n\r\n\r\n1\r\n\r\n\r\nLKR\r\n\r\n\r\n1\r\n\r\n\r\nMXN\r\n\r\n\r\n1\r\n\r\n\r\nNGN\r\n\r\n\r\n1\r\n\r\n\r\nPKR\r\n\r\n\r\n1\r\n\r\n\r\nRON\r\n\r\n\r\n1\r\n\r\n\r\nRUB\r\n\r\n\r\n1\r\n\r\n\r\nTRY\r\n\r\n\r\n1\r\n\r\n\r\nUAH\r\n\r\n\r\n1\r\n\r\n\r\nVND\r\n\r\n\r\n1\r\n\r\n\r\nZAR\r\n\r\n\r\n1\r\n\r\n\r\nBGN\r\n\r\n\r\n2\r\n\r\n\r\nBRL\r\n\r\n\r\n2\r\n\r\n\r\nCLP\r\n\r\n\r\n2\r\n\r\n\r\nCOP\r\n\r\n\r\n2\r\n\r\n\r\nCZK\r\n\r\n\r\n2\r\n\r\n\r\nKRW\r\n\r\n\r\n2\r\n\r\n\r\nMYR\r\n\r\n\r\n2\r\n\r\n\r\nPEN\r\n\r\n\r\n2\r\n\r\n\r\nPHP\r\n\r\n\r\n2\r\n\r\n\r\nPLN\r\n\r\n\r\n2\r\n\r\n\r\nSGD\r\n\r\n\r\n3\r\n\r\n\r\nTHB\r\n\r\n\r\n3\r\n\r\n\r\nTWD\r\n\r\n\r\n3\r\n\r\n\r\n\r\n\r\ndf2 <- df_standardize %>% \r\n  pivot_longer(!DATE, names_to = \"Vars\", values_to = \"Vals\") %>% \r\n  arrange(Vars) %>% \r\n  left_join(cluster, by = \"Vars\")\r\n\r\n\r\n\r\n\r\n\r\nfor(i in 1:length(unique(cluster$Cluster))){\r\n  \r\n  g <- ggplot(df2 %>% filter(Cluster == i), aes(x = DATE, y = Vals)) +\r\n    geom_line(data = df2 %>% filter(Cluster == i) %>% rename(Vars2 = Vars), aes(group = Vars2), color = \"gray\", size = 1) +\r\n    geom_line(color = \"dark blue\", size = 2) +\r\n    ggthemes::theme_fivethirtyeight() +\r\n    theme(strip.text = element_text(size = 20),\r\n          axis.text = element_blank()) +\r\n    facet_wrap(~Vars)\r\n  \r\n  plot(g)\r\n  \r\n}\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-09-18-post29/post29_files/figure-html5/unnamed-chunk-11-1.png",
    "last_modified": "2022-09-18T12:37:58+03:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-09-18-post30/",
    "title": "Scraping Data from Wikipedia Tables",
    "description": "How to scrape and clean wikipedia tables.",
    "author": [
      {
        "name": "Uraz Akgül",
        "url": {}
      }
    ],
    "date": "2022-09-18",
    "categories": [
      "Web",
      "Data Manipulation"
    ],
    "contents": "\r\nI’ve scraped many websites, and scraping Wikipedia is by far the\r\nsimplest. This post will teach you how to scrape tables from Wikipedia\r\nand how to clean the data that has been scraped.\r\nThe two urls we’ll use are given below.\r\nhttps://en.wikipedia.org/wiki/Democracy_Index\r\nThe index is based on 60 indicators grouped in five different\r\ncategories, measuring pluralism, civil liberties and political\r\nculture.\r\nhttps://en.wikipedia.org/wiki/Corruption_Perceptions_Index\r\nThe index which ranks countries by their perceived levels of\r\npublic sector corruption, as determined by expert assessments and\r\nopinion surveys.\r\nFor more information, please see the URLs above.\r\nThe index scores of countries participating in the Shanghai\r\nCooperation Organization Summit in 2022 will be compared to those of\r\ncountries that are not members of the organization.\r\n\r\n\r\nlibrary(rvest)\r\nlibrary(tidyverse)\r\n\r\n\r\n\r\n\r\n\r\nurl1 <- \"https://en.wikipedia.org/wiki/Democracy_Index\"\r\nurl2 <- \"https://en.wikipedia.org/wiki/Corruption_Perceptions_Index\"\r\n\r\nparticipants <- c(\r\n  \"China\",\r\n  \"Russia\",\r\n  \"Kyrgyzstan\",\r\n  \"Tajikistan\",\r\n  \"Kazakhstan\",\r\n  \"Uzbekistan\",\r\n  \"India\",\r\n  \"Pakistan\",\r\n  \"Iran\",\r\n  \"Turkmenistan\",\r\n  \"Mongolia\",\r\n  \"Belarus\",\r\n  \"Turkey\",\r\n  \"Azerbaijan\"\r\n)\r\n\r\n\r\n\r\nWhile scraping the Democracy Index table, we followed the steps\r\noutlined below.\r\nRead HTML code using the read_html() function\r\nGet tables using the html_table() function\r\nThe 6th table in the list is the one we want (by running the\r\nfirst two lines of code, you can see all of the tables)\r\nSelect country and score columns\r\nDivide the countries based on the previously stated\r\ncriteria\r\nRename score column\r\n\r\n\r\ndemocracy <- read_html(url1) %>% \r\n  html_table() %>% \r\n  .[[6]] %>% \r\n  select(3,5) %>% \r\n  mutate(\r\n    Shanghai = ifelse(\r\n      Country %in% participants, \"Shanghai\", \"Non-Shanghai\"\r\n    )\r\n  ) %>% \r\n  rename(\r\n    \"Democracy\"=2\r\n  )\r\n\r\n\r\n\r\nWhile scraping the Corruption Index table, we followed the steps\r\noutlined below.\r\nRead HTML code using the read_html() function\r\nGet tables using the html_table() function\r\nThe 5th table in the list is the one we want (by running the\r\nfirst two lines of code, you can see all of the tables)\r\nSelect country and score columns\r\nRemove the first line\r\nRename country and score columns\r\nConvert character to numeric for the score column\r\n\r\n\r\ncorruption <- read_html(url2) %>% \r\n  html_table() %>% \r\n  .[[5]] %>% \r\n  select(2,3) %>% \r\n  slice(-1) %>% \r\n  rename(\r\n    \"Country\"=1,\r\n    \"Corruption\"=2\r\n  ) %>% \r\n  mutate(\r\n    Corruption = as.numeric(Corruption)\r\n  )\r\n\r\n\r\n\r\nThe inner_join() function can be used to join common countries from\r\ntwo tables.\r\n\r\n\r\nmaster <- democracy %>% \r\n  inner_join(corruption, by = \"Country\") %>% # the common rows between two tables\r\n  select(Country,Democracy,Corruption,Shanghai)\r\n\r\n\r\n\r\nAs an example, the ten countries in the table are listed below.\r\n\r\n\r\nCountry\r\n\r\n\r\nDemocracy\r\n\r\n\r\nCorruption\r\n\r\n\r\nShanghai\r\n\r\n\r\nCanada\r\n\r\n\r\n8.87\r\n\r\n\r\n74\r\n\r\n\r\nNon-Shanghai\r\n\r\n\r\nUnited States\r\n\r\n\r\n7.85\r\n\r\n\r\n67\r\n\r\n\r\nNon-Shanghai\r\n\r\n\r\nAustria\r\n\r\n\r\n8.07\r\n\r\n\r\n74\r\n\r\n\r\nNon-Shanghai\r\n\r\n\r\nBelgium\r\n\r\n\r\n7.51\r\n\r\n\r\n73\r\n\r\n\r\nNon-Shanghai\r\n\r\n\r\nCyprus\r\n\r\n\r\n7.43\r\n\r\n\r\n53\r\n\r\n\r\nNon-Shanghai\r\n\r\n\r\nDenmark\r\n\r\n\r\n9.09\r\n\r\n\r\n88\r\n\r\n\r\nNon-Shanghai\r\n\r\n\r\nFinland\r\n\r\n\r\n9.27\r\n\r\n\r\n88\r\n\r\n\r\nNon-Shanghai\r\n\r\n\r\nFrance\r\n\r\n\r\n7.99\r\n\r\n\r\n71\r\n\r\n\r\nNon-Shanghai\r\n\r\n\r\nGermany\r\n\r\n\r\n8.67\r\n\r\n\r\n80\r\n\r\n\r\nNon-Shanghai\r\n\r\n\r\nGreece\r\n\r\n\r\n7.56\r\n\r\n\r\n49\r\n\r\n\r\nNon-Shanghai\r\n\r\n\r\nLet’s visualize the data in the last step.\r\n\r\n\r\nggplot(master, aes(x = Democracy, y = Corruption, color = Shanghai)) +\r\n  geom_point(size = 5, alpha = .5) +\r\n  ggrepel::geom_text_repel(data = master %>% filter(Shanghai == \"Shanghai\"),\r\n                           aes(label = Country), size = 7, show.legend = FALSE) +\r\n  geom_vline(xintercept = 5, linetype = \"dashed\") +\r\n  geom_hline(yintercept = 50, linetype = \"dashed\") +\r\n  ggthemes::theme_fivethirtyeight() +\r\n  theme(\r\n    axis.title = element_text(size = 25),\r\n    axis.text = element_text(size = 20),\r\n    legend.title = element_blank(),\r\n    legend.position = \"top\",\r\n    legend.text = element_text(size = 20)\r\n  ) +\r\n  scale_color_manual(values = c(\"gray60\",\"red\"))\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-09-18-post30/post30_files/figure-html5/unnamed-chunk-7-1.png",
    "last_modified": "2022-09-18T15:54:15+03:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-09-12-post28/",
    "title": "How to Create Bar Race Animation Charts",
    "description": "Animated bar race ranking.",
    "author": [
      {
        "name": "Uraz Akgül",
        "url": {}
      }
    ],
    "date": "2022-09-12",
    "categories": [
      "Data Visualization"
    ],
    "contents": "\r\nYou can compare variables over time using animated bar charts by\r\nfollowing the bars as if you were betting on a single one.\r\nLet’s create a static graph first, and then one that is dynamic, to\r\ncompare the two.\r\nFollow the steps below to access the data, or click here to download the post28.xlsx file.\r\nhttps://www.tuik.gov.tr/ (The English language option is\r\navailable in the upper right corner)\r\nStatistics/Transportation and Communication\r\nDatabases/Road Motor Vehicle Statistics\r\nThe number of road motor vehicles registered to the traffic/Type\r\nof Vehicle&Brand\r\nCars&All, respectively.\r\nMonthly (All), Region (Turkey)\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(gganimate)\r\nlibrary(gifski)\r\n\r\n\r\n\r\n\r\n\r\ndf <- readxl::read_excel(\"data.xlsx\") %>% \r\n  select(-1) %>% \r\n  slice(-c(1:3)) %>% \r\n  `colnames<-`(paste0(\"C\",seq(1,ncol(.),1))) %>% \r\n  mutate(\r\n    C1 = unlist(lapply(str_extract_all(C1, \"\\\\([^()]+\\\\)\"), \"[[\", 1)),\r\n    C1 = gsub(\"[()]\", \"\", C1),\r\n    C1 = zoo::na.locf(C1)\r\n  ) %>% \r\n  pivot_longer(!c(C1,C2), names_to = \"month\", values_to = \"value\") %>% \r\n  mutate(\r\n    month = case_when(\r\n      month == \"C3\" ~ \"01\",\r\n      month == \"C4\" ~ \"02\",\r\n      month == \"C5\" ~ \"03\",\r\n      month == \"C6\" ~ \"04\",\r\n      month == \"C7\" ~ \"05\",\r\n      month == \"C8\" ~ \"06\",\r\n      month == \"C9\" ~ \"07\",\r\n      month == \"C10\" ~ \"08\",\r\n      month == \"C11\" ~ \"09\",\r\n      month == \"C12\" ~ \"10\",\r\n      month == \"C13\" ~ \"11\",\r\n      month == \"C14\" ~ \"12\",\r\n    )\r\n  ) %>% \r\n  mutate(\r\n    date = as.Date(paste0(C2,\"-\",month,\"-\",1)),\r\n    value = as.numeric(value)\r\n  ) %>% \r\n  rename(\"brand\"=\"C1\") %>% \r\n  select(date,brand,value) %>% \r\n  filter(date <= as.Date(\"2022-07-01\")) %>% \r\n  mutate(value = ifelse(is.na(value), 0, value),\r\n         brand = ifelse(brand == \"Diger\",\"Others\",brand))\r\n\r\n\r\n\r\nThere are 49 different car brands.\r\n\r\n\r\ndf_filtered <- df %>% \r\n  group_by(date) %>% \r\n  mutate(p = value / sum(value) * 100) %>% \r\n  ungroup()\r\n\r\n\r\n\r\nStatic plot:\r\n\r\n\r\nggplot(df_filtered %>% filter(date == as.Date(\"2022-07-01\")),\r\n       aes(x = reorder(brand,p), y = p, fill = p)) +\r\n  geom_col() +\r\n  geom_text(aes(label = round(p,digits = 1)), hjust = -0.1, size = 5) +\r\n  coord_flip() +\r\n  theme_minimal() +\r\n  theme(legend.position = \"none\",\r\n        axis.title = element_blank(),\r\n        axis.text.y = element_text(size = 17),\r\n        axis.text.x = element_blank(),\r\n        plot.title = element_text(size = 20, face = \"bold\"),\r\n        plot.subtitle = element_text(size = 20)) +\r\n  scale_fill_gradient(low = \"orange\", high = \"red\") +\r\n  scale_y_continuous(limits = c(0,max(df_filtered$p)+1)) +\r\n  labs(\r\n    title = \"Number of Newly Registered Car Brands in Turkey (%)\",\r\n    subtitle = \"07/2022\"\r\n  )\r\n\r\n\r\n\r\n\r\nNow it’s time to set the animation up.\r\n\r\n\r\ndf_rank <- df_filtered %>% \r\n  arrange(date,desc(p)) %>% \r\n  group_by(date) %>% \r\n  mutate(rank = row_number(),\r\n         date = format(date,\"%Y/%m\"),\r\n         p = round(p, digits = 1))\r\n\r\nanim <- ggplot(df_rank, aes(x = rank, y = p, fill = p)) +\r\n  geom_col() +\r\n  geom_text(aes(y = 0, label = paste0(brand,\" \")), hjust = 1) +\r\n  geom_text(aes(y = p, label = p), hjust = -0.1) +\r\n  coord_flip(expand = TRUE) +\r\n  theme_minimal() +\r\n  theme(legend.position = \"none\",\r\n        axis.title = element_blank(),\r\n        axis.text = element_blank(),\r\n        plot.title = element_text(size = 20, face = \"bold\"),\r\n        plot.subtitle = element_text(size = 20)) +\r\n  scale_fill_gradient(low = \"orange\", high = \"red\") +\r\n  scale_x_reverse() +\r\n  labs(\r\n    title = \"Number of Newly Registered Car Brands in Turkey (%)\",\r\n    subtitle = \"{closest_state}\"\r\n  ) +\r\n  transition_states(date) +\r\n  view_follow()\r\n\r\nanimate(\r\n  anim,\r\n  223,\r\n  fps = 40,\r\n  duration = 50,\r\n  width = 1800,\r\n  height = 1000,\r\n  renderer = gifski_renderer(\"gganim.gif\")\r\n)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-09-12-post28/post28_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2022-09-12T19:00:17+03:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-09-11-post27/",
    "title": "How to Automate Repetitive and Time-Consuming Reports",
    "description": "Scheduling a report to run at regular intervals.",
    "author": [
      {
        "name": "Uraz Akgül",
        "url": {}
      }
    ],
    "date": "2022-09-11",
    "categories": [
      "Automation",
      "Web"
    ],
    "contents": "\r\nIn the age of technology we live in, manual processes are extremely\r\ntime-consuming. Fortunately, there are an increasing number of people\r\nwho know how to use technology or desire to do so.\r\nIn this post, I’ll show you how to run a script at regular intervals.\r\nAssume we want to get intraday cryptocurrencies data from Yahoo Finance\r\nevery 5 minutes using web scraping. The top 100 cryptocurrencies in the\r\ntable are what we are most interested in.\r\n\r\n\r\nlibrary(rvest)\r\nlibrary(tidyverse)\r\nlibrary(DBI)\r\nlibrary(RSQLite)\r\nlibrary(taskscheduleR)\r\n\r\n\r\n\r\n\r\n\r\nurl <- \"https://finance.yahoo.com/cryptocurrencies/?count=100&offset=0\"\r\n\r\ndf <- read_html(url) %>% \r\n  html_table() %>% \r\n  .[[1]]\r\n\r\n\r\n\r\nThe top ten cryptocurrencies in the table are listed below.\r\n\r\n\r\nSymbol\r\n\r\n\r\nName\r\n\r\n\r\nPrice (Intraday)\r\n\r\n\r\nChange\r\n\r\n\r\n% Change\r\n\r\n\r\nMarket Cap\r\n\r\n\r\nVolume in Currency (Since 0:00 UTC)\r\n\r\n\r\nVolume in Currency (24Hr)\r\n\r\n\r\nTotal Volume All Currencies (24Hr)\r\n\r\n\r\nCirculating Supply\r\n\r\n\r\n52 Week Range\r\n\r\n\r\nDay Chart\r\n\r\n\r\nBTC-USD\r\n\r\n\r\nBitcoin USD\r\n\r\n\r\n21,629.88\r\n\r\n\r\n321.140000\r\n\r\n\r\n+1.51%\r\n\r\n\r\n414.16B\r\n\r\n\r\n34.741B\r\n\r\n\r\n34.741B\r\n\r\n\r\n34.741B\r\n\r\n\r\n19.148M\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nETH-USD\r\n\r\n\r\nEthereum USD\r\n\r\n\r\n1,761.44\r\n\r\n\r\n41.500000\r\n\r\n\r\n+2.41%\r\n\r\n\r\n215.468B\r\n\r\n\r\n12.414B\r\n\r\n\r\n12.414B\r\n\r\n\r\n12.414B\r\n\r\n\r\n122.325M\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nUSDT-USD\r\n\r\n\r\nTether USD\r\n\r\n\r\n1.0003\r\n\r\n\r\n0.000000\r\n\r\n\r\n-0.00%\r\n\r\n\r\n67.672B\r\n\r\n\r\n48.586B\r\n\r\n\r\n48.586B\r\n\r\n\r\n48.586B\r\n\r\n\r\n67.652B\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nUSDC-USD\r\n\r\n\r\nUSD Coin USD\r\n\r\n\r\n0.999920\r\n\r\n\r\n0.000095\r\n\r\n\r\n+0.01%\r\n\r\n\r\n51.676B\r\n\r\n\r\n5.498B\r\n\r\n\r\n5.498B\r\n\r\n\r\n5.498B\r\n\r\n\r\n51.68B\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nBNB-USD\r\n\r\n\r\nBNB USD\r\n\r\n\r\n295.43\r\n\r\n\r\n0.860000\r\n\r\n\r\n+0.29%\r\n\r\n\r\n47.663B\r\n\r\n\r\n749.662M\r\n\r\n\r\n749.662M\r\n\r\n\r\n749.662M\r\n\r\n\r\n161.337M\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nBUSD-USD\r\n\r\n\r\nBinance USD USD\r\n\r\n\r\n1.0002\r\n\r\n\r\n0.000100\r\n\r\n\r\n+0.01%\r\n\r\n\r\n20.007B\r\n\r\n\r\n8.959B\r\n\r\n\r\n8.959B\r\n\r\n\r\n8.959B\r\n\r\n\r\n20.003B\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nXRP-USD\r\n\r\n\r\nXRP USD\r\n\r\n\r\n0.354907\r\n\r\n\r\n0.000657\r\n\r\n\r\n+0.19%\r\n\r\n\r\n17.684B\r\n\r\n\r\n626.395M\r\n\r\n\r\n626.395M\r\n\r\n\r\n626.395M\r\n\r\n\r\n49.826B\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nADA-USD\r\n\r\n\r\nCardano USD\r\n\r\n\r\n0.514886\r\n\r\n\r\n0.003404\r\n\r\n\r\n+0.67%\r\n\r\n\r\n17.6B\r\n\r\n\r\n767.03M\r\n\r\n\r\n767.03M\r\n\r\n\r\n767.03M\r\n\r\n\r\n34.182B\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nSOL-USD\r\n\r\n\r\nSolana USD\r\n\r\n\r\n34.56\r\n\r\n\r\n-0.140000\r\n\r\n\r\n-0.40%\r\n\r\n\r\n12.209B\r\n\r\n\r\n659.81M\r\n\r\n\r\n659.81M\r\n\r\n\r\n659.81M\r\n\r\n\r\n353.284M\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nDOT-USD\r\n\r\n\r\nPolkadot USD\r\n\r\n\r\n7.7056\r\n\r\n\r\n-0.024200\r\n\r\n\r\n-0.31%\r\n\r\n\r\n8.597B\r\n\r\n\r\n253.423M\r\n\r\n\r\n253.423M\r\n\r\n\r\n253.423M\r\n\r\n\r\n1.116B\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nIt’s enough for us to select the Symbol and % Change columns.\r\n\r\n\r\nmaster <- df %>% \r\n  select(1,5) %>% \r\n  rename(\"Change\"=2)\r\n\r\n\r\n\r\nI’d like to draw your attention to the fact that the values in the\r\nChange column are not numeric format.\r\n\r\n\r\nstr(master)\r\n\r\n\r\ntibble [100 x 2] (S3: tbl_df/tbl/data.frame)\r\n $ Symbol: chr [1:100] \"BTC-USD\" \"ETH-USD\" \"USDT-USD\" \"USDC-USD\" ...\r\n $ Change: chr [1:100] \"+1.51%\" \"+2.41%\" \"-0.00%\" \"+0.01%\" ...\r\n\r\nWe need to remove the plus and percent signs from the column I\r\nmentioned and convert the values to numeric format.\r\n\r\n\r\nmaster <- master %>% \r\n  mutate(\r\n    Change = as.numeric(gsub(\"[^0-9\\\\.]\",\"\",Change))\r\n  )\r\n\r\n\r\n\r\nThere, it is done!\r\n\r\n\r\nstr(master)\r\n\r\n\r\ntibble [100 x 2] (S3: tbl_df/tbl/data.frame)\r\n $ Symbol: chr [1:100] \"BTC-USD\" \"ETH-USD\" \"USDT-USD\" \"USDC-USD\" ...\r\n $ Change: num [1:100] 1.51 2.41 0 0.01 0.29 0.01 0.19 0.67 0.4 0.31 ...\r\n\r\nBefore proceeding, I should point out that we will need to create a\r\nnew column and get the dates as IDs as well.\r\n\r\n\r\nmaster <- master %>% \r\n  mutate(\r\n    ID = Sys.time()\r\n  )\r\n\r\n\r\n\r\nAnd now we can move on to the data visualization step.\r\n\r\n\r\ngainers <- master %>% \r\n  arrange(desc(Change)) %>% \r\n  filter(Change > 0) %>% \r\n  slice(1:10)\r\n\r\nif(nrow(gainers) > 0){\r\n  g <- ggplot(gainers, aes(x = reorder(Symbol, Change), y = Change, fill = Change)) +\r\n    geom_col() +\r\n    coord_flip() +\r\n    theme_minimal() +\r\n    theme(\r\n      plot.background = element_rect(fill = \"#262626\"),\r\n      panel.background = element_blank(),\r\n      panel.grid = element_line(color = \"#4c4c4c\"),\r\n      axis.title = element_blank(),\r\n      axis.text = element_text(color = \"#ffffff\", size = 15),\r\n      legend.position = \"none\",\r\n      plot.title = element_text(color = \"#ffffff\", size = 20, face = \"bold\"),\r\n      plot.subtitle = element_text(color = \"#ffffff\", size = 13, face = \"italic\")\r\n    ) +\r\n    scale_fill_gradient(low = \"red\", high = \"green\") +\r\n    labs(title = \"Top 10 Gainers\",\r\n         subtitle = gainers$ID[1])\r\n}\r\n\r\nggsave(\"g.png\")\r\n\r\n\r\n\r\n\r\n\r\n\r\nThe following are the codes that we have written thus far.\r\n\r\n\r\nlibrary(rvest)\r\nlibrary(tidyverse)\r\n\r\nurl <- \"https://finance.yahoo.com/cryptocurrencies/?count=100&offset=0\"\r\n\r\ndf <- read_html(url) %>% \r\n  html_table() %>% \r\n  .[[1]]\r\n\r\nmaster <- df %>% \r\n  select(1,5) %>% \r\n  rename(\"Change\"=2) %>% \r\n  mutate(\r\n    Change = as.numeric(gsub(\"[^0-9\\\\.]\",\"\",Change)),\r\n    ID = Sys.time()\r\n  )\r\n\r\ngainers <- master %>% \r\n  arrange(desc(Change)) %>% \r\n  filter(Change > 0) %>% \r\n  slice(1:10)\r\n\r\nif(nrow(gainers) > 0){\r\n  g <- ggplot(gainers, aes(x = reorder(Symbol, Change), y = Change, fill = Change)) +\r\n    geom_col() +\r\n    coord_flip() +\r\n    theme_minimal() +\r\n    theme(\r\n      plot.background = element_rect(fill = \"#262626\"),\r\n      panel.background = element_blank(),\r\n      panel.grid = element_line(color = \"#4c4c4c\"),\r\n      axis.title = element_blank(),\r\n      axis.text = element_text(color = \"#ffffff\", size = 15),\r\n      legend.position = \"none\",\r\n      plot.title = element_text(color = \"#ffffff\", size = 20, face = \"bold\"),\r\n      plot.subtitle = element_text(color = \"#ffffff\", size = 13, face = \"italic\")\r\n    ) +\r\n    scale_fill_gradient(low = \"red\", high = \"green\") +\r\n    labs(title = \"Top 10 Gainers\",\r\n         subtitle = gainers$ID[1])\r\n}\r\n\r\nggsave(\"g.png\")\r\n\r\n\r\n\r\nWriting the data to the database.\r\nSQLite will be used in this study. My reasoning for choosing SQLite\r\nis that we will not require any software or server setup. It also has a\r\nsimple structure. For SQLite, we will use the DBI and RSQLite packages,\r\nand the following command will be used to create the database (not\r\ntemporary; permanent). Wherever the address is, the file path will save\r\nthe database.\r\n\r\n\r\n# working directory\r\n# getwd()\r\n\r\nmyDB <- dbConnect(SQLite(), \"cryptoDB.sqlite\") # \"C:/.../cryptoDB.sqlite\"\r\n\r\n\r\n\r\nLet’s write the data to the database named cryptoDB. When we do this\r\non a regular basis, say every 5 minutes, we will be careful not to\r\noverwrite the table in the database with new data. The append parameter\r\nis set to TRUE for this purpose.\r\n\r\n\r\ndbWriteTable(myDB, \"master\", master, append = TRUE)\r\n\r\n\r\n\r\nLet’s get the data from the database using SQL.\r\n\r\n\r\nmastertbl <- dbGetQuery(myDB, \"SELECT * FROM master\")\r\n\r\n\r\n\r\n\r\n\r\nSymbol\r\n\r\n\r\nChange\r\n\r\n\r\nID\r\n\r\n\r\nBTC-USD\r\n\r\n\r\n1.29\r\n\r\n\r\n1662888193\r\n\r\n\r\nETH-USD\r\n\r\n\r\n2.46\r\n\r\n\r\n1662888193\r\n\r\n\r\nUSDT-USD\r\n\r\n\r\n0.01\r\n\r\n\r\n1662888193\r\n\r\n\r\nUSDC-USD\r\n\r\n\r\n0.01\r\n\r\n\r\n1662888193\r\n\r\n\r\nBNB-USD\r\n\r\n\r\n0.04\r\n\r\n\r\n1662888193\r\n\r\n\r\nBUSD-USD\r\n\r\n\r\n0.00\r\n\r\n\r\n1662888193\r\n\r\n\r\nXRP-USD\r\n\r\n\r\n0.17\r\n\r\n\r\n1662888193\r\n\r\n\r\nADA-USD\r\n\r\n\r\n0.63\r\n\r\n\r\n1662888193\r\n\r\n\r\nSOL-USD\r\n\r\n\r\n0.00\r\n\r\n\r\n1662888193\r\n\r\n\r\nDOT-USD\r\n\r\n\r\n0.20\r\n\r\n\r\n1662888193\r\n\r\n\r\nWhen the processes are finished, use the code below to exit the\r\ndatabase. It will be reconnected with the code that was written at the\r\nstart (comment line below).\r\n\r\n\r\ndbDisconnect(myDB)\r\n\r\n# myDB <- dbConnect(SQLite(), \"cryptoDB.sqlite\")\r\n\r\n\r\n\r\nThe following are the codes that we have written thus far.\r\n\r\n\r\nlibrary(DBI)\r\nlibrary(RSQLite)\r\n\r\nmyDB <- dbConnect(SQLite(), \"cryptoDB.sqlite\") # \"C:/.../cryptoDB.sqlite\"\r\ndbWriteTable(myDB, \"master\", master, append = TRUE)\r\ndbDisconnect(myDB)\r\n\r\n\r\n\r\nSchedule the task scheduler to run the script on a regular\r\nbasis.\r\nThe task scheduler can be configured via the PC or the taskscheduleR\r\npackage.\r\n\r\n\r\ntaskscheduler_create(\r\n  \r\n  taskname = \"Post27\",\r\n  rscript = \"post27.R\", # \"C:/.../\"\r\n  schedule = \"MINUTE\",\r\n  starttime = \"12:00\",\r\n  modifier = 5\r\n  \r\n)\r\n\r\n\r\n\r\nThe process has been automated.\r\nWhat can be done next?\r\nBy converting the .R extension to the .Rmd extension, the report\r\ncan be sent.\r\nE-mail can be used to send outputs with the .R or .Rmd\r\nextensions.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-09-11-post27/post27_files/figure-html5/unnamed-chunk-10-1.png",
    "last_modified": "2022-09-11T13:15:15+03:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-09-09-post26/",
    "title": "Make Plot Look Like a Reuters Plot",
    "description": "Making a Reuters style chart.",
    "author": [
      {
        "name": "A. Uraz Akgül",
        "url": {}
      }
    ],
    "date": "2022-09-09",
    "categories": [
      "Data Visualization"
    ],
    "contents": "\r\nIt is critical to convey data through beautiful visualizations in\r\naddition to being rich in data. Reuters is a platform where I can\r\nprovide an example of what I just said.\r\nIn this post, we will use ggplot2 to create a Reuters-style chart.\r\nI’d like to emphasize that our goal is not to make exactly the same\r\nthing, but to capture the main style.\r\n\r\n\r\n\r\nYou are free to use any data you want, but if you want to use the\r\ndata from this post, download the post26.xlsx file here.\r\n\r\n\r\nlibrary(tidyverse)\r\n\r\ndf <- readxl::read_excel(\"data.xlsx\") %>% \r\n  mutate(\r\n    date = lubridate::ymd(date)\r\n  )\r\n\r\n\r\n\r\n\r\n\r\nggplot(df, aes(x = date, y = close)) +\r\n  geom_line() +\r\n  theme_minimal()\r\n\r\n\r\n\r\n\r\nMaking the line orange could be a good starting point.\r\n\r\n\r\nggplot(df, aes(x = date, y = close)) +\r\n  geom_line(color = \"#ffa500\") +\r\n  theme_minimal()\r\n\r\n\r\n\r\n\r\nDarken the background.\r\n\r\n\r\nggplot(df, aes(x = date, y = close)) +\r\n  geom_line(color = \"#ffa500\") +\r\n  theme_minimal() +\r\n  theme(\r\n    plot.background = element_rect(fill = \"#262626\")\r\n  )\r\n\r\n\r\n\r\n\r\nChange the colors of the grid lines.\r\n\r\n\r\nggplot(df, aes(x = date, y = close)) +\r\n  geom_line(color = \"#ffa500\") +\r\n  theme_minimal() +\r\n  theme(\r\n    plot.background = element_rect(fill = \"#262626\"),\r\n    panel.grid = element_line(color = \"#4c4c4c\")\r\n  )\r\n\r\n\r\n\r\n\r\nMake the axis texts white, remove the axis titles and place y-axis on\r\nthe right.\r\n\r\n\r\nggplot(df, aes(x = date, y = close)) +\r\n  geom_line(color = \"#ffa500\") +\r\n  theme_minimal() +\r\n  theme(\r\n    plot.background = element_rect(fill = \"#262626\"),\r\n    panel.grid = element_line(color = \"#4c4c4c\"),\r\n    axis.title = element_blank(),\r\n    axis.text = element_text(color = \"#ffffff\")\r\n  ) +\r\n  scale_y_continuous(position = \"right\")\r\n\r\n\r\n\r\n\r\nFinally, we may have made the most significant contribution here. The\r\narea under the line will be filled and its transparency will be adjusted\r\nin this step.\r\n\r\n\r\nggplot(df, aes(x = date, y = close)) +\r\n  geom_line(color = \"#ffa500\", size = 1, alpha = 0.9) +\r\n  geom_area(alpha = 0.1, fill = \"#ffa500\") +\r\n  theme(\r\n    plot.background = element_rect(fill = \"#262626\"),\r\n    panel.background = element_blank(),\r\n    panel.grid = element_line(color = \"#4c4c4c\"),\r\n    axis.title = element_blank(),\r\n    axis.text = element_text(color = \"#ffffff\", size = 20)\r\n  ) +\r\n  scale_y_continuous(position = \"right\")\r\n\r\n\r\n\r\n\r\nSomeone looking at the chart will most likely identify it as a\r\nReuters type chart! :-)\r\nDon’t forget about your contributions!\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-09-09-post26/post26_files/figure-html5/unnamed-chunk-8-1.png",
    "last_modified": "2022-09-09T23:23:22+03:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-09-06-post25/",
    "title": "Adding a Shadow to a Line Chart in ggplot2",
    "description": "Shadowing your ggplot lines.",
    "author": [
      {
        "name": "A. Uraz Akgül",
        "url": {}
      }
    ],
    "date": "2022-09-06",
    "categories": [
      "Data Visualization"
    ],
    "contents": "\r\nLine charts with shadows let one feel in control. I’ll use the weekly\r\ndeath statistics from Istanbul to illustrate what I mean.\r\nThe data can be accessed by downloading the post25.xlsx file\r\nhere\r\nor visiting this\r\nsite.\r\n\r\n\r\nlibrary(tidyverse)\r\n\r\ndf <- readxl::read_excel(\"data.xlsx\") %>% \r\n  select(-day,-y2020,-y2021) %>% \r\n  pivot_longer(!week, names_to = \"years\", values_to = \"values\") %>% \r\n  group_by(week,years) %>% \r\n  summarise(total = sum(values)) %>% \r\n  ungroup() %>% \r\n  pivot_wider(names_from = \"years\", values_from = \"total\")\r\n\r\n\r\n\r\nLet’s see the lines before adding a shadow.\r\n\r\n\r\ndf %>% \r\n  pivot_longer(!week, names_to = \"years\", values_to = \"values\") %>% \r\n  ggplot(aes(x = week, y = values, group = years, color = years)) +\r\n  geom_line() +\r\n  ggthemes::theme_fivethirtyeight() +\r\n  theme(axis.text = element_text(size = 20),\r\n        strip.text = element_text(size = 20),\r\n        legend.title = element_blank(),\r\n        legend.text = element_text(size = 20),\r\n        legend.key.size = unit(1, 'cm'))\r\n\r\n\r\n\r\n\r\nFirst, we’ll create a ribbon by determining the minimum and maximum\r\nvalues by week from deaths between 2015 and 2019.\r\n\r\n\r\ndf2 <- df %>% \r\n  mutate(\r\n    normal_min = apply(X = .[,2:6], MARGIN = 1, FUN = function(x) min(x)),\r\n    normal_max = apply(X = .[,2:6], MARGIN = 1, FUN = function(x) max(x)),\r\n    normal_mean = apply(X = .[,2:6], MARGIN = 1, FUN = function(x) mean(x))\r\n  )\r\n\r\n\r\n\r\nThe following things are easier after the above step. We can add\r\nshadowed areas to our lines using geom_ribbon().\r\n\r\n\r\nlastWeek <- df2 %>% \r\n  pull(y2022) %>% \r\n  na.omit() %>% \r\n  length(.)\r\n\r\nggplot(df2, aes(x = week)) +\r\n  geom_ribbon(\r\n    aes(\r\n      ymin = normal_min,\r\n      ymax = normal_max\r\n    ),\r\n    fill = \"dark blue\",\r\n    alpha = .5\r\n  ) +\r\n  geom_line(aes(y = normal_mean)) +\r\n  geom_line(aes(y = y2022), color = \"dark red\", size = 2) +\r\n  geom_vline(xintercept = lastWeek, linetype = \"dotdash\", size = 1) +\r\n  ggthemes::theme_fivethirtyeight() +\r\n  theme(axis.text = element_text(size = 20),\r\n        axis.title.x = element_text(size = 20)) +\r\n  scale_x_continuous(breaks = c(1,lastWeek,52)) +\r\n  scale_y_continuous(labels = scales::comma) +\r\n  labs(\r\n    title = \"Weekly Deaths in Istanbul (from all causes), 2015-2019 vs 2022\",\r\n    x = \"Weeks\"\r\n  )\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-09-06-post25/post25_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2022-09-06T19:03:20+03:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-09-05-post24/",
    "title": "Plotting Multiple Groups in ggplot2 Using facet_wrap() in Two Ways",
    "description": "How to split groups using facet_wrap() in ggplot2.",
    "author": [
      {
        "name": "A. Uraz Akgül",
        "url": {}
      }
    ],
    "date": "2022-09-05",
    "categories": [
      "Data Visualization"
    ],
    "contents": "\r\nIn this post, I’ll show you how to split groups using facet_wrap() in\r\nggplot2. I’d like to point out that I prefer the second method over the\r\nfirst because it is more informative.\r\nYou can access the data we’ll be using by downloading the\r\npost24.xlsx file from here\r\nor by following the steps below.\r\nhttps://evds2.tcmb.gov.tr/index.php?/evds/serieMarket\r\nInterest Rates\r\nWeighted Average Interest Rates For Banks Loans (Flow Data, %)).\r\nSelect the following items and click the Add button:\r\nPersonal (TRY)(Flow Data, %)\r\nVehicle (TRY)(Flow Data, %)\r\nHousing (TRY)(Flow Data, %)\r\nCommercial (TRY)(Flow Data, %)\r\nDate From: 01-01-2019, Date To: 26-08-2022. Click the Create\r\nReport icon under Report Settings.\r\nYou’ll see the excel icon.\r\n\r\n\r\nlibrary(tidyverse)\r\n\r\ndf <- readxl::read_excel(\"data.xlsx\") %>% \r\n  pivot_longer(!date, names_to = \"vars\", values_to = \"vals\") %>% \r\n  mutate(date = lubridate::dmy(date))\r\n\r\n\r\n\r\n0. Before splitting groups with facet_wrap()\r\n\r\n\r\nggplot(df, aes(x = date, y = vals, group = vars, color = vars)) +\r\n  geom_line(size = 1) +\r\n  ggthemes::theme_fivethirtyeight() +\r\n  theme(legend.title = element_blank(),\r\n        legend.position = \"top\",\r\n        legend.key.size = unit(1, 'cm'),\r\n        legend.text = element_text(size = 20),\r\n        axis.text = element_text(size = 20)) +\r\n  labs(\r\n    title = \"Weighted Average Interest Rates For Banks Loans\"\r\n  )\r\n\r\n\r\n\r\n\r\n1. Splitting groups with facet_wrap()\r\n\r\n\r\nggplot(df, aes(x = date, y = vals, group = vars, color = vars)) +\r\n  geom_line(size = 1) +\r\n  ggthemes::theme_fivethirtyeight() +\r\n  theme(strip.text = element_text(size = 20),\r\n        legend.position = \"none\",\r\n        axis.text = element_text(size = 20)) +\r\n  facet_wrap(~vars) +\r\n  labs(\r\n    title = \"Weighted Average Interest Rates For Banks Loans\"\r\n  )\r\n\r\n\r\n\r\n\r\n2. Splitting groups with facet_wrap() using other\r\nvariables\r\nWhen making comparisons between variables, I would prefer the second\r\noption most of the time.\r\n\r\n\r\nggplot(df, aes(x = date, y = vals)) +\r\n  geom_line(data = df %>% rename(vars2 = vars), aes(group = vars2), color = \"gray\", size = 1) +\r\n  geom_line(color = \"dark blue\", size = 2) +\r\n  ggthemes::theme_fivethirtyeight() +\r\n  theme(strip.text = element_text(size = 20),\r\n        axis.text = element_text(size = 20)) +\r\n  facet_wrap(~vars, ncol = 2) +\r\n  labs(\r\n    title = \"Weighted Average Interest Rates For Banks Loans\"\r\n  )\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-09-05-post24/post24_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2022-09-06T09:35:17+03:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-09-04-post23/",
    "title": "6 Types of Investments: What Has Made You the Most Money?",
    "description": "Hands-on with performing data manipulation.",
    "author": [
      {
        "name": "A. Uraz Akgül",
        "url": {}
      }
    ],
    "date": "2022-09-04",
    "categories": [
      "Data Manipulation",
      "Finance"
    ],
    "contents": "\r\nAlthough I’ll focus on data manipulation rather than the question in\r\nthis post, at the end of the post we’ll have answered an important\r\nquestion. Let’s define data manipulation briefly before answering the\r\nquestion.\r\nData manipulation is the process of changing data to make it more\r\nreadable and organized. We manipulate data in order to analyze and\r\nvisualize it.\r\nThe data for this study were obtained from TURKSTAT. You can access\r\nthe data by downloading post23.xls file here.\r\nThe metadata can be found here.\r\nImporting the dataset:\r\n\r\n\r\ndf <- readxl::read_excel(\"data.xls\")\r\n\r\n\r\nI bet you can’t do analysis with this dataset! :-) In this case we\r\nconvert it into an analysis-ready format.\r\n\r\n\r\nlibrary(dplyr) # Our main package\r\nlibrary(ggplot2)\r\n\r\n\r\nBecause annual returns adjusted for CPI will be used, the relevant\r\ncolumns, including the years and months, will be chosen. The column\r\npositions are in the following order: 1, 17, 33, 49, … So, 16 is the\r\ndifference between two numbers that follow one another. Don’t forget the\r\nmonths in column 2!\r\n\r\n\r\ndf2 <- df %>% \r\n  select(2, seq(1,ncol(.),16)) # select(2,1,17,33,49,65,81,97)\r\n\r\n\r\n\r\n\r\n…2\r\n\r\n\r\nFinansal yatırım araçlarının yıllara göre dönemsel reel getiri oranları\r\n\r\n\r\n…17\r\n\r\n\r\n…33\r\n\r\n\r\n…49\r\n\r\n\r\n…65\r\n\r\n\r\n…81\r\n\r\n\r\n…97\r\n\r\n\r\nNA\r\n\r\n\r\nThe rates of profits created by means of financial invesment\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\n(%)\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nNA\r\n\r\n\r\nAy Months\r\n\r\n\r\nYıl Year\r\n\r\n\r\nTÜFE CPI\r\n\r\n\r\nTÜFE CPI\r\n\r\n\r\nTÜFE CPI\r\n\r\n\r\nTÜFE CPI\r\n\r\n\r\nTÜFE CPI\r\n\r\n\r\nTÜFE CPI\r\n\r\n\r\nOcak January\r\n\r\n\r\n1997\r\n\r\n\r\n10\r\n\r\n\r\n61.299999999999997\r\n\r\n\r\n5.5999999999999996\r\n\r\n\r\n-3.7999999999999998\r\n\r\n\r\n-5.9000000000000004\r\n\r\n\r\nNA\r\n\r\n\r\nŞubat February\r\n\r\n\r\n1997\r\n\r\n\r\n9\r\n\r\n\r\n65.200000000000003\r\n\r\n\r\n4.7000000000000002\r\n\r\n\r\n-8\r\n\r\n\r\n-9.5999999999999996\r\n\r\n\r\nNA\r\n\r\n\r\nMart March\r\n\r\n\r\n1997\r\n\r\n\r\n8.6999999999999993\r\n\r\n\r\n31.600000000000001\r\n\r\n\r\n3.2000000000000002\r\n\r\n\r\n-10.1\r\n\r\n\r\n-11.800000000000001\r\n\r\n\r\nNA\r\n\r\n\r\nNisan April\r\n\r\n\r\n1997\r\n\r\n\r\n8.1999999999999993\r\n\r\n\r\n25.699999999999999\r\n\r\n\r\n1.8999999999999999\r\n\r\n\r\n-10.800000000000001\r\n\r\n\r\n-10.5\r\n\r\n\r\nNA\r\n\r\n\r\nThe first 6 lines can be removed.\r\n\r\n\r\ndf2 <- df2 %>% \r\n  slice(-c(1:6))\r\n\r\n\r\nChanging column names:\r\n\r\n\r\ndf2 <- df2 %>% \r\n  rename(\r\n    \"Months\"=1,\r\n    \"Years\"=2,\r\n    \"DepositInterest\"=3,\r\n    \"StockExchange\"=4,\r\n    \"USDollar\"=5,\r\n    \"Euro\"=6,\r\n    \"GoldIngot\"=7,\r\n    \"GDDI\"=8 # Government Domestic Debt Instruments\r\n  )\r\n\r\n\r\nThe Months column contains two month names, one in English and one in\r\nTurkish. We’ll go with the English ones.\r\nI’d like to draw your attention to the fact that there is a line\r\nbreak between the month names in each line. Click the icon to the left\r\nof the df2 in the environment to see it.\r\n\r\n\r\ndf2$Months[1]\r\n\r\n[1] \"Ocak\\nJanuary\"\r\n\r\nThese line breaks need to be eliminated.\r\n\r\n\r\ndf2 <- df2 %>% \r\n  mutate(\r\n    Months = stringr::str_replace_all(Months, \"[\\n]\" , \" \")\r\n  )\r\n\r\n\r\n\r\n\r\ndf2$Months[1]\r\n\r\n[1] \"Ocak January\"\r\n\r\nExtracting the second word from each line:\r\n\r\n\r\ndf2 <- df2 %>% \r\n  mutate(\r\n    Months = stringr::word(Months,2,2)\r\n  )\r\n\r\n\r\n\r\n\r\ndf2$Months[1]\r\n\r\n[1] \"January\"\r\n\r\nCombining month and year into a date format column and removing the\r\nMonths and Years columns:\r\n\r\n\r\ndf2 <- df2 %>% \r\n  mutate(\r\n    Date = lubridate::ymd(paste0(Years,\"-\",Months,\"-\",1)), .before = Months\r\n  ) %>% \r\n  select(-c(Months,Years))\r\n\r\n\r\nWe can also remove lines that contain NA (Not Available).\r\n\r\n\r\ndf2 <- df2 %>% \r\n  na.omit()\r\n\r\n\r\nWe’ll convert each column to numeric format, but some values have the\r\nletter e next to them.\r\n\r\n\r\ndf2[211,6]\r\n\r\n# A tibble: 1 x 1\r\n  GoldIngot\r\n  <chr>    \r\n1 9,11(e)  \r\n\r\nLet’s remove the values containing the letter “e” with their\r\nparentheses.\r\ne: Data is corrected.\r\n\r\n\r\ndf2 <- df2 %>% \r\n  mutate_at(\r\n    vars(-Date), function(x) gsub(\"\\\\(.*\", \"\", x)\r\n  )\r\n\r\n\r\nExcept for the Date column, all columns can be converted to\r\nnumbers.\r\nWait a minute! There are several values that require the conversion\r\nof commas to dots.\r\n\r\n\r\ndf2 <- df2 %>% \r\n  mutate_at(\r\n    vars(-Date), function(x) stringr::str_replace_all(x, \",\", \".\")\r\n  ) %>% \r\n  mutate_if(is.character,as.numeric)\r\n\r\n\r\n\r\n\r\ndf2[211,6]\r\n\r\n# A tibble: 1 x 1\r\n  GoldIngot\r\n      <dbl>\r\n1      9.11\r\n\r\n\r\n\r\nDate\r\n\r\n\r\nDepositInterest\r\n\r\n\r\nStockExchange\r\n\r\n\r\nUSDollar\r\n\r\n\r\nEuro\r\n\r\n\r\nGoldIngot\r\n\r\n\r\nGDDI\r\n\r\n\r\n2021-10-01\r\n\r\n\r\n-7.46\r\n\r\n\r\n2.36\r\n\r\n\r\n-3.22\r\n\r\n\r\n-4.65\r\n\r\n\r\n-9.78\r\n\r\n\r\n-12.17\r\n\r\n\r\n2021-11-01\r\n\r\n\r\n-7.72\r\n\r\n\r\n9.77\r\n\r\n\r\n10.35\r\n\r\n\r\n6.36\r\n\r\n\r\n8.06\r\n\r\n\r\n-13.70\r\n\r\n\r\n2021-12-01\r\n\r\n\r\n-15.73\r\n\r\n\r\n4.00\r\n\r\n\r\n29.19\r\n\r\n\r\n19.99\r\n\r\n\r\n25.43\r\n\r\n\r\n-26.37\r\n\r\n\r\n2022-01-01\r\n\r\n\r\n-22.75\r\n\r\n\r\n-11.67\r\n\r\n\r\n23.08\r\n\r\n\r\n14.45\r\n\r\n\r\n19.70\r\n\r\n\r\n-32.69\r\n\r\n\r\n2022-02-01\r\n\r\n\r\n-25.50\r\n\r\n\r\n-15.28\r\n\r\n\r\n24.86\r\n\r\n\r\n17.02\r\n\r\n\r\n28.13\r\n\r\n\r\n-33.37\r\n\r\n\r\n2022-03-01\r\n\r\n\r\n-27.73\r\n\r\n\r\n-12.50\r\n\r\n\r\n18.14\r\n\r\n\r\n9.40\r\n\r\n\r\n33.59\r\n\r\n\r\n-35.98\r\n\r\n\r\n2022-04-01\r\n\r\n\r\n-31.42\r\n\r\n\r\n2.59\r\n\r\n\r\n6.04\r\n\r\n\r\n-3.98\r\n\r\n\r\n17.31\r\n\r\n\r\n-34.68\r\n\r\n\r\n2022-05-01\r\n\r\n\r\n-32.86\r\n\r\n\r\n-2.17\r\n\r\n\r\n8.31\r\n\r\n\r\n-5.60\r\n\r\n\r\n8.76\r\n\r\n\r\n-37.08\r\n\r\n\r\n2022-06-01\r\n\r\n\r\n-35.02\r\n\r\n\r\n0.21\r\n\r\n\r\n10.46\r\n\r\n\r\n-3.09\r\n\r\n\r\n10.40\r\n\r\n\r\n-36.34\r\n\r\n\r\n2022-07-01\r\n\r\n\r\n-35.26\r\n\r\n\r\n0.52\r\n\r\n\r\n13.06\r\n\r\n\r\n-2.45\r\n\r\n\r\n9.11\r\n\r\n\r\n-34.81\r\n\r\n\r\nBefore performing the calculation that will take place soon, the\r\ntable needs to be converted from wide to long format.\r\n\r\n\r\ndf2 <- df2 %>% \r\n  tidyr::pivot_longer(!Date, names_to = \"Types\", values_to = \"Returns\") %>% \r\n  arrange(Types)\r\n\r\n\r\nCalculating the average rate of return on an investment that is\r\ncompounded over several periods is possible using the geometric\r\naverage return formula, also known as geometric mean\r\nreturn.\r\nGeometric Average Return = \\(\\sqrt[n]{(1+r_1)x(1+r_2)x...x(1+r_n)}-1\\)\r\nr: Rate of return\r\nn: Number of periods\r\nTo calculate the compound average return, we first add 1.00 to each\r\nannual return. Then the above formula can be applied.\r\n\r\n\r\ngar <- df2 %>% \r\n  mutate(\r\n    Returns2 = 1 + Returns/100\r\n  ) %>% \r\n  group_by(Types) %>% \r\n  summarise(\r\n    GAR = ((prod(Returns2)^(1/211))-1)*100\r\n  ) %>% \r\n  ungroup()\r\n\r\n\r\n\r\n\r\nggplot(gar, aes(x = reorder(Types,GAR), y = GAR, fill = GAR)) +\r\n  geom_col() +\r\n  geom_text(aes(label = paste0(round(GAR,digits = 1),\"%\")), hjust = 1, size = 4) +\r\n  theme_minimal() +\r\n  theme(legend.position = \"none\",\r\n        axis.title = element_blank(),\r\n        axis.text = element_text(size = 15),\r\n        plot.title = element_text(face = \"bold\", size = 15, hjust = 0.5)) +\r\n  scale_fill_gradient(low = \"orange\", high = \"red\") +\r\n  coord_flip() +\r\n  labs(\r\n    title = \"Geometric Average Returns\"\r\n  )\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-09-04-post23/post23_files/figure-html5/unnamed-chunk-21-1.png",
    "last_modified": "2022-09-04T16:18:06+03:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-08-30-post22/",
    "title": "Using Cosine Similarity to Build a Movie Recommendation System",
    "description": "Create your own dataset and build your own movie recommender system.",
    "author": [
      {
        "name": "A. Uraz Akgül",
        "url": {}
      }
    ],
    "date": "2022-08-30",
    "categories": [
      "Machine Learning",
      "Text",
      "Web"
    ],
    "contents": "\r\nHave you ever fallen into a void after finishing a movie and tried to\r\nfind similar ones? After a movie that impressed me, I immediately try to\r\nfind alternatives. If we really enjoy working with data and are good at\r\nalgorithms, why not build our own system?\r\nWe used the cosine similarity metric to achieve our results. I think\r\nI’ve been obsessed with it lately. I use it especially when I study on\r\ntext similarities. In this study, we are going to use the method I\r\nmentioned. Let’s dive into it.\r\nCosine similarity measures the similarity between two vectors and is\r\nthe cosine of the angle between two vectors. The smaller the angle\r\nbetween two vectors, the more similar they are to each other.\r\nConsider two vectors, x and y. We can calculate the cosine similarity\r\nbetween the vectors as follows:\r\n\\(cos(\\theta) = \\frac{x.y}{||x||||y||} =\r\n\\frac{\\sum_{i=1}^{n}x_iy_i}{\\sqrt{\\sum_{i=1}^{n}x_i^2}\\sqrt{\\sum_{i=1}^{n}y_i^2}}\\)\r\nSee how it’s calculated with a simple example.\r\n\r\n\r\nx <- c(3,4,1,0)\r\ny <- c(3,4,4,8)\r\n\r\n\r\n\\(x.y = 3*3 + 4*4 + 1*4 + 0*8 =\r\n29\\)\r\n\\(||x|| = \\sqrt{3^2 + 4^2 + 1^2 + 0^2} =\r\n5.09902\\)\r\n\\(||y|| = \\sqrt{3^2 + 4^2 + 4^2 + 8^2} =\r\n10.24695\\)\r\n\\(cos(x,y) = cos(\\theta) =\r\n\\frac{29}{5.09902*10.24695} = 0.5550303\\)\r\nRemember: The smaller the angle between two vectors, the more similar\r\nthey are to each other. So, if the angle between vectors is zero\r\ndegrees, then the cosine similarity value is 1, which means the two\r\nvectors are similar or relevant. The similarity can be any value in the\r\nrange [−1,1]. In addition to this, the cosine distance is as:\r\n\\(Cosine\\ Distance\\ =\\ 1\\ -\\ Cosine\\\r\nSimilarity\\)\r\nCosine similarity can also be calculated with the help of the {lsa}\r\npackage.\r\n\r\n\r\nas.numeric(lsa::cosine(x,y))\r\n\r\n[1] 0.5550303\r\n\r\nSo, how can similarity be calculated when it comes to texts? Let me\r\ngive you an example.\r\n\r\n\r\ntext1 <- \"I am learning R programming language\"\r\ntext2 <- \"I am learning Python programming language\"\r\n\r\n\r\nWe can create two vectors with the word frequencies corresponding to\r\neach other as follows:\r\n\r\n\r\n# I, am, learning, R, Python, programming, language\r\n\r\ntext1_n <- c(1,1,1,1,0,1,1)\r\ntext2_n <- c(1,1,1,0,1,1,1)\r\n\r\nas.numeric(lsa::cosine(text1_n,text2_n))\r\n\r\n[1] 0.8333333\r\n\r\nWe can move on to building a movie recommender system. The data to be\r\nused in the study will be obtained by web scraping from IMDB.\r\nThe following steps can be followed:\r\nhttps://www.imdb.com/\r\nMenu/Browse Movies by Genre\r\nPopular Movies by Genre/Sci-Fi (Scroll down to see)\r\nhttps://www.imdb.com/search/title/?title_type=feature&genres=sci-fi&start=1&explore=genres&ref_=adv_nxt\r\nFocus on last URL. The value you see as 1 next to start will increase\r\nby 50 each time you switch to the next page. In fact, you do not\r\nencounter this URL at first, but when you move to the next page, you can\r\nobtain this URL and edit the first page yourself. The movie list starts\r\nwith the value you type in the value to the right of start.\r\nI want to get the first 100 URLs in the list sorted by\r\npopularity.\r\n\r\n\r\nlibrary(rvest)\r\nlibrary(tidyverse)\r\n\r\n\r\nURLs need to be created first.\r\n\r\n\r\nurls <- str_c(\r\n  \"https://www.imdb.com/search/title/?title_type=feature&genres=sci-fi&start=\",\r\n  seq(1,5000,50),\r\n  \"&explore=genres&ref_=adv_nxt\"\r\n)\r\n\r\nurls[c(1,length(urls))]\r\n\r\n[1] \"https://www.imdb.com/search/title/?title_type=feature&genres=sci-fi&start=1&explore=genres&ref_=adv_nxt\"   \r\n[2] \"https://www.imdb.com/search/title/?title_type=feature&genres=sci-fi&start=4951&explore=genres&ref_=adv_nxt\"\r\n\r\nWe are going to get the titles and descriptions of the movies using a\r\nfor loop. See the examples below for the first page.\r\n\r\n\r\nfirstURL <- urls[1]\r\n\r\ntitle <- read_html(firstURL) %>% \r\n  html_nodes(\"div.lister-item-content h3.lister-item-header a\") %>% \r\n  html_text()\r\n\r\nhead(title)\r\n\r\n[1] \"Nope\"                             \r\n[2] \"Prey\"                             \r\n[3] \"Samaritan\"                        \r\n[4] \"DC League of Super-Pets\"          \r\n[5] \"Thor: Love and Thunder\"           \r\n[6] \"Everything Everywhere All at Once\"\r\n\r\ndescription <- read_html(firstURL) %>% \r\n  html_nodes(\"p.text-muted\") %>% \r\n  html_text() %>% \r\n  .[c(FALSE,TRUE)] %>% \r\n  gsub(\"[\\n]\", \"\", .)\r\n\r\nhead(paste0(substr(description,1,100),\"...\"))\r\n\r\n[1] \"The residents of a lonely gulch in inland California bear witness to an uncanny and chilling discove...\"\r\n[2] \"The origin story of the Predator in the world of the Comanche Nation 300 years ago. Naru, a skilled ...\"\r\n[3] \"A young boy learns that a superhero who was thought to have gone missing after an epic battle twenty...\"\r\n[4] \"Krypto the Super-Dog and Superman are inseparable best friends, sharing the same superpowers and fig...\"\r\n[5] \"Thor enlists the help of Valkyrie, Korg and ex-girlfriend Jane Foster to fight Gorr the God Butcher,...\"\r\n[6] \"An aging Chinese immigrant is swept up in an insane adventure, where she alone can save the world by...\"\r\n\r\ndf <- data.frame(\r\n  title = title,\r\n  description = description\r\n)\r\n\r\n\r\n\r\n\r\ntitle\r\n\r\n\r\ndescription\r\n\r\n\r\nNope\r\n\r\n\r\nThe residents of a lonely gulch in inland California bear witness to an\r\nuncanny and chilling discovery.\r\n\r\n\r\nPrey\r\n\r\n\r\nThe origin story of the Predator in the world of the Comanche Nation 300\r\nyears ago. Naru, a skilled warrior, fights to protect her tribe against\r\none of the first highly-evolved Predators to land on Earth.\r\n\r\n\r\nSamaritan\r\n\r\n\r\nA young boy learns that a superhero who was thought to have gone missing\r\nafter an epic battle twenty years ago may in fact still be around.\r\n\r\n\r\nDC League of Super-Pets\r\n\r\n\r\nKrypto the Super-Dog and Superman are inseparable best friends, sharing\r\nthe same superpowers and fighting crime side by side in Metropolis.\r\nHowever, Krypto must master his own powers for a rescue mission when\r\nSuperman is kidnapped.\r\n\r\n\r\nThor: Love and Thunder\r\n\r\n\r\nThor enlists the help of Valkyrie, Korg and ex-girlfriend Jane Foster to\r\nfight Gorr the God Butcher, who intends to make the gods extinct.\r\n\r\n\r\nEverything Everywhere All at Once\r\n\r\n\r\nAn aging Chinese immigrant is swept up in an insane adventure, where she\r\nalone can save the world by exploring other universes connecting with\r\nthe lives she could have led.\r\n\r\n\r\nCreating the dataset…\r\n\r\n\r\nmaster <- data.frame()\r\n\r\nfor(i in seq_along(urls)){\r\n  \r\n  title <- read_html(urls[i]) %>% \r\n  html_nodes(\"div.lister-item-content h3.lister-item-header a\") %>% \r\n  html_text()\r\n  \r\n  description <- read_html(urls[i]) %>% \r\n  html_nodes(\"p.text-muted\") %>% \r\n  html_text() %>% \r\n  .[c(FALSE,TRUE)] %>% \r\n  gsub(\"[\\n]\", \"\", .)\r\n  \r\n  master <- master %>% \r\n    bind_rows(\r\n      data.frame(\r\n        title = title,\r\n        description = description\r\n      )\r\n    )\r\n  \r\n  #print(i)\r\n  Sys.sleep(1)\r\n  \r\n}\r\n\r\n\r\nA dataset should be examined after it is created, but in this work I\r\nignore this step. Ultimately, we have 5000 movies.\r\nFinally we can move on to the cosine similarity calculation. The\r\n{widyr} package can be used to calculate it. The 2 packages shown below\r\nwill be used.\r\n\r\n\r\nlibrary(tidytext)\r\nlibrary(widyr)\r\n\r\n\r\nWe are going to calculate cosine similarity based on two methods:\r\nWord Frequencies and TF-IDF.\r\n\r\n\r\nmaster2 <- master %>% \r\n  unnest_tokens(output = \"word\", input = \"description\") %>% \r\n  anti_join(get_stopwords()) %>% \r\n  count(word, title) %>% \r\n  bind_tf_idf(word, title, n)\r\n\r\n\r\nCosine Similarity - Word Frequencies:\r\n\r\n\r\nmaster_freq <- master2 %>% \r\n  pairwise_similarity(title, word, n, upper = FALSE, sort = TRUE) %>% \r\n  mutate(item = paste0(item1,\"---\",item2), .before = similarity) %>% \r\n  select(-c(item1,item2)) %>% \r\n  rename(\"similarity_freq\"=2)\r\n\r\n\r\nCosine Similarity - TF-IDF:\r\nTF-IDF stands for Term Frequency-Inverse Document Frequency, and it’s\r\nintended to measure how important a word is to a document in a\r\ncollection or corpus.\r\ni: term (word),\r\nj: document (set of words),\r\nN: count of corpus (the total document set)\r\ntf(i,j) = count of i in j / number of words in j\r\ndf(i) = occurrence of i in documents\r\nidf(i) = log(N/(df + 1))\r\ntf-idf(i,j) = tf(i,j) * log(N/(df + 1))\r\n\r\n\r\nmaster_tf_idf <- master2 %>% \r\n  pairwise_similarity(title, word, tf_idf, upper = FALSE, sort = TRUE) %>% \r\n  mutate(item = paste0(item1,\"---\",item2), .before = similarity) %>% \r\n  select(-c(item1,item2)) %>% \r\n  rename(\"similarity_tf_idf\"=2)\r\n\r\n\r\nI used Minority Report, of the Sci-Fi genre as the item to be used\r\nfor cosine similarity calculations when recommending the top 10\r\nmovies.\r\nBased on word frequency:\r\n\r\n\r\ndf_wf <- master_freq %>% \r\n  filter(grepl(\"Minority Report\",item)) %>% \r\n  top_n(10, similarity_freq) %>% \r\n  arrange(desc(similarity_freq))\r\n\r\n\r\n\r\n\r\nitem\r\n\r\n\r\nsimilarity_freq\r\n\r\n\r\nMinority Report—Judge Dredd\r\n\r\n\r\n0.3077935\r\n\r\n\r\nCyborg 2087—Minority Report\r\n\r\n\r\n0.2766858\r\n\r\n\r\nMinority Report—Omega Cop\r\n\r\n\r\n0.2601330\r\n\r\n\r\nLe dernier combat—Minority Report\r\n\r\n\r\n0.2545139\r\n\r\n\r\nPara iz buduschego—Minority Report\r\n\r\n\r\n0.2445580\r\n\r\n\r\nMinority Report—Eternity\r\n\r\n\r\n0.2445580\r\n\r\n\r\nMinority Report—Maayavan\r\n\r\n\r\n0.2433321\r\n\r\n\r\nMinority Report—Knights\r\n\r\n\r\n0.2433321\r\n\r\n\r\nMinority Report—Narcopolis\r\n\r\n\r\n0.2391824\r\n\r\n\r\nMinority Report—Trudno byt bogom\r\n\r\n\r\n0.2369396\r\n\r\n\r\nThe title with the closest similarity was Judge Dredd based on word\r\nfrequency.\r\nBased on TF-IDF:\r\n\r\n\r\ndf_tfidf <- master_tf_idf %>% \r\n  filter(grepl(\"Minority Report\",item)) %>% \r\n  top_n(10, similarity_tf_idf) %>% \r\n  arrange(desc(similarity_tf_idf))\r\n\r\n\r\n\r\n\r\nitem\r\n\r\n\r\nsimilarity_tf_idf\r\n\r\n\r\nMinority Report—Time Frame\r\n\r\n\r\n0.2416788\r\n\r\n\r\nMinority Report—Strange Behavior\r\n\r\n\r\n0.1993392\r\n\r\n\r\nMinority Report—Outpost 37\r\n\r\n\r\n0.1971800\r\n\r\n\r\nMinority Report—Resident Evil\r\n\r\n\r\n0.1873535\r\n\r\n\r\nMinority Report—Im Stahlnetz des Dr. Mabuse\r\n\r\n\r\n0.1752203\r\n\r\n\r\nKidô keisatsu patorebâ: The Movie 2—Minority Report\r\n\r\n\r\n0.1744674\r\n\r\n\r\nMinority Report—In the Shadow of the Moon\r\n\r\n\r\n0.1643750\r\n\r\n\r\nMinority Report—Time Under Fire\r\n\r\n\r\n0.1642217\r\n\r\n\r\nMinority Report—Judge Dredd\r\n\r\n\r\n0.1626650\r\n\r\n\r\nMinority Report—Maayavan\r\n\r\n\r\n0.1608787\r\n\r\n\r\nThe title with the closest similarity was Time Frame based on\r\nTF-IDF.\r\n\r\n\r\ndf_wf %>% \r\n  ggplot(aes(x = reorder(item, similarity_freq), y = similarity_freq, fill = similarity_freq)) +\r\n  geom_col() +\r\n  ggthemes::theme_fivethirtyeight() +\r\n  theme(legend.position = \"none\") +\r\n  scale_fill_gradient(low = \"orange\", high = \"red\") +\r\n  coord_flip() +\r\n  labs(title = \"Word Frequency\") -> g1\r\n\r\ndf_tfidf %>% \r\n  ggplot(aes(x = reorder(item, similarity_tf_idf), y = similarity_tf_idf, fill = similarity_tf_idf)) +\r\n  geom_col() +\r\n  ggthemes::theme_fivethirtyeight() +\r\n  theme(legend.position = \"none\") +\r\n  scale_fill_gradient(low = \"orange\", high = \"red\") +\r\n  coord_flip() +\r\n  labs(title = \"TF-IDF\") -> g2\r\n\r\n\r\n\r\n\r\ngridExtra::grid.arrange(g1,g2,ncol=2)\r\n\r\n\r\n\r\n\r\n\r\ntitle\r\n\r\n\r\ndescription\r\n\r\n\r\nMinority Report\r\n\r\n\r\nIn a future where a special police unit is able to arrest murderers\r\nbefore they commit their crimes, an officer from that unit is himself\r\naccused of a future murder.\r\n\r\n\r\nJudge Dredd\r\n\r\n\r\nIn a dystopian future, Joseph Dredd, the most famous Judge (a police\r\nofficer with instant field judiciary powers), is convicted for a crime\r\nhe did not commit and must face his murderous counterpart.\r\n\r\n\r\nTime Frame\r\n\r\n\r\nWhat if you were eleven hours away from being executed for a murder you\r\ndidn’t commit, and the only way to save yourself was to go back in time,\r\nand commit the murder?\r\n\r\n\r\nIf everything is clear so far, you can do more using your\r\nimagination. These studies don’t work without story and imagination.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-08-30-post22/post22_files/figure-html5/unnamed-chunk-19-1.png",
    "last_modified": "2022-08-30T17:42:30+03:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-08-28-post21/",
    "title": "Displaying Distributions of Continuous Data With 5 Different Charts",
    "description": "Histogram, Density, Violin, Boxplot, Ridgeline.",
    "author": [
      {
        "name": "A. Uraz Akgül",
        "url": {}
      }
    ],
    "date": "2022-08-28",
    "categories": [
      "Data Visualization"
    ],
    "contents": "\r\nBorsa Istanbul calculates variety of indices to follow the\r\nmovements of markets, to be an underlying asset for the financial\r\nproducts, and to be used as a benchmark for collective investment\r\ninstruments. Click here for\r\nfurther information.\r\nYou can access the data by downloading post21.xlsx file on\r\nmy\r\nGitHub account. The 12 BIST Indices were chosen by me. You can find\r\nall of BIST indices here. Reuters data is\r\nused.\r\n\r\n\r\nlibrary(tidyverse)\r\n\r\ndf <- readxl::read_excel(\"data.xlsx\") %>% \r\n  select(-DATE) %>% \r\n  mutate_all(\r\n    ., function(x) (lead(x)/x-1)*100\r\n  ) %>% \r\n  na.omit()\r\n\r\n\r\n\r\n\r\nhead(df)\r\n\r\n\r\n\r\n\r\nXU030\r\n\r\n\r\nXU050\r\n\r\n\r\nXU100\r\n\r\n\r\nXUTUM\r\n\r\n\r\nXBANK\r\n\r\n\r\nXBLSM\r\n\r\n\r\nXGIDA\r\n\r\n\r\nXHOLD\r\n\r\n\r\nXTRZM\r\n\r\n\r\nXULAS\r\n\r\n\r\nXUTEK\r\n\r\n\r\nXUSIN\r\n\r\n\r\n0.51\r\n\r\n\r\n0.53\r\n\r\n\r\n0.67\r\n\r\n\r\n0.65\r\n\r\n\r\n0.85\r\n\r\n\r\n-0.22\r\n\r\n\r\n0.03\r\n\r\n\r\n1.02\r\n\r\n\r\n-0.01\r\n\r\n\r\n-1.03\r\n\r\n\r\n0.29\r\n\r\n\r\n1.02\r\n\r\n\r\n0.91\r\n\r\n\r\n0.88\r\n\r\n\r\n0.95\r\n\r\n\r\n0.97\r\n\r\n\r\n2.58\r\n\r\n\r\n0.97\r\n\r\n\r\n1.35\r\n\r\n\r\n-0.04\r\n\r\n\r\n2.61\r\n\r\n\r\n-0.01\r\n\r\n\r\n0.74\r\n\r\n\r\n0.98\r\n\r\n\r\n-0.09\r\n\r\n\r\n-0.02\r\n\r\n\r\n0.14\r\n\r\n\r\n0.25\r\n\r\n\r\n-0.36\r\n\r\n\r\n0.68\r\n\r\n\r\n1.07\r\n\r\n\r\n0.13\r\n\r\n\r\n1.93\r\n\r\n\r\n1.55\r\n\r\n\r\n-0.20\r\n\r\n\r\n0.20\r\n\r\n\r\n0.30\r\n\r\n\r\n0.33\r\n\r\n\r\n0.34\r\n\r\n\r\n0.41\r\n\r\n\r\n0.48\r\n\r\n\r\n0.96\r\n\r\n\r\n0.16\r\n\r\n\r\n0.94\r\n\r\n\r\n0.81\r\n\r\n\r\n-0.01\r\n\r\n\r\n0.56\r\n\r\n\r\n0.32\r\n\r\n\r\n-0.88\r\n\r\n\r\n-0.75\r\n\r\n\r\n-0.71\r\n\r\n\r\n-0.49\r\n\r\n\r\n-0.55\r\n\r\n\r\n0.35\r\n\r\n\r\n0.29\r\n\r\n\r\n-0.76\r\n\r\n\r\n-0.66\r\n\r\n\r\n-0.11\r\n\r\n\r\n0.09\r\n\r\n\r\n-0.36\r\n\r\n\r\n0.36\r\n\r\n\r\n0.36\r\n\r\n\r\n0.41\r\n\r\n\r\n0.47\r\n\r\n\r\n0.97\r\n\r\n\r\n0.68\r\n\r\n\r\n0.63\r\n\r\n\r\n0.41\r\n\r\n\r\n0.70\r\n\r\n\r\n0.78\r\n\r\n\r\n0.32\r\n\r\n\r\n0.44\r\n\r\n\r\n\r\n\r\ndf <- df %>% \r\n  mutate(t = seq(1,nrow(.),1), .before = 1) %>% \r\n  pivot_longer(!t, names_to = \"vars\", values_to = \"vals\")\r\n\r\n\r\nHistogram and Density\r\nIf there are two variables:\r\n\r\n\r\ndf %>% \r\n  filter(vars %in% c(\"XU100\",\"XBANK\")) %>% \r\n  ggplot(aes(x = vals, fill = vars)) +\r\n  geom_histogram(position = \"identity\", alpha = .5) +\r\n  theme_minimal() +\r\n  theme(axis.title = element_blank(),\r\n        axis.text.y = element_blank(),\r\n        legend.title = element_blank(),\r\n        legend.position = \"top\") +\r\n  scale_fill_manual(values = c(\"red\",\"blue\"))\r\n\r\n\r\n\r\n\r\n\r\ndf %>% \r\n  filter(vars %in% c(\"XU100\",\"XBANK\")) %>% \r\n  ggplot(aes(x = vals, fill = vars)) +\r\n  geom_density(alpha = .5) +\r\n  theme_minimal() +\r\n  theme(axis.title = element_blank(),\r\n        axis.text.y = element_blank(),\r\n        legend.title = element_blank(),\r\n        legend.position = \"top\") +\r\n  scale_fill_manual(values = c(\"red\",\"blue\"))\r\n\r\n\r\n\r\n\r\n\r\ndf %>% \r\n  filter(vars %in% c(\"XU100\",\"XBANK\")) %>% \r\n  pivot_wider(names_from = \"vars\", values_from = \"vals\") %>% \r\n  ggplot(aes(x = vals)) +\r\n  geom_histogram(aes(x = XU100, y = ..density..), fill = \"red\" ) +\r\n  geom_label(aes(x = 4, y = 0.25, label = \"XU100\"), color = \"red\") +\r\n  geom_histogram(aes(x = XBANK, y = -..density..), fill = \"blue\") +\r\n  geom_label(aes(x = 4, y = -0.25, label = \"XBANK\"), color = \"blue\") +\r\n  theme_minimal() +\r\n  theme(axis.title = element_blank(),\r\n        axis.text.y = element_blank())\r\n\r\n\r\n\r\n\r\n\r\ndf %>% \r\n  filter(vars %in% c(\"XU100\",\"XBANK\")) %>% \r\n  pivot_wider(names_from = \"vars\", values_from = \"vals\") %>% \r\n  ggplot(aes(x = vals)) +\r\n  geom_density(aes(x = XU100, y = ..density..), fill = \"red\" ) +\r\n  geom_label(aes(x=4, y=0.25, label = \"XU100\"), color=\"red\") +\r\n  geom_density(aes(x = XBANK, y = -..density..), fill = \"blue\") +\r\n  geom_label(aes(x=4, y = -0.25, label = \"XBANK\"), color=\"blue\") +\r\n  theme_minimal() +\r\n  theme(axis.title = element_blank(),\r\n        axis.text.y = element_blank())\r\n\r\n\r\n\r\nIf there are more than two variables:\r\n\r\n\r\ndf %>% \r\n  filter(vars %in% c(\"XU100\",\"XBANK\",\"XBLSM\")) %>% \r\n  ggplot(aes(x = vals, color = vars)) +\r\n  geom_density(lwd = 1.5) +\r\n  theme_minimal() +\r\n  theme(axis.title = element_blank(),\r\n        axis.text.y = element_blank(),\r\n        legend.title = element_blank(),\r\n        legend.position = \"top\")\r\n\r\n\r\n\r\n\r\n\r\ndf %>% \r\n  ggplot(aes(x = vals, fill = vars)) +\r\n  geom_histogram(position = \"identity\") +\r\n  theme_minimal() +\r\n  theme(axis.title = element_blank(),\r\n        axis.text.y = element_blank(),\r\n        legend.position = \"none\") +\r\n  facet_wrap(~vars)\r\n\r\n\r\n\r\n\r\n\r\ndf %>% \r\n  ggplot(aes(x = vals, fill = vars)) +\r\n  geom_density() +\r\n  theme_minimal() +\r\n  theme(axis.title = element_blank(),\r\n        axis.text.y = element_blank(),\r\n        legend.position = \"none\") +\r\n  facet_wrap(~vars)\r\n\r\n\r\n\r\nViolin and Boxplot\r\n\r\n\r\ndf %>% \r\n  ggplot(aes(x = vars, y = vals, fill = vars)) +\r\n  geom_violin() +\r\n  theme_minimal() +\r\n  theme(axis.title = element_blank(),\r\n        legend.position = \"none\")\r\n\r\n\r\n\r\n\r\n\r\ndf %>% \r\n  ggplot(aes(x = vars, y = vals, fill = vars)) +\r\n  geom_violin() +\r\n  theme_minimal() +\r\n  theme(axis.title = element_blank(),\r\n        legend.position = \"none\") +\r\n  coord_flip()\r\n\r\n\r\n\r\n\r\n\r\ndf %>% \r\n  ggplot(aes(x = vars, y = vals, fill = vars)) +\r\n  geom_boxplot() +\r\n  theme_minimal() +\r\n  theme(axis.title = element_blank(),\r\n        legend.position = \"none\")\r\n\r\n\r\n\r\n\r\n\r\ndf %>% \r\n  ggplot(aes(x = vars, y = vals, fill = vars)) +\r\n  geom_boxplot() +\r\n  theme_minimal() +\r\n  theme(axis.title = element_blank(),\r\n        legend.position = \"none\") +\r\n  coord_flip()\r\n\r\n\r\n\r\n\r\n\r\ndf %>% \r\n  ggplot(aes(x = vars, y = vals, fill = vars)) +\r\n  geom_violin() +\r\n  geom_boxplot(alpha = .5) +\r\n  theme_minimal() +\r\n  theme(axis.title = element_blank(),\r\n        legend.position = \"none\")\r\n\r\n\r\n\r\n\r\n\r\ndf %>% \r\n  ggplot(aes(x = vars, y = vals, fill = vars)) +\r\n  geom_violin() +\r\n  geom_boxplot(alpha = .5) +\r\n  theme_minimal() +\r\n  theme(axis.title = element_blank(),\r\n        legend.position = \"none\") +\r\n  coord_flip()\r\n\r\n\r\n\r\nRidgeline\r\n\r\n\r\nlibrary(ggridges)\r\n\r\n\r\n\r\n\r\ndf %>% \r\n  ggplot(aes(x = vals, y = vars, fill = stat(x))) +\r\n  geom_density_ridges_gradient(scale = 3) +\r\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\r\n  scale_fill_viridis_c(option = \"C\") +\r\n  theme_minimal() +\r\n  theme(axis.title = element_blank(),\r\n        legend.position = \"none\")\r\n\r\n\r\n\r\n\r\n\r\ndf %>% \r\n  ggplot(aes(vals, y = vars, fill = 0.5 - abs(0.5 - stat(ecdf)))) +\r\n  stat_density_ridges(geom = \"density_ridges_gradient\", calc_ecdf = TRUE) +\r\n  scale_fill_gradient(low = \"white\", high = \"red\") +\r\n  theme_minimal() +\r\n  theme(axis.title = element_blank(),\r\n        legend.position = \"none\")\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-08-28-post21/post21_files/figure-html5/unnamed-chunk-8-1.png",
    "last_modified": "2022-08-28T21:46:30+03:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-08-21-post20/",
    "title": "Sentiment Comparisons on Interest Rate Releases",
    "description": "Sentiment analysis of the CBRT's interest rates on press releases.",
    "author": [
      {
        "name": "A. Uraz Akgül",
        "url": {}
      }
    ],
    "date": "2022-08-21",
    "categories": [
      "Text",
      "Finance"
    ],
    "contents": "\r\nThe CBRT cut its interest rate by 100 bps to 13% in its August 2022\r\nmeeting. In a country where official inflation goes to 100% and the\r\nexchange rate moves towards 20, the central bank of that country decides\r\nto cut interest rates. Hakan Kara, former chief economist of the CBRT,\r\ntweeted the day the decision was announced:\r\nEconomists are discussing why the CBRT cut its interest rate. I\r\nthink this is no longer economists’ question. Sociologists, political\r\nscientists, psychologists, etc. should evaluate the issue.\r\nAlthough, Press Releases on Interest Rates are not taken seriously\r\nbecause the credibility of the CBRT has been shaken, we will take it\r\nseriously for sentiment analysis. You can access the data I collected\r\nusing web scraping by downloading post20_1.xlsx file on my GitHub\r\naccount. The codes I used to collect the data will be available at\r\nthe end of the post. The reason I started the analysis date from 2014 is\r\nthat the CBRT stopped publishing its press releases in pdf format as of\r\n2014.\r\n\r\n\r\nlibrary(tidyverse)\r\n\r\nmaster <- readxl::read_excel(\"data1.xlsx\") %>% \r\n  group_by(date) %>% \r\n  slice(-1) %>% # remove dates from 'text' column\r\n  ungroup()\r\n\r\n\r\nIn the table below, you can see the CBRT’s latest press release on\r\ninterest rates.\r\n\r\n\r\ntext\r\n\r\n\r\nThe Monetary Policy Committee (MPC) has decided to reduce the policy\r\nrate (one-week repo auction rate) from 14 percent to 13 percent.\r\n\r\n\r\nThe weakening effects of geopolitical risks on global economic activity\r\ncontinue to increase. Global growth forecasts for the upcoming period\r\nare being revised downwards and recession is increasingly assessed as an\r\ninevitable risk factor. While the negative consequences of supply\r\nconstraints in some sectors, particularly basic food, have been\r\nalleviated by the strategic solutions facilitated by Türkiye, the upward\r\ntrend in producer and consumer prices continues on an international\r\nscale. The effects of high global inflation on inflation expectations\r\nand international financial markets are closely monitored. Moreover,\r\ncentral banks in advanced economies emphasize that the rise in inflation\r\nmay last longer than previously anticipated due to rising energy prices,\r\nimbalances between supply and demand, and rigidities in labor markets.\r\nThe divergence in monetary policy steps and communications of central\r\nbanks in advanced economies continue due to their diverse economic\r\noutlook. It is observed that central banks continue their efforts to\r\ndevelop new supportive measures and tools to cope with the increasing\r\nuncertainties in financial markets.\r\n\r\n\r\nRobust growth in the beginning of the year continued in the second\r\nquarter as well, with the support of external demand. Compared to peer\r\neconomies, job creation has been stronger. Considering the sectors that\r\ncontribute to the employment increase, it is observed that the growth\r\ndynamics are supported by structural gains. While share of sustainable\r\ncomponents of economic growth increases, the stronger than expected\r\ncontribution of tourism revenues to the current account balance\r\ncontinues. On the other hand, high course of energy prices and the\r\nlikelihood of a recession in main trade partners keep the risks on\r\ncurrent account balance alive. Sustainable current account balance is\r\nimportant for price stability. The rate of credit growth and allocation\r\nof funds for real economic activity purposes are closely monitored. In\r\naddition, the recent increase in spread between policy rate and the loan\r\ninterest rate is considered to reduce the effectiveness of monetary\r\ntransmission. In this context, the Committee decided to further\r\nstrengthen the macroprudential policy set with tools supporting the\r\neffectiveness of the monetary transmission mechanism.\r\n\r\n\r\nIncrease in inflation is driven by the lagged and indirect effects of\r\nrising energy costs resulting from geopolitical developments, effects of\r\npricing formations that are not supported by economic fundamentals,\r\nstrong negative supply shocks caused by the rise in global energy, food\r\nand agricultural commodity prices. The Committee expects disinflation\r\nprocess to start on the back of measures taken and decisively\r\nimplemented for strengthening sustainable price and financial stability\r\nalong with the resolution of the ongoing regional conflict.\r\nAdditionally, leading indicators for the third quarter point to some\r\nloss of momentum in economic activity. It is important that financial\r\nconditions remain supportive to preserve the growth momentum in\r\nindustrial production and the positive trend in employment in a period\r\nof increasing uncertainties regarding global growth as well as\r\nescalating geopolitical risk. Accordingly, the Committee has decided to\r\nreduce the policy rate by 100 basis points, and has assessed that the\r\nupdated level of policy rate is adequate under the current outlook. To\r\ncreate an institutional basis for sustainable price stability, the\r\ncomprehensive review of the policy framework continues with the aim of\r\nencouraging permanent and strengthened liraization in all policy tools\r\nof the CBRT. The credit, collateral and liquidity policy actions, of\r\nwhich the review process is finalized, will continue to be implemented\r\nto strengthen the effectiveness of the monetary policy transmission\r\nmechanism.\r\n\r\n\r\nThe CBRT will continue to use all available instruments decisively\r\nwithin the framework of liraization strategy until strong indicators\r\npoint to a permanent fall in inflation and the medium-term 5 percent\r\ntarget is achieved in pursuit of the primary objective of price\r\nstability. Stability in the general price level will foster\r\nmacroeconomic stability and financial stability through the fall in\r\ncountry risk premium, continuation of the reversal in currency\r\nsubstitution and the upward trend in foreign exchange reserves, and\r\ndurable decline in financing costs. This would create a viable\r\nfoundation for investment, production and employment to continue growing\r\nin a healthy and sustainable way.\r\n\r\n\r\nThe Committee will continue to take its decisions in a transparent,\r\npredictable and data-driven framework.\r\n\r\n\r\nThe summary of the Monetary Policy Committee Meeting will be released\r\nwithin five working days.\r\n\r\n\r\nCombining the rows of each date group into a single row is as\r\nfollows.\r\n\r\n\r\nmaster2 <- data.frame()\r\n\r\nfor(m in seq_along(unique(master$date))){\r\n  \r\n  master_filtered <- master %>% \r\n    filter(date == unique(master$date)[m]) %>% \r\n    pull(text) %>% \r\n    paste(., collapse = \" \") %>% \r\n    str_replace_all(., \"[\\r\\n]\" , \"\") %>% \r\n    as.data.frame() %>% \r\n    rename(\"text\"=1) %>% \r\n    mutate(date = unique(master$date)[m])\r\n  \r\n  master2 <- master2 %>% \r\n    bind_rows(master_filtered)\r\n  \r\n}\r\n\r\n\r\nWe can move on to sentiment analysis and we are going to use the\r\nsentimentr package to do it at sentence level. sentiment_by(), one of\r\nthe functions of the sentimentr package, is used to calculate polarity\r\nscore by groups.\r\n\r\n\r\n#install.packages(\"sentimentr\")\r\nlibrary(sentimentr)\r\n\r\ndf_sentiment <- data.frame()\r\n\r\nfor(s in 1:nrow(master2)){\r\n  \r\n  sentiment_by_date <- as.data.frame(sentiment_by(master2$text[s])) %>% \r\n    mutate(date = master2$date[s])\r\n  \r\n  df_sentiment <- df_sentiment %>% \r\n    bind_rows(sentiment_by_date)\r\n  \r\n}\r\n\r\n\r\n\r\n\r\nfor(p in 2:4){\r\n  \r\n  df_sentiment %>% \r\n    select(5,all_of(p)) %>% \r\n    ggplot(aes(x = .[[1]], y = .[[2]])) +\r\n    geom_line() +\r\n    ggthemes::theme_fivethirtyeight() +\r\n    theme(plot.title = element_text(size = 30),\r\n          axis.text = element_text(size = 20)) +\r\n    labs(title = names(df_sentiment)[p]) -> plt\r\n  \r\n  plot(plt)\r\n  \r\n}\r\n\r\n\r\n\r\nwords_count: Since the middle of 2019, there has been an increase in\r\nthe number of words. The latest press release is at its peak. I leave\r\nthe press release of the day the sharp rise started and the previous day\r\nbelow.\r\n2019-06-12:\r\n\r\n\r\ntext\r\n\r\n\r\nThe Monetary Policy Committee (the Committee) has decided to keep the\r\npolicy rate (one-week repo auction rate) constant at 24 percent.\r\n\r\n\r\nRecently released data show that rebalancing trend in the economy has\r\ncontinued. External demand maintains its relative strength while\r\neconomic activity displays a slow pace, partly due to tight financial\r\nconditions. Current account balance is expected to maintain its\r\nimproving trend.\r\n\r\n\r\nDevelopments in domestic demand conditions and the tight monetary policy\r\nsupport disinflation. In order to contain the risks to the pricing\r\nbehavior and to reinforce the disinflation process, the Committee has\r\ndecided to maintain the tight monetary policy stance.\r\n\r\n\r\nThe Central Bank will continue to use all available instruments in\r\npursuit of the price stability objective. Factors affecting inflation\r\nwill be closely monitored and, monetary stance will be determined to\r\nkeep inflation in line with the targeted path.\r\n\r\n\r\nIt should be emphasized that any new data or information may lead the\r\nCommittee to revise its stance.\r\n\r\n\r\nThe summary of the Monetary Policy Committee Meeting will be released\r\nwithin five working days.\r\n\r\n\r\n2019-07-25:\r\nMurat Uysal, the former governor of the CBRT, took office in July\r\n2019.\r\n\r\n\r\ntext\r\n\r\n\r\nThe Monetary Policy Committee (the Committee) has decided to reduce the\r\npolicy rate (one-week repo auction rate) from 24 percent to 19.75\r\npercent.\r\n\r\n\r\nRecently released data indicate a moderate recovery in the economic\r\nactivity. Goods and services exports continue to display an upward trend\r\ndespite the weakening in the global economic outlook, indicating\r\nimproved competitiveness. In particular, strong tourism revenues support\r\nthe economic activity through direct and indirect channels. Looking\r\nforward, net exports are expected to contribute to the economic growth\r\nand the gradual recovery is likely to continue with the help of the\r\ndisinflation trend and the partial improvement in financial conditions.\r\nThe composition of growth is having a positive impact on the external\r\nbalance. Current account balance is expected to maintain its improving\r\ntrend.\r\n\r\n\r\nRecently, weaker global economic activity and heightened downside risks\r\nto inflation have strengthened the possibility that advanced economy\r\ncentral banks will take expansionary monetary policy steps. While these\r\ndevelopments support the demand for emerging market assets and the risk\r\nappetite, rising protectionism and uncertainty regarding global economic\r\npolicies are closely monitored in terms of their impact on both capital\r\nflows and international trade.\r\n\r\n\r\nInflation outlook continued to improve. In the second quarter, inflation\r\ndisplayed a significant fall with the contribution from a deceleration\r\nin unprocessed food and energy prices. Domestic demand conditions and\r\nthe tight monetary policy continue to support disinflation. Underlying\r\ntrend indicators, supply side factors, and import prices lead to an\r\nimprovement in the inflation outlook. In light of these developments,\r\nrecent forecast revisions suggest that inflation is likely to\r\nmaterialize slightly below the projections of the April Inflation Report\r\nby the end of the year. Accordingly, considering all the factors\r\naffecting inflation outlook, the Committee decided to reduce the policy\r\nrate by 425 basis points.\r\n\r\n\r\nThe Committee assesses that maintaining a sustained disinflation process\r\nis the key for achieving lower sovereign risk, lower long-term interest\r\nrates, and stronger economic recovery.  Keeping the disinflation process\r\nin track with the targeted path requires the continuation of a cautious\r\nmonetary stance. In this respect, the extent of the monetary tightness\r\nwill be determined by considering the indicators of the underlying\r\ninflation trend to ensure the continuation of the disinflation process.\r\nThe Central Bank will continue to use all available instruments in\r\npursuit of the price stability and financial stability objectives.\r\n\r\n\r\nIt should be emphasized that any new data or information may lead the\r\nCommittee to revise its stance.\r\n\r\n\r\nThe summary of the Monetary Policy Committee Meeting will be released\r\nwithin five working days.\r\n\r\n\r\nsd: The ups and downs after 2018 are more volatile than before 2018.\r\nThe worsening in inflation can be seen more clearly after 2018.\r\nave_sentiment: Although it is a series with ups and downs, it can be\r\nseen that the average score is on the positive side and is in an upward\r\ntrend after 2022. The first thing that comes to my mind here is that the\r\ncentral banks at least try to stabilize the situation with their verbal\r\nguidance, even though it is getting worse.\r\nInflation in Turkey started to give signals of getting out of control\r\nas of 2018. The reason I shared the inflation graph is both to read it\r\ntogether with the graphs above and because the primary objective of the\r\nBank is to achieve and maintain price stability.\r\n\r\n\r\n# post20_2.xlsx\r\n\r\ncpi <- readxl::read_excel(\"data2.xlsx\") %>% \r\n  mutate(date = as.Date(paste0(date,\"-\",1)))\r\n\r\nggplot(cpi, aes(x = date, y = inflation)) +\r\n  geom_line() +\r\n  ggthemes::theme_fivethirtyeight()\r\n\r\n\r\n\r\nWe may wonder: Have we succeeded in perceiving emotions? Let’s take a\r\nlook at the latest press release as an example.\r\n\r\n\r\nhighlight(sentiment_by(master2$text[nrow(master2)]))\r\n\r\n\r\n\r\n\r\n\r\nThe weakening effects of geopolitical risks on global economic\r\nactivity continue to increase.\r\nThe above sentence is negative but perceived as positive. Let’s take\r\na closer look.\r\n\r\n\r\n\"The weakening effects of geopolitical risks on global economic activity continue to increase.\" %>% \r\n  extract_sentiment_terms() %>% \r\n  unlist(.)\r\n\r\n                                                                                     element_id \r\n                                                                                            \"1\" \r\n                                                                                    sentence_id \r\n                                                                                            \"1\" \r\n                                                                                      negative1 \r\n                                                                                    \"weakening\" \r\n                                                                                      negative2 \r\n                                                                                        \"risks\" \r\n                                                                                      negative3 \r\n                                                                                     \"economic\" \r\n                                                                                       neutral1 \r\n                                                                                          \"the\" \r\n                                                                                       neutral2 \r\n                                                                                      \"effects\" \r\n                                                                                       neutral3 \r\n                                                                                           \"of\" \r\n                                                                                       neutral4 \r\n                                                                                 \"geopolitical\" \r\n                                                                                       neutral5 \r\n                                                                                           \"on\" \r\n                                                                                       neutral6 \r\n                                                                                     \"activity\" \r\n                                                                                       neutral7 \r\n                                                                                           \"to\" \r\n                                                                                      positive1 \r\n                                                                                       \"global\" \r\n                                                                                      positive2 \r\n                                                                                     \"continue\" \r\n                                                                                      positive3 \r\n                                                                                     \"increase\" \r\n                                                                                       sentence \r\n\"The weakening effects of geopolitical risks on global economic activity continue to increase.\" \r\n\r\nAlthough the content of the package used is useful, it will be useful\r\nto use it carefully. This study also shows that a sentiment analysis\r\npackage can be made for central banks, for example.\r\nYou can access the other codes I mentioned at the beginning\r\nbelow.\r\n\r\n\r\nlibrary(rvest)\r\nlibrary(tidyverse)\r\n\r\nurls <- str_c(\r\n  \"https://www.tcmb.gov.tr/wps/wcm/connect/EN/TCMB+EN/Main+Menu/Announcements/Press+Releases/\",\r\n  seq(2014,2022,1)\r\n)\r\n\r\ndf_urls <- data.frame()\r\n\r\nfor(i in seq_along(urls)){\r\n  \r\n  c1 <- read_html(urls[i]) %>% \r\n    html_nodes(\"div.collection-content.w-clearfix a.collection-title\") %>% \r\n    html_attr(\"href\")\r\n  \r\n  c2 <- read_html(urls[i]) %>% \r\n    html_nodes(\"div.collection-content.w-clearfix a.collection-title\") %>% \r\n    html_text()\r\n  \r\n  tbl <- data.frame(\r\n    c1 = c1,\r\n    c2 = c2\r\n  )\r\n  \r\n  df_urls <- df_urls %>% \r\n    bind_rows(tbl)\r\n  \r\n  Sys.sleep(3)\r\n  \r\n}\r\n\r\ndf_urls <- df_urls %>% \r\n  filter(grepl(\"Press Release on Interest Rates\",c2)) %>% \r\n  mutate(c1 = paste0(\"https://www.tcmb.gov.tr\",c1))\r\n\r\ndf_text <- data.frame()\r\n\r\nfor(j in 1:nrow(df_urls)){\r\n  \r\n  txt <- read_html(df_urls$c1[j]) %>% \r\n    html_nodes(\"div.tcmb-content.type-prg p\") %>% \r\n    html_text() %>% \r\n    as.data.frame() %>% \r\n    rename(\"text\"=1) %>% \r\n    filter(text != \"\") %>% \r\n    mutate(\"date\"=.$text[2])\r\n  \r\n  df_text <- df_text %>% \r\n    bind_rows(txt)\r\n  \r\n  print(j)\r\n  \r\n  Sys.sleep(30)\r\n  \r\n}\r\n\r\ngovernors <- c(\r\n  \"Şahap Kavcıoğlu\",\r\n  \"Naci Ağbal\",\r\n  \"Murat Uysal\",\r\n  \"Murat Çetinkaya\",\r\n  \"Erdem Başçı\"\r\n)\r\n\r\ndf_text2 <- df_text %>% \r\n  mutate(date = gsub(\"Release Date: \",\"\",date),\r\n         date = gsub(\"Aygust\",\"August\",date),\r\n         date = lubridate::dmy(date),\r\n         text = as.character(text)) %>% \r\n  filter(!(grepl(\"No:|No.\",text)) & grepl(\"[[:space:]]\",text) & !(grepl(paste(governors,collapse=\"|\"),text))) %>% \r\n  arrange(date)\r\n\r\n#openxlsx::write.xlsx(df_text2,\"data.xlsx\")\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-08-21-post20/img1.PNG",
    "last_modified": "2022-08-21T16:22:42+03:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-08-17-post19/",
    "title": "Pay Attention to Every Little Detail",
    "description": "Knowing the data and visualizing it in all its details.",
    "author": [
      {
        "name": "A. Uraz Akgül",
        "url": {}
      }
    ],
    "date": "2022-08-17",
    "categories": [
      "Data Visualization"
    ],
    "contents": "\r\nIs there any value in visualizing data that you do not know? What is\r\nthe value of visualizing the data we know in a bad way?\r\nIn this post, I would like to answer the two questions I asked above.\r\nI’m going to use TURKSTAT’s Consumer Confidence Index (CCI) data to\r\nanswer my questions. TURKSTAT or Turkish Statistical Institute is an\r\ninstitution which is gathering and publishing national data of specific\r\nissues in different areas in Turkey. You can access the site by clicking\r\nthe link here. If you encounter a\r\nTurkish page, you can change the TR abbreviation in the upper right\r\ncorner to EN.\r\nAfter clicking the link above, you can follow the steps below.\r\nYou can access the data (post19.xls) on my GitHub\r\naccount.\r\nStatistics\r\nEconomic Confidence\r\nStatistical Tables\r\nConsumer Confidence Index\r\n4.1. Consumer Confidence Index, 2004-2011\r\n4.2. Indices Concerning the Consumer Tendency and Monthly Changes\r\nAccording to the definition in the metadata on TURKSTAT’s website,\r\nMonthly Consumer Tendency Survey aims to measure present situation\r\nassessments and future period expectations of consumers’ on personal\r\nfinancial standing and general economic course and determining\r\nconsumers’ expenditure and saving tendencies for near future.\r\nWe know little about the data. Let’s import and visualize it.\r\n\r\n\r\nlibrary(tidyverse)\r\n\r\ndf <- readxl::read_excel(\"data.xls\") %>% \r\n  mutate(date = as.Date(date))\r\n\r\nggplot(df, aes(x = date, y = cons_conf_index)) +\r\n  geom_line()\r\n\r\n\r\n\r\nAt first glance, we see a falling index, right? Yes, it’s true! We\r\ncan add one more function, although it’s obvious that it’s falling.\r\n\r\n\r\nggplot(df, aes(x = date, y = cons_conf_index)) +\r\n  geom_line() +\r\n  geom_smooth(method = \"lm\") +\r\n  geom_smooth(method = \"loess\")\r\n\r\n\r\n\r\nHow about coloring it?\r\n\r\n\r\nggplot(df, aes(x = date, y = cons_conf_index)) +\r\n  geom_line() +\r\n  geom_smooth(method = \"lm\", color = \"red\") +\r\n  geom_smooth(method = \"loess\", color = \"blue\")\r\n\r\n\r\n\r\nThe main thing I want to draw your attention to is the y-axis values.\r\nA confidence level below 100 reflects a pessimistic outlook, while a\r\nreading above 100 indicates optimism.\r\nLet’s redraw the graph taking into account the above information. I’m\r\nremoving the function we added in the previous graph for now.\r\n\r\n\r\nggplot(df, aes(x = date, y = cons_conf_index)) +\r\n  geom_line() +\r\n  geom_hline(yintercept = 100)\r\n\r\n\r\n\r\nThe consumers of Turkey have almost always been pessimistic!\r\nAnother piece of information is that the index is evaluated between 0\r\nand 200.\r\n\r\n\r\nggplot(df, aes(x = date, y = cons_conf_index)) +\r\n  geom_line() +\r\n  geom_hline(yintercept = 100) +\r\n  scale_y_continuous(limits = c(0,200))\r\n\r\n\r\n\r\nIn the previous graphs, there was a perception that the index had\r\nreached its bottom point. Although the index is still bad, at least we\r\nhave eliminated this perception.\r\nI would like to show you step-by-step the ways to make a graphic more\r\neye-catching, based on my experience.\r\nI prefer the geom_ribbon() function when I need to color the above\r\nand below of a value. In our example, this value is 100.\r\nFirst of all, I want to split the index values into two groups.\r\n\r\n\r\ndf <- df %>% \r\n  mutate(\r\n    IndexGroups = case_when(\r\n      cons_conf_index > 100 ~ \"Optimistic\",\r\n      cons_conf_index < 100 ~ \"Pessimistic\"\r\n    )\r\n  )\r\n\r\n\r\nWe splitted them into groups and we can use the geom_ribbon()\r\nfunction. Before making the graph, some values need to be created.\r\n\r\n\r\n# Part - I\r\n\r\ndf$ymin_below <- pmin(df$cons_conf_index,100)\r\ndf$ymax_below <- 100\r\n\r\nggplot(df, aes(x = date, y = cons_conf_index)) +\r\n  geom_line() +\r\n  geom_hline(yintercept = 100) +\r\n  scale_y_continuous(limits = c(0,200)) +\r\n  geom_ribbon(\r\n    aes(ymin = ymin_below, ymax = ymax_below),\r\n    color = \"red\", #line\r\n    fill = \"red\", #area\r\n    alpha = .5 #transparency\r\n  )\r\n\r\n\r\n\r\n\r\n\r\n# Part - II\r\n\r\ndf$ymin_above <- 100\r\ndf$ymax_above <- pmax(df$cons_conf_index,100)\r\n\r\nggplot(df, aes(x = date, y = cons_conf_index)) +\r\n  geom_line() +\r\n  geom_hline(yintercept = 100) +\r\n  scale_y_continuous(limits = c(0,200)) +\r\n  geom_ribbon(\r\n    aes(ymin = ymin_above, ymax = ymax_above),\r\n    color = \"orange\", #line\r\n    fill = \"orange\", #area\r\n    alpha = .5 #transparency\r\n  )\r\n\r\n\r\n\r\nCombining two graphs into one…\r\n\r\n\r\nggplot(df, aes(x = date, y = cons_conf_index)) +\r\n  geom_line() +\r\n  #geom_hline(yintercept = 100) + ---> We don't need this line anymore\r\n  scale_y_continuous(limits = c(0,200)) +\r\n  geom_ribbon(\r\n    aes(ymin = ymin_below, ymax = ymax_below),\r\n    color = \"red\", #line\r\n    fill = \"red\", #area\r\n    alpha = .5 #transparency\r\n  ) +\r\n  geom_ribbon(\r\n    aes(ymin = ymin_above, ymax = ymax_above),\r\n    color = \"orange\", #line\r\n    fill = \"orange\", #area\r\n    alpha = .5 #transparency\r\n  )\r\n\r\n\r\n\r\nI’m one of those who like FiveThirtyEight’s theme! Use it to make it\r\nlook good!\r\n\r\n\r\nggplot(df, aes(x = date, y = cons_conf_index)) +\r\n  geom_line() +\r\n  #geom_hline(yintercept = 100) + ---> We don't need this line anymore\r\n  scale_y_continuous(limits = c(0,200)) +\r\n  geom_ribbon(\r\n    aes(ymin = ymin_below, ymax = ymax_below),\r\n    color = \"red\", #line\r\n    fill = \"red\", #area\r\n    alpha = .5 #transparency\r\n  ) +\r\n  geom_ribbon(\r\n    aes(ymin = ymin_above, ymax = ymax_above),\r\n    color = \"orange\", #line\r\n    fill = \"orange\", #area\r\n    alpha = .5 #transparency\r\n  ) +\r\n  ggthemes::theme_fivethirtyeight()\r\n\r\n\r\n\r\nA little more information?\r\n\r\n\r\nggplot(df, aes(x = date, y = cons_conf_index)) +\r\n  geom_line() +\r\n  #geom_hline(yintercept = 100) + ---> We don't need this line anymore\r\n  scale_y_continuous(limits = c(0,200)) +\r\n  geom_ribbon(\r\n    aes(ymin = ymin_below, ymax = ymax_below),\r\n    color = \"red\", #line\r\n    fill = \"red\", #area\r\n    alpha = .5 #transparency\r\n  ) +\r\n  geom_ribbon(\r\n    aes(ymin = ymin_above, ymax = ymax_above),\r\n    color = \"orange\", #line\r\n    fill = \"orange\", #area\r\n    alpha = .5 #transparency\r\n  ) +\r\n  ggthemes::theme_fivethirtyeight() +\r\n  labs(\r\n    title = \"Turkish Consumer Confidence\",\r\n    subtitle = paste0(\r\n      format(min(df$date),\"%Y/%m\"),\r\n      \"-\",\r\n      format(max(df$date),\"%Y/%m\")\r\n    ), # To make it dynamic\r\n    caption = \"The data were collected from TURKSTAT\"\r\n  ) +\r\n  theme(\r\n    plot.subtitle = element_text(face = \"italic\", size = 10),\r\n    plot.caption = element_text(face = \"italic\")\r\n  )\r\n\r\n\r\n\r\nIn this post, I wanted to show you the importance of obtaining\r\ninformation about data and also showed you ways to visualize it better.\r\nI hope that you enjoyed reading and found this post helpful.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-08-17-post19/post19_files/figure-html5/unnamed-chunk-11-1.png",
    "last_modified": "2022-08-17T19:11:06+03:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-08-14-post18/",
    "title": "Does Linear Regression Work? A Study on BIST-100",
    "description": "Testing the Linear Regression method in a non-quantitative way.",
    "author": [
      {
        "name": "A. Uraz Akgül",
        "url": {}
      }
    ],
    "date": "2022-08-14",
    "categories": [
      "Finance",
      "Technical Analysis"
    ],
    "contents": "\r\nIn\r\nthe previous post, I shared a study using BIST-100 data and linear\r\nregression method. In this post, we are going to test the success of the\r\nmethod again using the same index.\r\nAs I mentioned before, we can get the data from the Electronic Data\r\nDelivery System of the Central Bank of the Republic of Turkey. Please\r\ncheck the previous post. You can also access the data\r\n(post18.xlsx) on my GitHub\r\naccount.\r\n\r\n\r\nlibrary(tidyverse)\r\n\r\ndf_xu100 <- readxl::read_excel(\"data.xlsx\") %>% \r\n  mutate(\r\n    date = lubridate::dmy(date)\r\n  )\r\n\r\n\r\nIn order to measure the success of the method, we are going to split\r\nthe time series into a certain number of small series. Let me put it\r\nthis way, data splitting is when data is divided into two or more\r\nsubsets. Using the ntile() function to split the series is an\r\noption.\r\n\r\n\r\ndf <- df_xu100 %>% \r\n  slice(1:4900) %>% \r\n  mutate(\r\n    cluster = ntile(date, 49)\r\n  )\r\n\r\n\r\nAssuming there are 4900 rows instead of 4914, we can split it into 49\r\ndifferent series. In this case, each series will have 100\r\nobservations.\r\nAfter splitting the series, it’s time to build the model. If the data\r\nyou have is grouped, you can make different models as shown below.\r\n\r\n\r\ndf_models <- df %>% \r\n  group_by(cluster) %>% \r\n  mutate(rn = row_number()) %>% \r\n  do(model = lm(close ~ rn, data = .))\r\n\r\n\r\nLet’s look at the first model as an example.\r\n\r\n\r\nsummary(df_models[[2]][[1]])\r\n\r\n\r\nCall:\r\nlm(formula = close ~ rn, data = .)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-18.2203  -2.9681   0.4508   4.2494  11.3862 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 105.42839    1.22710  85.917   <2e-16 ***\r\nrn            0.03242    0.02110   1.537    0.128    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 6.09 on 98 degrees of freedom\r\nMultiple R-squared:  0.02354,   Adjusted R-squared:  0.01357 \r\nF-statistic: 2.362 on 1 and 98 DF,  p-value: 0.1275\r\n\r\nWe can get the fitted values and confidence intervals from the models\r\nwe built as follows.\r\n\r\n\r\ndf_model_final <- data.frame()\r\n\r\nfor(i in unique(df$cluster)){\r\n  \r\n  df_filtered <- df %>% \r\n    filter(cluster == i)\r\n  \r\n  df_filtered$fitted <- df_models[[2]][[i]][[\"fitted.values\"]]\r\n  df_filtered$lwr <- predict(df_models[[2]][[i]], interval = \"prediction\", level = 0.95)[,2]\r\n  df_filtered$upr <- predict(df_models[[2]][[i]], interval = \"prediction\", level = 0.95)[,3]\r\n  \r\n  df_model_final <- df_model_final %>% \r\n    bind_rows(df_filtered)\r\n  \r\n}\r\n\r\n\r\nWe are almost there!\r\n\r\n\r\ndf_model_final <- df_model_final %>% \r\n  group_by(cluster) %>% \r\n  mutate(rn = row_number())\r\n\r\n\r\n\r\n\r\n\r\nWe’ve finished most of the work but need to brainstorm. How do we\r\nmeasure success? We can examine some series in a non-quantitative way\r\nunder some assumptions. Maybe I can come up with different methods in\r\nthe next posts.\r\nOur assumption is to make a 100-day decision with a 100-day\r\ntrend.\r\nLet’s start with 2-3 and continue with 37-38.\r\n\r\n\r\ndf_2_3 <- df_model_final %>% \r\n  ungroup() %>% \r\n  filter(cluster %in% c(2,3)) %>% \r\n  mutate(t = row_number())\r\n\r\n\r\n\r\n\r\n\r\nIn the graph above, I could expect a drop from here as the index is\r\nin contact with the upper band. The index which was 153.81 became 201.85\r\nafter 100 days.\r\n\r\n\r\ndf_37_38 <- df_model_final %>% \r\n  ungroup() %>% \r\n  filter(cluster %in% c(37,38)) %>% \r\n  mutate(t = row_number())\r\n\r\n\r\n\r\n\r\n\r\nIn the graph above, since the index is below the lower band, I could\r\nexpect a rise from here. The index which was 1012.18 became 1165.10\r\nafter 100 days.\r\nSome more graphs…\r\n\r\n\r\ndf_18_19 <- df_model_final %>% \r\n  ungroup() %>% \r\n  filter(cluster %in% c(18,19)) %>% \r\n  mutate(t = row_number())\r\n\r\n\r\n\r\n\r\n\r\nThe index is actually close to the fitted line here. Most of the time\r\nit can be difficult to decide the direction around the fitted line, but\r\nthere is a movement from the lower band here. Therefore, I could expect\r\nan upward movement. The index, which was 529.59, became 607.37 at the\r\nend of the series.\r\nWell, based on the 49th series, what will be the situation for the\r\n50th series we are currently in?\r\n\r\n\r\ndf_49_50 <- df_model_final %>% \r\n  ungroup() %>% \r\n  filter(cluster %in% c(49,50)) %>% \r\n  mutate(t = row_number())\r\n\r\n\r\n\r\n\r\n\r\nA movement of the series beginning from the lower band stands out.\r\nThe index is still below the fitted line. An upward movement can be\r\nexpected from here.\r\nBring back the 14 days we removed from the series earlier. When we\r\nadd this, it becomes 114 days in total.\r\n\r\n\r\ndf_50 <- df_xu100 %>% \r\n  filter(date > as.Date(\"2022-07-22\")) %>% \r\n  mutate(\r\n    cluster = rep(50,14),\r\n    fitted = rep(NA,14),\r\n    lwr = rep(NA,14),\r\n    upr = rep(NA,14),\r\n    rn = seq(101,114,1),\r\n    t = seq(101,114,1)\r\n  )\r\n\r\ndf_49_50 <- rbind(df_49_50,df_50)\r\n\r\n\r\n\r\n\r\n\r\nPerfect, just as we expected! In the coming days, we may see a trend\r\nbreak just like the other graphs!\r\nRemember, we said: As can be seen from the graph, the real value\r\nis further away, which means that it is both moving away from the fitted\r\nvalue and in the overbought zone.\r\nI should point out that we reached this conclusion from the monthly\r\nfrequency series. In this study, we used daily frequency series. I mean,\r\nshort-term rises can be expected, but it is necessary to pay attention\r\nto a possible bubble in longer term.\r\nMore is coming, stay tuned!\r\nThe codes of the graphics used in the study can be found below.\r\n\r\n\r\nggplot(df_model_final) +\r\n  geom_line(aes(x = rn, y = close)) +\r\n  geom_line(aes(x = rn, y = fitted), color = \"blue\") +\r\n  geom_line(aes(x = rn, y = lwr), color = \"red\", linetype = \"dashed\") +\r\n  geom_line(aes(x = rn, y = upr), color = \"red\", linetype = \"dashed\") +\r\n  facet_wrap(~cluster, scales = \"free\") +\r\n  theme(axis.title = element_blank())\r\n\r\nggplot(df_2_3) +\r\n  geom_line(aes(x = t, y = close)) +\r\n  geom_vline(xintercept = (100+101)/2, linetype = \"dashed\") +\r\n  geom_line(aes(x = t, y = fitted, group = cluster), color = \"blue\") +\r\n  geom_line(aes(x = t, y = lwr, group = cluster), color = \"red\", linetype = \"dashed\") +\r\n  geom_line(aes(x = t, y = upr, group = cluster), color = \"red\", linetype = \"dashed\") +\r\n  theme_minimal() +\r\n  theme(axis.title = element_blank())\r\n\r\nggplot(df_37_38) +\r\n  geom_line(aes(x = t, y = close)) +\r\n  geom_vline(xintercept = (100+101)/2, linetype = \"dashed\") +\r\n  geom_line(aes(x = t, y = fitted, group = cluster), color = \"blue\") +\r\n  geom_line(aes(x = t, y = lwr, group = cluster), color = \"red\", linetype = \"dashed\") +\r\n  geom_line(aes(x = t, y = upr, group = cluster), color = \"red\", linetype = \"dashed\") +\r\n  theme_minimal() +\r\n  theme(axis.title = element_blank())\r\n\r\nggplot(df_18_19) +\r\n  geom_line(aes(x = t, y = close)) +\r\n  geom_vline(xintercept = (100+101)/2, linetype = \"dashed\") +\r\n  geom_line(aes(x = t, y = fitted, group = cluster), color = \"blue\") +\r\n  geom_line(aes(x = t, y = lwr, group = cluster), color = \"red\", linetype = \"dashed\") +\r\n  geom_line(aes(x = t, y = upr, group = cluster), color = \"red\", linetype = \"dashed\") +\r\n  theme_minimal() +\r\n  theme(axis.title = element_blank())\r\n\r\nggplot(df_49_50) +\r\n  geom_line(aes(x = t, y = close)) +\r\n  geom_vline(xintercept = (100+101)/2, linetype = \"dashed\") +\r\n  geom_line(aes(x = t, y = fitted, group = cluster), color = \"blue\") +\r\n  geom_line(aes(x = t, y = lwr, group = cluster), color = \"red\", linetype = \"dashed\") +\r\n  geom_line(aes(x = t, y = upr, group = cluster), color = \"red\", linetype = \"dashed\") +\r\n  theme_minimal() +\r\n  theme(axis.title = element_blank())\r\n\r\nggplot(df_49_50) +\r\n  geom_line(aes(x = t, y = close)) +\r\n  geom_vline(xintercept = (100+101)/2, linetype = \"dashed\") +\r\n  geom_line(aes(x = t, y = fitted, group = cluster), color = \"blue\") +\r\n  geom_line(aes(x = t, y = lwr, group = cluster), color = \"red\", linetype = \"dashed\") +\r\n  geom_line(aes(x = t, y = upr, group = cluster), color = \"red\", linetype = \"dashed\") +\r\n  theme_minimal() +\r\n  theme(axis.title = element_blank())\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-08-14-post18/post18_files/figure-html5/unnamed-chunk-7-1.png",
    "last_modified": "2022-08-14T22:31:24+03:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-08-07-post17/",
    "title": "KantitatifTurk 0.0.0.9000",
    "description": "KantitatifTurk paketi.",
    "author": [
      {
        "name": "A. Uraz Akgül",
        "url": {}
      }
    ],
    "date": "2022-08-07",
    "categories": [
      "Development",
      "Finance"
    ],
    "contents": "\r\nTürkçe içerikli ve finans alanındaki konulara odaklanacak şekilde\r\ntasarladığım {KantitatifTurk} paketinin başlangıç sayılabilecek\r\nversiyonunu bugün itibarıyla paylaştım. Hem kendi tarafımdan hem de\r\nprojeye destek olmak isteyenler tarafından zamanla büyümesini istediğim\r\nbu projenin, finans alanına ilgi duyan ve bu alanı R ile birleştirenlere\r\nfaydalı olmasını dilerim.\r\nPaketi, {devtools} yardımı ile GitHub üzerinden aşağıdaki gibi\r\nindirebilirsiniz.\r\n\r\n\r\ndevtools::install_github(\"rpydaneogrendim/KantitatifTurk\")\r\n\r\n\r\nPaketin yüklenme işlemi tamamlandıktan sonra aşağıdaki gibi\r\nçağrılabilir.\r\n\r\n\r\nlibrary(KantitatifTurk)\r\n\r\n\r\nPaketin fonksiyonlarını kütüphaneyi çağırmadan aşağıdaki gibi\r\nkullanmak da mümkündür.\r\n\r\nKantitatifTurk::\r\n\r\nİlk kullanılması gereken fonksiyon veri()’dir. Bu fonksiyon\r\nyardımı ile veriler tarihsel olarak çekilecek ve\r\nvarlik_tarihsel adında bir obje yaratılacak. Bu objenin hiçbir\r\nşekilde değiştirilmemesi veya silinmemesi gerekiyor. Tarihsel veriler\r\niçin şimdilik tek kaynak İş Yatırım’dır. Bu da paketin hisseler özelinde\r\nolduğu algısı oluşturabilir ancak hisseler dışında da varlıkları\r\nhedeflediğim için zamanla veri kaynağını genişleteceğim.\r\nÖrnek olarak, 9 adet hissenin 253 günlük tarihsel verilerini\r\nçekelim.\r\nveri() fonksiyonu ile ilgili dokümantasyona ulaşmak için\r\naşağıdaki komutu çalıştırabilirsiniz.\r\n\r\n\r\n?veri\r\n\r\n\r\n\r\n\r\nhisseler <- c(\r\n  \"THYAO\",\r\n  \"SISE\",\r\n  \"SASA\",\r\n  \"BIMAS\",\r\n  \"GARAN\",\r\n  \"TCELL\",\r\n  \"PETKM\",\r\n  \"TUPRS\",\r\n  \"SOKM\"\r\n)\r\n\r\nveri(\r\n  varlik_kod = hisseler,\r\n  baslangic_tarih = \"2021-08-05\",\r\n  bitis_tarih = \"2022-08-05\",\r\n  gorsel = TRUE, # varsayilan\r\n  kaynak = \"isyatirim\" # varsayilan\r\n)\r\n\r\n\r\n\r\nGörsel olarak incelenebilen varlıkların verileri\r\nvarlik_tarihsel olarak ortamınızda oluşturulmuştur.\r\n\r\n\r\nTARIH\r\n\r\n\r\nTHYAO\r\n\r\n\r\nSISE\r\n\r\n\r\nSASA\r\n\r\n\r\nBIMAS\r\n\r\n\r\nGARAN\r\n\r\n\r\nTCELL\r\n\r\n\r\nPETKM\r\n\r\n\r\nTUPRS\r\n\r\n\r\nSOKM\r\n\r\n\r\n2022-07-29\r\n\r\n\r\n50.40\r\n\r\n\r\n21.82\r\n\r\n\r\n44.48\r\n\r\n\r\n92.10\r\n\r\n\r\n14.52\r\n\r\n\r\n16.92\r\n\r\n\r\n8.98\r\n\r\n\r\n268.4\r\n\r\n\r\n14.02\r\n\r\n\r\n2022-08-01\r\n\r\n\r\n51.50\r\n\r\n\r\n23.10\r\n\r\n\r\n46.50\r\n\r\n\r\n95.15\r\n\r\n\r\n14.71\r\n\r\n\r\n17.25\r\n\r\n\r\n9.18\r\n\r\n\r\n273.0\r\n\r\n\r\n14.68\r\n\r\n\r\n2022-08-02\r\n\r\n\r\n52.35\r\n\r\n\r\n22.86\r\n\r\n\r\n48.26\r\n\r\n\r\n93.45\r\n\r\n\r\n14.68\r\n\r\n\r\n17.03\r\n\r\n\r\n9.06\r\n\r\n\r\n270.8\r\n\r\n\r\n14.17\r\n\r\n\r\n2022-08-03\r\n\r\n\r\n55.00\r\n\r\n\r\n23.28\r\n\r\n\r\n49.84\r\n\r\n\r\n94.05\r\n\r\n\r\n14.84\r\n\r\n\r\n17.20\r\n\r\n\r\n9.07\r\n\r\n\r\n273.0\r\n\r\n\r\n13.97\r\n\r\n\r\n2022-08-04\r\n\r\n\r\n55.30\r\n\r\n\r\n23.76\r\n\r\n\r\n50.05\r\n\r\n\r\n95.30\r\n\r\n\r\n15.20\r\n\r\n\r\n17.30\r\n\r\n\r\n9.17\r\n\r\n\r\n270.4\r\n\r\n\r\n14.60\r\n\r\n\r\n2022-08-05\r\n\r\n\r\n58.20\r\n\r\n\r\n23.40\r\n\r\n\r\n49.80\r\n\r\n\r\n95.55\r\n\r\n\r\n15.68\r\n\r\n\r\n17.44\r\n\r\n\r\n9.23\r\n\r\n\r\n269.0\r\n\r\n\r\n14.40\r\n\r\n\r\nİncelenmek istenen varlıkların getirilerine ait özet bilgiye\r\nozet_getiri() fonksiyonu yardımı ile ulaşılabilir.\r\n\r\n\r\n?ozet_getiri\r\n\r\n\r\nBu fonksiyon yardımı ile logaritmik veya aritmetik olarak\r\nhesaplanabilen getirilerin minimum, maksimum, ortalama, ortanca ve\r\nstandart sapma bilgilerine ulaşılabilir.\r\n\r\n\r\nozet_getiri(\r\n  getiri = \"logaritmik\", # varsayilan\r\n  gorsel = TRUE # varsayilan\r\n)\r\n\r\n# A tibble: 9 × 6\r\n  KOD   Minimum Maksimum Ortalama Ortanca StandartSapma\r\n  <chr>   <dbl>    <dbl>    <dbl>   <dbl>         <dbl>\r\n1 BIMAS -0.0962   0.0605 0.00178  0.00226        0.0216\r\n2 GARAN -0.0688   0.0946 0.00217  0              0.0208\r\n3 PETKM -0.105    0.0764 0.00188  0.00283        0.0265\r\n4 SASA  -0.105    0.0951 0.00481  0.00328        0.0297\r\n5 SISE  -0.105    0.0950 0.00440  0.00256        0.0266\r\n6 SOKM  -0.105    0.0950 0.000831 0              0.0255\r\n7 TCELL -0.105    0.0699 0.000762 0              0.0232\r\n8 THYAO -0.105    0.0953 0.00602  0.00568        0.0303\r\n9 TUPRS -0.105    0.0863 0.00402  0.00204        0.0289\r\n\r\n\r\ngetiri_risk() fonksiyonu ile varlık getirilerinin\r\nyıllıklandırılmış getiri ve risk bilgileri aşağıdaki gibi incelenebilir.\r\nFonksiyon çalıştırıldıktan sonra risk_getiri isminde bir obje\r\nyaratacaktır. Getiriler logaritmik veya aritmetik olarak\r\nhesaplanabilir.\r\n\r\n\r\n?getiri_risk\r\n\r\n\r\n\r\n\r\ngetiri_risk(\r\n  getiri = \"aritmetik\",\r\n  gun = 252, # varsayilan\r\n  gorsel = TRUE # varsayilan\r\n)\r\n\r\n\r\n\r\n\r\n\r\nKOD\r\n\r\n\r\nYillikGetiri\r\n\r\n\r\nYillikRisk\r\n\r\n\r\nOran\r\n\r\n\r\nBIMAS\r\n\r\n\r\n0.5081250\r\n\r\n\r\n0.3407252\r\n\r\n\r\n1.4913044\r\n\r\n\r\nGARAN\r\n\r\n\r\n0.6027357\r\n\r\n\r\n0.3324938\r\n\r\n\r\n1.8127726\r\n\r\n\r\nPETKM\r\n\r\n\r\n0.5629509\r\n\r\n\r\n0.4159958\r\n\r\n\r\n1.3532611\r\n\r\n\r\nSASA\r\n\r\n\r\n1.3261321\r\n\r\n\r\n0.4770458\r\n\r\n\r\n2.7798841\r\n\r\n\r\nSISE\r\n\r\n\r\n1.2007094\r\n\r\n\r\n0.4222509\r\n\r\n\r\n2.8435923\r\n\r\n\r\nSOKM\r\n\r\n\r\n0.2905614\r\n\r\n\r\n0.4021772\r\n\r\n\r\n0.7224710\r\n\r\n\r\nTCELL\r\n\r\n\r\n0.2594781\r\n\r\n\r\n0.3664732\r\n\r\n\r\n0.7080411\r\n\r\n\r\nTHYAO\r\n\r\n\r\n1.6377039\r\n\r\n\r\n0.4825286\r\n\r\n\r\n3.3940038\r\n\r\n\r\nTUPRS\r\n\r\n\r\n1.1192899\r\n\r\n\r\n0.4584011\r\n\r\n\r\n2.4417260\r\n\r\n\r\nmaks_dusme_noktasi() fonksiyonu ile varlıkların maksimum\r\ndüşme noktası (Maximum Drawdown) hesaplanabilir. Fonksiyon tek bir\r\nparametre almaktadır. Çalıştırıldıktan sonra\r\nmaks_dusme_noktalari isminde bir obje oluşturacaktır.\r\n\r\n\r\n?maks_dusme_noktasi\r\n\r\n\r\n\r\n\r\nmaks_dusme_noktasi(\r\n  gorsel = TRUE # varsayilan\r\n)\r\n\r\n\r\n\r\n\r\n\r\nKOD\r\n\r\n\r\nMaksDD\r\n\r\n\r\nBIMAS\r\n\r\n\r\n-27.92473\r\n\r\n\r\nGARAN\r\n\r\n\r\n-16.54275\r\n\r\n\r\nPETKM\r\n\r\n\r\n-31.00848\r\n\r\n\r\nSASA\r\n\r\n\r\n-26.01054\r\n\r\n\r\nSISE\r\n\r\n\r\n-33.70972\r\n\r\n\r\nSOKM\r\n\r\n\r\n-32.73810\r\n\r\n\r\nTCELL\r\n\r\n\r\n-31.18101\r\n\r\n\r\nTHYAO\r\n\r\n\r\n-24.84076\r\n\r\n\r\nTUPRS\r\n\r\n\r\n-30.73194\r\n\r\n\r\nkaydirmali_korelasyon() fonksiyonu ile varlıklar arası\r\nkorelasyonlar kayan bir şekilde (Rolling Correlation) hesaplanabilir.\r\nFonksiyon, ne kadarlık bir kayma olacağını belirten tek bir parametre\r\nile çalışmaktadır.\r\n\r\n\r\n?kaydirmali_korelasyon\r\n\r\n\r\n\r\n\r\nkaydirmali_korelasyon(\r\n  gun = 90\r\n)\r\n\r\n\r\n\r\nPaketin diğer fonksiyonlarında öngörüyü de dikkate aldım.\r\ndogrusal_regresyon() fonksiyonu, Doğrusal Regresyon yöntemi\r\nile tahmin ve öngörü yapılmasını sağlar. Kullanıcı, güven aralığı ve\r\nöngörüsü yapılacak gün uzunluğunu belirleyebilir. Fonksiyon\r\nçalıştırıldıktan sonra DR_varlik_ongoru isminde bir obje\r\noluşacaktır.\r\n\r\n\r\n?dogrusal_regresyon\r\n\r\n\r\n\r\n\r\ndogrusal_regresyon(\r\n  guven = 0.95, # varsayilan\r\n  ongoru_gun = 180,\r\n  gorsel = TRUE # varsayilan\r\n)\r\n\r\n\r\n\r\n\r\n\r\nKOD\r\n\r\n\r\nONGORU\r\n\r\n\r\nALT\r\n\r\n\r\nUST\r\n\r\n\r\nBIMAS\r\n\r\n\r\n107.47772\r\n\r\n\r\n98.29485\r\n\r\n\r\n116.66059\r\n\r\n\r\nGARAN\r\n\r\n\r\n20.22989\r\n\r\n\r\n18.70555\r\n\r\n\r\n21.75423\r\n\r\n\r\nPETKM\r\n\r\n\r\n12.96083\r\n\r\n\r\n11.26202\r\n\r\n\r\n14.65964\r\n\r\n\r\nSASA\r\n\r\n\r\n66.90545\r\n\r\n\r\n60.34000\r\n\r\n\r\n73.47091\r\n\r\n\r\nSISE\r\n\r\n\r\n33.25522\r\n\r\n\r\n29.88172\r\n\r\n\r\n36.62872\r\n\r\n\r\nSOKM\r\n\r\n\r\n14.08098\r\n\r\n\r\n11.65936\r\n\r\n\r\n16.50259\r\n\r\n\r\nTCELL\r\n\r\n\r\n23.12055\r\n\r\n\r\n19.02521\r\n\r\n\r\n27.21590\r\n\r\n\r\nTHYAO\r\n\r\n\r\n82.56969\r\n\r\n\r\n75.99628\r\n\r\n\r\n89.14309\r\n\r\n\r\nTUPRS\r\n\r\n\r\n418.84886\r\n\r\n\r\n389.84658\r\n\r\n\r\n447.85113\r\n\r\n\r\nÖngörü için kullanılabilecek diğer bir fonksiyon\r\ngeometrik_brownian_hareketi() ile varliklarin ileriye yönelik\r\nsimülasyonu yapılabilir. Öngörüler, elde edilen simüle değerlerin\r\nortanca ya da ortalamasıdır. Bu fonksiyon, varlik_tarihsel veri\r\nçerçevesinde bulunan tek bir hisse ile çalıştırılmalıdır. Bir obje\r\noluşturulmamakla beraber sonuçlara görsel üzerinden ulaşılabilir.\r\n\r\n\r\n?geometrik_brownian_hareketi\r\n\r\n\r\n\r\n\r\ngeometrik_brownian_hareketi(\r\n  varlik_kod = \"TUPRS\",\r\n  simulasyon_sayisi = 100, # varsayilan\r\n  gun_sayisi = 90,\r\n  beklenen_getiri = 0, # varsayilan\r\n  volatilite = 0.5, # varsayilan\r\n  is_gunu_sayisi = nrow(varlik_tarihsel),\r\n  ongoru_tipi = \"ortanca\", # varsayilan\r\n  gorsel = TRUE # varsayilan\r\n)\r\n\r\n\r\n\r\nÖngörü için kullanılabilecek son bir fonksiyon\r\nmarkov_zinciri() ile varliklarin ileriye yönelik yön\r\nolasılıkları hesaplanabilir. Fonksiyon, Markov Zinciri yöntemi ile\r\nvarlığın son hareketinden yola çıkarak belirlenen gün sayısı kadar\r\nsonrası için yön olasılıkları hesaplamaktadır. Çalıştırıldıktan sonra\r\ngörsel olarak sonuçlara ulaşılabileceği gibi markov_olasilik\r\nobjesinden de faydalanılabilir.\r\n\r\n\r\n?markov_zinciri\r\n\r\n\r\n\r\n\r\nmarkov_zinciri(\r\n  n_adim = 1, # varsayilan\r\n  gorsel = TRUE # varsayilan\r\n)\r\n\r\n\r\n\r\n\r\n\r\nGunSonrasi\r\n\r\n\r\nKOD\r\n\r\n\r\nDurum\r\n\r\n\r\nDurum1\r\n\r\n\r\nDurum2\r\n\r\n\r\nOlasilik\r\n\r\n\r\n1\r\n\r\n\r\nTUPRS\r\n\r\n\r\nAyni-Dusus\r\n\r\n\r\nAyni\r\n\r\n\r\nDusus\r\n\r\n\r\n0.0000000\r\n\r\n\r\n1\r\n\r\n\r\nTUPRS\r\n\r\n\r\nAyni-Ayni\r\n\r\n\r\nAyni\r\n\r\n\r\nAyni\r\n\r\n\r\n0.0000000\r\n\r\n\r\n1\r\n\r\n\r\nTUPRS\r\n\r\n\r\nDusus-Ayni\r\n\r\n\r\nDusus\r\n\r\n\r\nAyni\r\n\r\n\r\n0.0039841\r\n\r\n\r\n1\r\n\r\n\r\nTUPRS\r\n\r\n\r\nYukselis-Ayni\r\n\r\n\r\nYukselis\r\n\r\n\r\nAyni\r\n\r\n\r\n0.0159363\r\n\r\n\r\n1\r\n\r\n\r\nTUPRS\r\n\r\n\r\nAyni-Yukselis\r\n\r\n\r\nAyni\r\n\r\n\r\nYukselis\r\n\r\n\r\n0.0199203\r\n\r\n\r\n1\r\n\r\n\r\nTUPRS\r\n\r\n\r\nDusus-Dusus\r\n\r\n\r\nDusus\r\n\r\n\r\nDusus\r\n\r\n\r\n0.1752988\r\n\r\n\r\n1\r\n\r\n\r\nTUPRS\r\n\r\n\r\nDusus-Yukselis\r\n\r\n\r\nDusus\r\n\r\n\r\nYukselis\r\n\r\n\r\n0.2430279\r\n\r\n\r\n1\r\n\r\n\r\nTUPRS\r\n\r\n\r\nYukselis-Dusus\r\n\r\n\r\nYukselis\r\n\r\n\r\nDusus\r\n\r\n\r\n0.2509960\r\n\r\n\r\n1\r\n\r\n\r\nTUPRS\r\n\r\n\r\nYukselis-Yukselis\r\n\r\n\r\nYukselis\r\n\r\n\r\nYukselis\r\n\r\n\r\n0.2908367\r\n\r\n\r\nSon olarak, riske_maruz_deger() fonksiyonu ile şimdilik\r\nportföy bazında olmasa da varlıklar bazında riske maruz değerler\r\nparametrik veya tarihsel olarak hesaplanabilir. Fonksiyon, ilgili\r\nhesaplamaları obje olarak oluşturacaktır. Hata alınmaması için girilen\r\ngozlem sayısının varlik_tarihsel veri çerçevesindeki satırdan\r\nküçük olmasına dikkat edilmelidir. Örneğin, gözlem sayısı 252 girildiği\r\nzaman son 252 günü (getiriler için +1 daha 253) alacağı için satır\r\nsayısı 253 veya daha büyük olmalıdır.\r\n\r\n\r\n?riske_maruz_deger\r\n\r\n\r\n\r\n\r\nriske_maruz_deger(\r\n  rmd = \"parametrik\", # varsayilan\r\n  guven_araligi = 0.99, # varsayilan\r\n  gozlem_sayisi = 252, # varsayilan\r\n  elde_tutma_suresi = 1, # varsayilan\r\n  volatilite = \"standartsapma\", #varsayilan\r\n  getiri = \"logaritmik\", #varsayilan\r\n  piyasa_degeri = 100, #varsayilan\r\n  gorsel = TRUE # varsayilan\r\n)\r\n\r\n\r\n\r\n\r\n\r\nVarlik\r\n\r\n\r\nParametrik_RMD\r\n\r\n\r\nTHYAO\r\n\r\n\r\n7.044808\r\n\r\n\r\nSISE\r\n\r\n\r\n6.194875\r\n\r\n\r\nSASA\r\n\r\n\r\n6.916234\r\n\r\n\r\nBIMAS\r\n\r\n\r\n5.020284\r\n\r\n\r\nGARAN\r\n\r\n\r\n4.844383\r\n\r\n\r\nTCELL\r\n\r\n\r\n5.396344\r\n\r\n\r\nPETKM\r\n\r\n\r\n6.165011\r\n\r\n\r\nTUPRS\r\n\r\n\r\n6.727724\r\n\r\n\r\nSOKM\r\n\r\n\r\n5.925761\r\n\r\n\r\nURL:\r\nBugReports:\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-08-07-post17/post17_files/figure-html5/unnamed-chunk-18-1.png",
    "last_modified": "2022-08-07T19:26:44+03:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-08-02-post16/",
    "title": "Creating Trend Indicator Using Linear Regression Method: A Study on BIST-100",
    "description": "Trend tracking with Linear Regression method.",
    "author": [
      {
        "name": "A. Uraz Akgül",
        "url": {}
      }
    ],
    "date": "2022-08-02",
    "categories": [
      "Finance",
      "Technical Analysis"
    ],
    "contents": "\r\nLinear Regression is a method already used in technical analysis, but\r\nusing existing methods as they are should not be a rule! So, we should\r\nalways be open to different adventures. In this study, we will focus on\r\na method that identifies both trend and overbought/oversold zones.\r\nThe data we are going to use are taken from the Electronic Data\r\nDelivery System of the Central Bank of the Republic of Turkey.\r\nAfter clicking this\r\nlink follow the steps shown below.\r\ns1. Market Statistics\r\ns2. Borsa Istanbul (BIST) Trading Volume (Thousand TRY, Thousand\r\nUnit)\r\ns3. (PRICE INDICES) BIST-100 (XU100), According to Closing Price\r\n(January, 1986=0.01)\r\ns4. Click the Add button\r\ns5. Report Settings\r\ns5.1. Frequency: Business\r\ns5.2. Date From: 01-01-2003\r\ns5.3. Date To: 01-08-2022\r\ns5.4. Click the Create Report button\r\ns5.5. The file can be downloaded by clicking the export button in the\r\nupper right corner\r\nYou can also access the data (post16.xlsx) on my GitHub\r\naccount.\r\n\r\n\r\nlibrary(tidyverse)\r\n\r\ndf_xu100 <- readxl::read_excel(\"data.xlsx\") %>% \r\n  mutate(\r\n    date = lubridate::dmy(date)\r\n  )\r\n\r\n\r\nIt may be a good idea to visualize before starting a study. You can\r\nfind the codes related to visualization at the end of the post.\r\n\r\n\r\n\r\nWe can say that there is no problem with the data.\r\nIn the next part, we are going to use monthly data. Let’s extract the\r\nyears and months from the date column.\r\n\r\n\r\ndf_xu100 <- df_xu100 %>% \r\n  mutate(\r\n    Year = format(as.Date(date),\"%Y\"),\r\n    Month = format(as.Date(date),\"%m\")\r\n  )\r\n\r\n\r\nAfter running the above code, we get the closing values at the end of\r\nthe month as follows:\r\n\r\n\r\ndf_xu100 <- df_xu100 %>% \r\n  group_by(Month,Year) %>% \r\n  filter(date == max(date)) %>% \r\n  ungroup()\r\n\r\n\r\nA data frame that can be used for monthly data has been created.\r\nMonthly data are as follows:\r\n\r\n\r\nxu100_m <- df_xu100 %>% \r\n  select(date,xu100)\r\n\r\n\r\n\r\n\r\n\r\nLet’s use the first 19 years from 2003 to 2019 and predict the\r\nremaining 1 year, but first let me explain the method we are going to\r\nuse.\r\nA Linear Regression model is created by fitting a trend line to a\r\ndataset where a linear relationship already exists.\r\n\r\n\r\ntrain_xu100_m <- xu100_m %>% \r\n  filter(date <= as.Date(\"2021-12-31\")) %>% \r\n  mutate(t = seq(1,nrow(.),1))\r\n\r\nmodel <- lm(xu100 ~ t, data = train_xu100_m)\r\n#summary(model)\r\n\r\n\r\n\r\n\r\n===============================================\r\n                        Dependent variable:    \r\n                    ---------------------------\r\n                               xu100           \r\n-----------------------------------------------\r\nt                            5.033***          \r\n                              (0.118)          \r\n                                               \r\nConstant                    102.259***         \r\n                             (15.562)          \r\n                                               \r\n-----------------------------------------------\r\nObservations                    228            \r\nR2                             0.890           \r\nAdjusted R2                    0.889           \r\nResidual Std. Error     117.105 (df = 226)     \r\nF Statistic         1,824.293*** (df = 1; 226) \r\n===============================================\r\nNote:               *p<0.1; **p<0.05; ***p<0.01\r\n\r\nAnd a prediction interval captures the uncertainty around a single\r\nvalue.\r\n\r\n\r\ntrain_xu100_m$Fitted <- as.numeric(predict(model))\r\n\r\ntrain_xu100_m$Lwr90 <- as.numeric(predict(model, interval = \"predict\", level = 0.9)[,2])\r\ntrain_xu100_m$Upr90 <- as.numeric(predict(model, interval = \"predict\", level = 0.9)[,3])\r\n\r\n\r\nPut them all together.\r\n\r\n\r\n\r\nSome questions about the method.\r\nHow far are the actual values from the fitted values? If the distance\r\nis positive, it means that the actual values are above the fitted\r\nvalues.\r\n\r\n\r\ntrain_xu100_m$ActualFitted <- train_xu100_m$xu100 - train_xu100_m$Fitted\r\n\r\n\r\nHow far are the actual values from the upper band values? If the\r\ndistance is positive, it means that the real values are above the upper\r\nband values, which is the overbought zone.\r\n\r\n\r\ntrain_xu100_m$ActualUpr <- train_xu100_m$xu100 - train_xu100_m$Upr90\r\n\r\n\r\nHow far are the actual values from the lower band values? If the\r\ndistance is negative, it means that the real values are below the lower\r\nband values, which is the oversold zone. Notice that I multiplied the\r\nresult by minus 1.\r\n\r\n\r\ntrain_xu100_m$ActualLwr <- (train_xu100_m$xu100 - train_xu100_m$Lwr90)*(-1)\r\n\r\n\r\nIt’s hard to understand the values we get. In such cases, values can\r\nbe normalized. Here’s the formula for normalization:\r\n\\(x_{normalized} = \\frac{(x -\r\nx_{minimum})}{(x_{maximum} - x_{minimum})}\\)\r\n\r\n\r\n# Don't run!\r\n\r\ntrain_xu100_m <- train_xu100_m %>% \r\n  mutate_at(vars(4:6), function(x) (x - min(x))/(max(x) - min(x)))\r\n\r\n\r\nIn general, values are normalized to be between 0 and 1, but it is\r\nalso possible to normalize it to be between -1 and 1.\r\n\\(x_{normalized} = 2*\\frac{(x -\r\nx_{minimum})}{(x_{maximum} - x_{minimum})}-1\\)\r\n\r\n\r\ntrain_xu100_m <- train_xu100_m %>% \r\n  mutate_at(vars(7:9), function(x) 2*((x - min(x))/(max(x) - min(x)))-1)\r\n\r\n\r\n\r\n\r\n\r\nIn the above graph, we expect the line to fluctuate around the zero\r\nline, but BIST-100 is far from it which tells us that we should consider\r\na downward move.\r\n\r\n\r\n\r\nIn the graph above, which we created using the upper bands, the\r\ncloseness to the 1 implies that it is in the overbought zone. We can\r\ninterpret that BIST-100 is in this zone.\r\n\r\n\r\n\r\nIn the graph above, which we created using the lower bands, the\r\ncloseness to the 1 implies that it is in the oversold zone. We can say\r\nthat BIST-100 is far from the oversold zone.\r\nWe have 236 observations and we used 228 of them. Let’s try to\r\npredict the remaining 7 months (we didn’t include August) and see how\r\nfar from the line we specified.\r\n\r\n\r\nmaster1 <- train_xu100_m %>% \r\n  select(3,2,4,5,6)\r\n\r\nmaster2 <- data.frame(\r\n  t = seq(nrow(master1)+1,nrow(df_xu100)-1,1),\r\n  xu100 = df_xu100$xu100[c((nrow(master1)+1):(nrow(df_xu100)-1))]\r\n)\r\n\r\nmaster2$Fitted <- predict(model, newdata = data.frame(t = master2$t))\r\nmaster2$Lwr90 <- predict(model, newdata = data.frame(t = master2$t),\r\n                         interval = \"prediction\",\r\n                         level = 0.9)[,2]\r\nmaster2$Upr90 <- predict(model, newdata = data.frame(t = master2$t),\r\n                         interval = \"prediction\",\r\n                         level = 0.9)[,3]\r\n\r\nmaster <- rbind(master1,master2)\r\n\r\n\r\n\r\n\r\n\r\nAs can be seen from the graph, the real value is further away, which\r\nmeans that it is both moving away from the fitted value and in the\r\noverbought zone.\r\nMy in-depth studies will continue here :)\r\nThe codes of the graphics used in the study can be found below.\r\n\r\n\r\nggplot(df_xu100, aes(x = date, y = xu100)) +\r\n  geom_line() +\r\n  theme_minimal() +\r\n  theme(axis.title = element_blank()) +\r\n  scale_y_continuous(labels = scales::comma)\r\n\r\nggplot(xu100_m, aes(x = date, y = xu100)) +\r\n  geom_line(size = 1) +\r\n  theme_minimal() +\r\n  theme(axis.title = element_blank()) +\r\n  scale_y_continuous(labels = scales::comma)\r\n\r\nggplot(train_xu100_m, aes(x = t)) +\r\n  geom_line(aes(y = xu100), color = \"gray30\") +\r\n  geom_line(aes(y = Fitted), color = \"blue\") +\r\n  geom_line(aes(y = Lwr90), color = \"red\", linetype = \"dashed\") +\r\n  geom_line(aes(y = Upr90), color = \"red\", linetype = \"dashed\") +\r\n  theme_minimal() +\r\n  theme(axis.title = element_blank()) +\r\n  scale_y_continuous(labels = scales::comma)\r\n\r\nggplot(train_xu100_m, aes(x = t)) +\r\n  geom_line(aes(y = xu100), color = \"gray30\") +\r\n  geom_line(aes(y = Fitted), color = \"blue\") +\r\n  geom_line(aes(y = Lwr90), color = \"red\", linetype = \"dashed\") +\r\n  geom_line(aes(y = Upr90), color = \"red\", linetype = \"dashed\") +\r\n  theme_minimal() +\r\n  theme(axis.title = element_blank()) +\r\n  scale_y_continuous(labels = scales::comma) -> g1\r\n\r\nggplot(train_xu100_m, aes(x = t)) +\r\n  geom_line(aes(y = ActualFitted)) +\r\n  geom_hline(yintercept = 0) +\r\n  theme_minimal() +\r\n  theme(axis.title = element_blank()) -> g2\r\n\r\ngridExtra::grid.arrange(g1,g2)\r\n\r\nggplot(train_xu100_m, aes(x = t)) +\r\n  geom_line(aes(y = ActualUpr)) +\r\n  geom_hline(yintercept = 0) +\r\n  theme_minimal() +\r\n  theme(axis.title = element_blank()) -> g3\r\n\r\ngridExtra::grid.arrange(g1,g3)\r\n\r\nggplot(train_xu100_m, aes(x = t)) +\r\n  geom_line(aes(y = ActualLwr)) +\r\n  geom_hline(yintercept = 0) +\r\n  theme_minimal() +\r\n  theme(axis.title = element_blank()) -> g4\r\n\r\ngridExtra::grid.arrange(g1,g4)\r\n\r\nggplot(master, aes(x = t)) +\r\n  geom_line(aes(y = xu100), color = \"gray30\") +\r\n  geom_line(aes(y = Fitted), color = \"blue\") +\r\n  geom_line(aes(y = Lwr90), color = \"red\", linetype = \"dashed\") +\r\n  geom_line(aes(y = Upr90), color = \"red\", linetype = \"dashed\") +\r\n  geom_vline(xintercept = nrow(master1)+1, linetype = \"dashed\") +\r\n  theme_minimal() +\r\n  theme(axis.title = element_blank(),\r\n        plot.title = element_text(face = \"bold\", size = 15)) +\r\n  scale_y_continuous(labels = scales::comma) +\r\n  labs(\r\n    title = \"BIST-100 Monthly\",\r\n    subtitle = \"Is it in the overbought zone?\"\r\n  )\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-08-02-post16/post16_files/figure-html5/unnamed-chunk-20-1.png",
    "last_modified": "2022-08-02T20:47:44+03:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-08-01-post15/",
    "title": "Tarihsel Sıcaklıklar: İstanbul için 2014-2022 Yıllarının Aylık Bazda İncelenmesi",
    "description": "NOAA verileri ve rnoaa paketi ile tarihsel sıcaklıkların incelenmesi.",
    "author": [
      {
        "name": "A. Uraz Akgül",
        "url": {}
      }
    ],
    "date": "2022-08-01",
    "categories": [
      "Web"
    ],
    "contents": "\r\nNOAA (National Oceanic and Atmospheric Administration/Ulusal Okyanus\r\nve Atmosfer Dairesi), Amerika Birleşik Devletleri’nin Dünya’daki hava ve\r\ndeniz olaylarını araştırması amacıyla görevlendirilmiş bir kurumudur.\r\n1807 yılında kurulmuş olan NOAA, hava, deniz ve gökyüzündeki tehlikeleri\r\naraştırıp, okyanus ve kıyı kaynaklarının korunması konusunda araştırma\r\nve geliştirme yapmaktadır (Vikipedi).\r\nNOAA’den faydalanabilmek için öncelikle bir token almak gerekiyor. Şu adresten ilgili\r\nsayfaya ulaşılabilir. Yapılması gereken adımlar sitede yazmaktadır. Bir\r\nmail adresi girerek submit butonuna tıklayın. An email has been sent\r\ncontaining your CDO web services token mesajından sonra mailinizi\r\nkontrol edin. Örnek olması için gelen mail görüntüsünü paylaşıyorum.\r\n\r\n\r\n\r\nNOAA’den verileri çekebilmek için rnoaa paketini kullanacağız.\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(ggridges)\r\n\r\n#install.packages(\"rnoaa\")\r\nlibrary(rnoaa)\r\n\r\nmyToken <- \"...\"\r\n\r\n\r\n\r\n\r\n\r\nÖnce Türkiye’deki istasyonların listesini alalım. ghcnd_stations()\r\nfonksiyonunda kullanacağımız id sütununun ilk iki harfi ülke kodunu\r\ntemsil etmektedir. Türkiye için bu TU’dur.\r\n\r\n\r\ncountries <- ghcnd_countries() %>% \r\n  filter(name == \"Turkey\") %>% \r\n  pull(code)\r\n\r\n# id: The weather station’s ID number. The first two letters denote the country\r\n# (using FIPS country codes).\r\n\r\nstations <- ghcnd_stations() %>% \r\n  filter(grepl(paste0(\"^\",countries), id))\r\n\r\n\r\nSiz bundan sonrasında istediğiniz istasyon ile devam edebilirsiniz.\r\nBen bu yazıda İstanbul için bir uygulama yapacağım.\r\nAşağıdaki filtreleri yapalım.\r\nname sütunu İstanbul olsun.\r\n\r\n\r\n# name: The station’s name.\r\n\r\n\r\nelement sütununda TAVG olsun. TAVG, ortalama sıcaklığı temsil\r\nediyor.\r\n\r\n\r\n# TAVG: Average temperature, in tenths of degrees Celsius\r\n\r\n\r\nSon ölçüm yılı olan last_year 2022 olsun.\r\n\r\n\r\n# last_year: The last year of data available at that station for that weather element.\r\n\r\n\r\nİlk ölçüm yılı olan first_year değişkeni de en eski yıl olsun.\r\n\r\n\r\n# first_year: The first year of data available at that station for that weather element.\r\n\r\n\r\nFiltreleyelim.\r\n\r\n\r\nistanbul <- stations %>% \r\n  select(id,name,element,first_year,last_year) %>% \r\n  filter(\r\n    grepl(\"ISTANBUL\",name) & element == \"TAVG\" & last_year == 2022\r\n  )\r\n\r\n\r\nYukarıdaki filtreye göre 1 tane istasyon kaldı. O da bundan sonra\r\nkullanacağımız TUM00017064 id’li istasyondur.\r\nBu istasyondaki ulaşabileceğimiz verilere bakalım.\r\n\r\n\r\nmyStation <- ncdc_datatypes(stationid = \"GHCND:TUM00017064\",\r\n                            datasetid = \"GHCND\",\r\n                            token = myToken)$data\r\n\r\n\r\n\r\n\r\nmindate\r\n\r\n\r\nmaxdate\r\n\r\n\r\nname\r\n\r\n\r\ndatacoverage\r\n\r\n\r\nid\r\n\r\n\r\n1781-01-01\r\n\r\n\r\n2022-07-28\r\n\r\n\r\nPrecipitation\r\n\r\n\r\n1\r\n\r\n\r\nPRCP\r\n\r\n\r\n1857-01-18\r\n\r\n\r\n2022-07-28\r\n\r\n\r\nSnow depth\r\n\r\n\r\n1\r\n\r\n\r\nSNWD\r\n\r\n\r\n1874-10-13\r\n\r\n\r\n2022-07-28\r\n\r\n\r\nAverage Temperature.\r\n\r\n\r\n1\r\n\r\n\r\nTAVG\r\n\r\n\r\n1763-01-01\r\n\r\n\r\n2022-07-28\r\n\r\n\r\nMaximum temperature\r\n\r\n\r\n1\r\n\r\n\r\nTMAX\r\n\r\n\r\n1763-01-01\r\n\r\n\r\n2022-07-28\r\n\r\n\r\nMinimum temperature\r\n\r\n\r\n1\r\n\r\n\r\nTMIN\r\n\r\n\r\nVerileri çekelim.\r\n\r\n\r\navg34 <- ncdc(\r\n  datasetid = \"GHCND\",\r\n  stationid = \"GHCND:TUM00017064\",\r\n  datatypeid = \"TAVG\",\r\n  startdate = \"2014-01-01\",\r\n  enddate = \"2022-07-31\",\r\n  token = myToken,\r\n  units = \"metric\",\r\n  limit = 1000\r\n)$data\r\n\r\n\r\nYukarıdaki kodu çalıştırdığımızda tarih aralığının 1 yıldan az olması\r\ngerektiğini belirten The date range must be less than 1 year\r\nuyarısını alacağız.\r\nO halde şöyle yapalım: Elimizde 1 yıllık farklı aralıklarda tarihler\r\nolsun ve bir döngü yardımı ile hepsine ulaşalım.\r\n\r\n\r\ndf_date <- data.frame(\r\n  \"StartDate\" = (Sys.Date()-1)-365,\r\n  \"EndDate\" = Sys.Date()-1\r\n)\r\n\r\nfor(i in 2:9){\r\n  \r\n  df_date[i,2] <- df_date[(i-1),1] - 1\r\n  df_date[i,1] <- df_date[i,2] - 365\r\n  \r\n}\r\n\r\ndf_date <- df_date %>% \r\n  arrange(StartDate)\r\n\r\n\r\n\r\n\r\nStartDate\r\n\r\n\r\nEndDate\r\n\r\n\r\n2013-07-25\r\n\r\n\r\n2014-07-25\r\n\r\n\r\n2014-07-26\r\n\r\n\r\n2015-07-26\r\n\r\n\r\n2015-07-27\r\n\r\n\r\n2016-07-26\r\n\r\n\r\n2016-07-27\r\n\r\n\r\n2017-07-27\r\n\r\n\r\n2017-07-28\r\n\r\n\r\n2018-07-28\r\n\r\n\r\n2018-07-29\r\n\r\n\r\n2019-07-29\r\n\r\n\r\n2019-07-30\r\n\r\n\r\n2020-07-29\r\n\r\n\r\n2020-07-30\r\n\r\n\r\n2021-07-30\r\n\r\n\r\n2021-07-31\r\n\r\n\r\n2022-07-31\r\n\r\n\r\nYukarıdaki tabloda yer alan her bir satır 1 yıllık aralıklar ile\r\noluşturulan tarihlerdir. Her bir satır için verileri çekelim.\r\n\r\n\r\ndf_avg34 <- data.frame()\r\n\r\nfor(i in 1:nrow(df_date)) {\r\n  \r\n  avg34 <- ncdc(\r\n    datasetid = \"GHCND\",\r\n    stationid = \"GHCND:TUM00017064\",\r\n    datatypeid = \"TAVG\",\r\n    startdate = as.character(df_date[i, 1]),\r\n    enddate = as.character(df_date[i, 2]),\r\n    token = myToken,\r\n    units = \"metric\",\r\n    limit = 1000\r\n  )\r\n  \r\n  df_avg34 <- df_avg34 %>%\r\n    bind_rows(\r\n      data.frame(\r\n        \"Date\" = avg34$data$date,\r\n        \"AvgTemp\" = avg34$data$value\r\n      )\r\n    )\r\n  \r\n}\r\n\r\n\r\nElde ettiğimiz veri setindeki minimum ve maksimum tarih aralıklarına\r\nbakalım.\r\n\r\n\r\ndf_avg34 %>% \r\n  slice(1,nrow(.))\r\n\r\n\r\n\r\n\r\nDate\r\n\r\n\r\nAvgTemp\r\n\r\n\r\n2014-02-18T00:00:00\r\n\r\n\r\n130\r\n\r\n\r\n2022-07-27T00:00:00\r\n\r\n\r\n276\r\n\r\n\r\nTabloda yer alan ortalama sıcaklıkları 10’a böldüğümüz zaman doğru\r\ndeğerlere ulaşacağız. Çünkü ham veride herhangi bir ondalık yer almıyor.\r\nBunun yanında tarihlerin yer aldığı sütunu da tarih formatına çevirmemiz\r\ngerekiyor.\r\n\r\n\r\ndf_avg34$AvgTemp <- df_avg34$AvgTemp / 10\r\ndf_avg34$Date <- as.Date(substr(df_avg34$Date,1,10))\r\n\r\n\r\n\r\n\r\nDate\r\n\r\n\r\nAvgTemp\r\n\r\n\r\n2014-02-18\r\n\r\n\r\n13.0\r\n\r\n\r\n2022-07-27\r\n\r\n\r\n27.6\r\n\r\n\r\nSıcaklık verileri ile çalıştığım zaman ggridges paketini kullanmaya\r\nözen gösteriyorum. Bu paket ile özel bir görselleştirme türü olan\r\nRidgeline veya Joy plot kullanılabilir.\r\nTarihlerden ayları ve yılları elde edelim ve grafiği ay sayısına\r\nbölüp yukarıda belirttiğim Ridgeline’ı yapalım.\r\n\r\n\r\ndf_avg34_split <- df_avg34 %>% \r\n  mutate(\r\n    \"Year\" = format(as.Date(Date),\"%Y\"),\r\n    \"Month\" = factor(\r\n      format(as.Date(Date),\"%B\"),\r\n      levels = c(\"Ocak\",\"Şubat\",\"Mart\",\"Nisan\",\"Mayıs\",\"Haziran\",\r\n                 \"Temmuz\",\"Ağustos\",\"Eylül\",\"Ekim\",\"Kasım\",\"Aralık\")\r\n    )\r\n  ) %>% \r\n  mutate(\r\n    Year = forcats::fct_rev(Year)\r\n  )\r\n\r\nggplot(df_avg34_split, aes(x = AvgTemp, y = Year, fill = stat(x))) +\r\n  geom_density_ridges_gradient(scale = 3, rel_min_height = 0.01) +\r\n  scale_fill_viridis_c(option = \"C\") +\r\n  facet_wrap(~Month) +\r\n  theme_minimal() +\r\n  theme(axis.title = element_blank(),\r\n        legend.position = \"top\",\r\n        legend.title = element_blank(),\r\n        strip.text = element_text(size = 20),\r\n        plot.title = element_text(face = \"bold\", hjust = 0.5)) +\r\n  labs(title = \"İstanbul'da Hava Sıcaklıkları, 2014-2022\")\r\n\r\n\r\n\r\nSon olarak, aylara göre ortalamada en sıcak yılın hangisi olduğuna\r\nbakalım.\r\n\r\n\r\ndf_avg34_split %>% \r\n  select(-Date) %>% \r\n  group_by(Year,Month) %>% \r\n  summarise(AvgTemp = mean(AvgTemp)) %>% \r\n  ungroup() %>% \r\n  group_by(Month) %>% \r\n  filter(AvgTemp == max(AvgTemp)) %>% \r\n  ggplot(aes(x = Month, y = AvgTemp, fill = AvgTemp)) +\r\n  geom_col() +\r\n  geom_text(aes(label = paste0(Year,\"\\n(\",round(AvgTemp, digits = 1),\")\")), vjust = 1.5) +\r\n  theme_minimal() +\r\n  theme(axis.title = element_blank(),\r\n        legend.title = element_blank()) +\r\n  scale_fill_gradient(low = \"turquoise\", high = \"red\")\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-08-01-post15/post15_files/figure-html5/unnamed-chunk-19-1.png",
    "last_modified": "2022-08-01T17:07:18+03:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-07-03-post14/",
    "title": "CNN ile Resim Sınıflandırma: Milletvekillerinin Fotoğrafları Üzerinden Bir Uygulama",
    "description": "Konvolüsyonel Sinir Ağları'nın ve milletvekillerinin fotoğraflarının kullanıldığı bir sınıflandırma çalışması.",
    "author": [
      {
        "name": "A. Uraz Akgül",
        "url": {}
      }
    ],
    "date": "2022-07-03",
    "categories": [
      "Deep Learning"
    ],
    "contents": "\nKonvolüsyonel Sinir Ağı (Convolutional Neural\nNetwork-CNN), genellikle görsel bilginin analiz\nedilmesinde kullanılan bir Yapay Sinir Ağı (Artificial Neural\nNetwork-ANN) sınıfıdır.\nEn basit şekilde CNN’i aşağıdaki gibi tanımlayabiliriz.\n\n\n\nFigure 1: https://www.mdpi.com/2076-3417/9/21/4500\n\n\n\nGirdi (Input) bölümü uygulamamızdaki milletvekillerinin\nfotoğraflarıdır. Yani, verinin ham olarak ağa verildiği bölümdür.\nKonvolüsyon (Convolution) ve Ortaklama/Havuzlama (Pooling) denilen\nbölümde görsellerdeki özellikler yakalanır (feature extraction).\nArdından Tam Bağlantı (Fully Connected) ve Çıktı (Output) denilen bölüme\ngeçilir ve burada sınıflandırma (classification) gerçekleştirilir.\nKonvolüsyon, ortaklama ve tam bağlantı katmanlarına yakından\nbakalım.\nKonvolüsyon: Görselden belirli özelliklerin çıkarılması konvolüsyon\nkatmanında olur. Bu katmanda, tüm görüntüye 2x2, 3x3, 5x5 gibi küçük\nboyutlu filtrelerin uygulanması yapılmaktadır. Böylelikle görüntüdeki\ndaha ayırt edici özellikler çıkartılarak yeni bir görüntü elde edilir.\nAKP’li bir milletvekili ile CHP’li bir milletvekilini birbirinden ayıran\nözelliklerin belirleneceği katman konvolüsyon katmanı olacaktır.\nBu bölümü daha iyi anlayabilmek için bilgisayarın bir resmi nasıl\nokuduğuna bakalım.\nTüm renkler üç renkten oluşur: Kırmızı, Yeşil ve Mavi. Bunu RGB (Red,\nGreen, Blue) olarak kısaltabiliriz. Bu uygulamada göreceğimiz resimler\nrenklidir ve her biri RGB’den oluşuyor. Aynı zamanda renkli resimler 3\nkatmandan oluşur da diyebiliriz.\n\n\n\nFigure 2: Image Credit: Diane Rohrer\n\n\n\nHer renk 0 ile 255 arasında değer alır.\nRGB(255,0,0): Kırmızı\nRGB(0,255,0): Yeşil\nRGB(0,0,255): Mavi\nÖrnek olarak, RGB(87,51,100) ile oluşturduğum renk aşağıdadır.\n\n\n\nFigure 3: RGB(87,51,100)\n\n\n\nR’da RGB renklerinin yoğunluğunu örnek olarak seçtiğim bir tablo\nüzerinden aşağıdaki gibi inceleyebiliriz.\n\n\n\nFigure 4: The Kiss, Gustav Klimt\n\n\n\n\n\nimg4 <- EBImage::readImage(\"img4.jpg\")\nEBImage::hist(img4)\n\n\n\n\n0’a yaklaştıkça koyuluk; 1’e yaklaştıkça açıklık artar. Örneğin,\nRGB(0,0,0) siyah; RGB(255,255,255) beyazdır.\nBakalım tabloda hangi renkler kullanılmış. Başta da öğrendiğimiz\nüzere bu renkler RGB’nin karışımı ile oluşur.\nAşağıdaki resim 1200x1204x3 boyutundadır ki bu da 4,334,400 piksel\ndemektir.\n\n\nthekiss <- jpeg::readJPEG(\"img4.jpg\")\nm_thekiss <- RImagePalette::image_palette(thekiss, n = 20)\nscales::show_col(m_thekiss)\n\n\n\n\nKonumuzla pek alakalı değil ama madem yukarıdaki çıktıyı aldık,\nekstra bir bilgi vermek isterim. Yukarıda görülen #’li değerler\nHex/Hexadecimal (16 tabanlı sayı sistemi) kodudur ve RGB’yi temsil\netmenin bir yoludur. Örneğin, RGB(255,0,0) kırmızıdır. Bunu Hex kodu ile\n#FF0000 şeklinde gösteririz. Burada # sonrası 6 digit vardır. İlk ikisi\nR, sonraki ikisi G ve kalan ikisi B’yi temsil eder. Peki, #FF0000 neden\nkırmızıdır? Hex sisteminde değerler, 0’dan 9’a kadar bildiğimiz rakamlar\nile; 10’dan 16’ya kadar da sırasıyla A, B, C, D, E ve F ile temsil\nedilirler. O halde, FF’i şöyle çevirebiliriz:\n\\(15 * (16^0) + 15 * (16^1) = 15 + 240 =\n255\\) ki bu da \\(R = FF_{16} =\n255_{10}, G = 00_{16} = 0_{10}\\) ve \\(B\n= 00_{16} = 0_{10}\\)’dur.\nDevam edelim ve içinde bulunduğumuz konvolüsyon katmanında\nhesaplamanın nasıl yapıldığına bakalım.\n\n\n\nFigure 5: https://www.youtube.com/watch?v=1GUgD2SBl9A\n\n\n\nYukarıda 5x5 boyutunda bir matris görüyoruz. Burada renkler matris\nşeklinde verilmiş. Sağda görülen 3x3’lük matris ise filtre (Kernel). Bu\nfiltre soldaki matrisin üzerinde gezdirilir ve elde edilen değerler yeni\nmatrise yazdırılır.\n\n\n\n\\((-1x170) + (0x245) + (1x0) + (2x234) +\n(1x42) + (2x64) + (1x32) + (-2x53) + (0x128) = 394\\)\n\n\n\nYukarıdaki işlemi bir ya da birkaç adım kaydırarak devam ettiririz.\nBurada, kodun içerisinde de göreceğimiz Stride’ın alacağı değer\nbelirlenir. Stride ne kadar piksel kaydıracağımızı belirlediğimiz\nparametredir. Stride 1 olursa aşağıdaki gibi kaydırılır ve Kernel\nişlemine devam edilir.\n\n\n\nGünün sonunda bu işlemler bittiğinde 3x3’lük bir çıktı matrisi\n(convolved feature) elde edilmiş olunur.\nPeki, renkli bir resim için bu işlemler nasıl yapılır?\nYukarıdaki işlem 2 defa daha olmak üzere toplamda 3 defa yapılır (RGB\nüç boyutludur) ve elde edilen değerler toplanarak çıktı matrisine\nyazılır.\nOrtaklama: Büyük bir resmin piksel sayısının azaltıldığı işlem\nortaklama ile yapılır. Burada bir Max Pooling veya Average Pooling\nişlemi tercih edilir. Bu işlem ile resmin özellikleri kaybedilmeden\nboyut küçültmeye geçilir. Ayrıca, ortaklama ile görüntüdeki gürültü,\nyani görüntünün sınıflandırmaya katkıda bulunmayan öğeleri de\nfiltrelenmiş olur. Uygulamada kullanacağımız fotoğraflar gürültüden uzak\nolacaktır.\n\n\n\nFigure 6: https://www.researchgate.net/figure/Max-pooling-and-average-pooling_fig5_343675998\n\n\n\nYukarıda görüldüğü gibi burada da kaydırarak ya her penceredeki\nmaksimum değeri ya da ortalama değeri alarak boyut küçültmüş\noluyoruz.\nKonvolüsyon ve ortaklama aşamaları istenildiği kadar tekrar\nettirilebilir.\nTam Bağlantı: Tam bağlantıya kadar verilerimiz matris şeklinde gelir.\nTam bağlantı katmanına geçmeden önce bir Flattening (düzleştirme) işlemi\nyapılır. Bu katman sınıflandırmada kullanılan standart sinir ağlarından\noluşur.\nDüzleştirme işlemini matristen vektöre dönüştürmek gibi\ndüşünebiliriz.\n\n\n\nFigure 7: https://towardsai.net/p/machine-learning/beginner-guides-to-convolutional-neural-network-from-scratch-kuzushiji-mnist-75f42c175b21\n\n\n\nÖğrendiklerimizi toparlamak adına şurada CNN’i\naçıklayan çok güzel bir sayfanın adresini verebilirim. Örnek olarak\npizza resmini seçtim. Buraya kadar öğrendiğimiz tüm bilgiler aşağıdaki\nresmin üzerinde yer almaktadır.\n\n\n\nFigure 8: https://poloclub.github.io/cnn-explainer/\n\n\n\nBu uygulamada üzerinde çok durmayacağız ama birazdan model\noluştururken Adam isimli bir optimizasyon algoritması kullanacağız.\nSadece bunu neden yaptığımıza bakalım. Derin öğrenmede, öğrenme\nişleminin sağlıklı bir sonuca ulaşması için hata fonksiyonunun mutlak\nminimum değerinin bulunması gerekir. Bu da optimizasyon yöntemleri\nkullanılarak gerçekleştiriliyor. Hatayı en küçük yapmada (ağın ürettiği\nçıkış değeri ile gerçek değer arasındaki farkın minimizasyonu)\nkullanılan yöntemlere optimizasyon deniliyor. Yapay sinir ağlarında da\noptimizasyon için en çok kullanılan yöntemlerden biri Gradyan İnişi’dir\n(Gradient Descent). Optimizasyon algoritmalarında öğrenme katsayısının\nayarlanması modelin eğitimi için kritiktir. Biz burada Adam’ı\nseçeceğiz.\nBunun dışında yine üzerinde çok durmayacağımız ama bilgimizin\nolmasında fayda olacak aktivasyon fonksiyonlarından bahsedelim.\nAktivasyon fonksiyonları, bir katmanda bulunan nöronlara ait çıktı\ndeğerini sonraki katmanlara aktarırken kullanılır. Biz ReLU (Rectified\nLinear) ve Sigmoid’i kullanacağız. ReLU’da, giriş değeri sıfırın altında\niken çıktı sıfırdır, ancak giriş değeri sıfırın üzerinde ise çıkış giriş\ndeğerine eşittir ve bağımlı değişkenle doğrusal bir ilişki oluşmaktadır.\nSigmoid ise tanım kümesindeki elemanların her biri için 0-1 arasında bir\ndeğer üretir.\nCNN’i anlatmak bu kadar basit değil tabiki de. Bu uygulama bir giriş\nniteliğinde olsun istedim. Umuyorum başlangıç için yeterli olmuştur.\nUygulama bölümüne geçebiliriz.\nBu çalışmada, TBMM’nin 27. dönem milletvekillerine ait fotoğrafları\nkullanacağız. Milletvekilleri sayı nedeniyle AKP ve CHP ile\nsınırlandırılmıştır. Yani, çalışmanın sonunda ikili bir sınıflandırma\nyapmış olacağız.\nVerilere, çalışmanın sonunda paylaşacağım kod ile ulaşabileceğiniz\ngibi GitHub\nhesabımdan post14 dosyasını indirerek de\nulaşabilirsiniz.\nDetaylara geçmeden önce fotoğraflar ile ilgili bir not düşmek\nistiyorum. Bu çalışmada amacım süreç boyunca sadece yüze odaklanmak\noldu. Yani, fotoğrafın geri kalanı ile ilgilenmek istemedim. Evet, CNN\nde resimler arasındaki farka odaklanır ve etiketlerin hangi özellik ile\nöne çıktığını öğrenir ancak kişisel deneyimim beni validasyonda da\nbaşarıyı arttıracak bir yola yönlendirdi. Bu çalışmaya ilk başladığımda\nbir fotoğrafın tamamını eğitmeye çalıştım ancak validasyon sırasında\ndüşük doğrulama (accuracy) alıyordum. Çeşitli denemelerden sonra tatmin\nedici sonuç alamayınca (ki bu sürede PC de ciddi yoruluyor) yüzleri\nnasıl kırparım sorusuna cevap bulmaya çalıştım ve aşağıdaki paketler ve\nkod yardımı ile tüm yüzleri tespit ettirip yeni bir eğitim ve test seti\nyarattım.\nKullanacağımız paket image.libfacedetection\nolacak. Bu paket, görüntülerdeki yüzleri algılamak için önceden\neğitilmiş bir konvolüsyonel sinir ağı imkanı sunuyor.\n\n\nlibrary(magick)\nlibrary(image.libfacedetection)\n\n########## fotoğrafların isimlerinin aktarımı ##########\n\nimg_face_akp <- list.files(\"tbmm/training/AKP/\", pattern = \"*.jpg\")\nimg_face_chp <- list.files(\"tbmm/training/CHP/\", pattern = \"*.jpg\")\nimg_face_train <- c(img_face_akp,img_face_chp)\nimg_face_test <- list.files(\"tbmm/test/\", pattern = \"*.jpg\")\n\n########## fotoğrafların okutulması ##########\n\nimg_train_read <- list()\nfor(i in 1:length(img_face_train)){\n  \n  if(i <= length(img_face_akp)){\n    \n    img_train_read[[i]] <- image_read(paste0(\"tbmm/training/AKP/\",img_face_train[i]))\n    \n  } else {\n    \n    img_train_read[[i]] <- image_read(paste0(\"tbmm/training/CHP/\",img_face_train[i]))\n    \n  }\n  \n}\n\nimg_test_read <- list()\nfor(i in 1:length(img_face_test)){\n  \n  img_test_read[[i]] <- image_read(paste0(\"tbmm/test/\",img_face_test[i]))\n  \n}\n\n########## yüzlerin elde edilip kaydedilmesi ##########\n\n# training:\nfor(i in 1:length(img_train_read)) {\n  \n  # Hata veren fotoğraf(lar) için tryCatch() koyuldu\n  # Hatalı fotoğraf(lar) otomatik olarak silinmiş olacak\n  \n  tryCatch(\n    expr = {\n      \n      face <- image_detect_faces(img_train_read[[i]])\n      \n      image_crop(\n        img_train_read[[i]],\n        geometry_area(\n          x = face$detections$x,\n          y = face$detections$y,\n          width = face$detections$width,\n          height = face$detections$height\n        )\n      ) -> imageCrop\n      \n      image_write(\n        imageCrop,\n        path = ifelse(i <= length(img_face_akp),\n                      paste0(\"tbmm/training/AKP_face_only/\", img_face_train[i]),\n                      paste0(\"tbmm/training/CHP_face_only/\", img_face_train[i])),\n        format = \"jpg\"\n      )\n      \n    },\n    \n    error = function(e) {\n      message(\"Caught an error!\",img_face_train[i])\n    }\n  )\n  \n}\n\n# test:\nfor(i in 1:length(img_test_read)) {\n  \n  tryCatch(\n    expr = {\n      \n      face <- image_detect_faces(img_test_read[[i]])\n      \n      image_crop(\n        img_test_read[[i]],\n        geometry_area(\n          x = face$detections$x,\n          y = face$detections$y,\n          width = face$detections$width,\n          height = face$detections$height\n        )\n      ) -> imageCrop\n      \n      image_write(\n        imageCrop,\n        path = paste0(\"tbmm/test/test_face_only/\", img_face_test[i]),\n        format = \"jpg\"\n      )\n      \n    },\n    \n    error = function(e) {\n      message(\"Caught an error!\",img_face_test[i])\n    }\n  )\n  \n}\n\n\n\nEğitilecek veri setine ulaşalım. Eğitim (train) veri setinin\niçerisinde yer alan sınıfları iki dosyaya ayırdım: AKP ve CHP.\n\n\nlibrary(keras)\nlibrary(tensorflow)\nlibrary(EBImage)\n\nimgs_akp <- list.files(\"tbmm/training/AKP_face_only/\")\nimgs_chp <- list.files(\"tbmm/training/CHP_face_only/\")\nimgs <- c(imgs_akp,imgs_chp)\n\n\n\nToplamda 418 adet fotoğraf bulunmaktadır. Ancak eğitilecek veri seti\ndengesiz verilerden (imbalanced data) oluşmaktadır ki bu da bir\nproblemdir. Aşağıda görüldüğü üzere AKP sınıfından 284; CHP sınıfından\n134 milletvekili bulunmaktadır.\n\n\nlength(grep(\"AKP\",imgs))\n\n\n[1] 284\n\nlength(grep(\"CHP\",imgs))\n\n\n[1] 134\n\nBurada, AKP daha fazla olduğu için AKP’den 150 adet milletvekilini\nrandom bir şekilde kaldırabiliriz. Seçtiğimiz bu yöntemin karşılığı\nUndersampling olarak geçiyor. Kolay bir yöntem olmasının yanında önemli\nbilgileri kaybetmemize büyük olasılıkla neden olacak da bir\nyöntemdir.\n\n\nimgs_akp <- grep(\"AKP\",imgs, value = TRUE)\nimgs_chp <- grep(\"CHP\",imgs, value = TRUE)\n\nimgs_akp <- imgs_akp[sample(1:length(imgs_akp),length(imgs_chp))]\nremoved_akp <- imgs[!(imgs %in% c(imgs_akp,imgs_chp))]\nimgs <- imgs[imgs %in% c(imgs_akp,imgs_chp)]\n\n\n\nUndersampling ile iki kategoriye de 134 fotoğraf vererek eğitim\nsetini dengelemiş olduk.\nEğitim setindeki dosyaların içerisinde bulunan fotoğraflara\nulaşalım.\n\n\ntrain <- list()\nfor(i in 1:length(imgs)){\n  \n  if(i <= length(imgs_akp)){\n    \n    train[[i]] <- readImage(paste0(\"tbmm/training/AKP_face_only/\",imgs[i]))\n    \n  } else {\n    \n    train[[i]] <- readImage(paste0(\"tbmm/training/CHP_face_only/\",imgs[i]))\n    \n  }\n  \n}\n\n\n\nŞimdi de test seti dosyasındaki iki fotoğrafı alalım.\n\n\nimgs_test <- list.files(\"tbmm/test/test_face_only/\")\n\ntest <- list()\nfor(i in 1:length(imgs_test)){\n  \n  test[[i]] <- readImage(paste0(\"tbmm/test/test_face_only/\",imgs_test[i]))\n  \n}\n\n\n\nEğitim setindeki fotoğraflar birbirine yakın boyutlu olsalar da\nfarklılık bulunmaktadır. Tüm fotoğrafları aynı boyutlu yapacağız. Aynı\nboyuta getirirken şuna dikkat etmeliyiz: Daha büyük boyutlar daha fazla\nözelliğe sahip olacaktır ancak daha büyük boyut aynı zamanda eğitim\naşamasında daha çok zaman alacaktır. Tam tersi, daha küçük boyutta da\nişimize yarayacak özellikleri kaybetmiş olabiliriz. Bu noktada\nsanatımızı konuşturacağız. Ön bir inceleme ile 64x64 olabilir diye\ndüşündüm.\n\n\nfor(i in 1:length(imgs)){\n  \n  train[[i]] <- resize(train[[i]], 64, 64)\n  \n  if(i == length(imgs)){\n    \n    train <- combine(train)\n    \n  }\n  \n}\n\nstr(train)\n\n\nFormal class 'Image' [package \"EBImage\"] with 2 slots\n  ..@ .Data    : num [1:64, 1:64, 1:3, 1:268] 0.9191 0.9625 0.7857 0.4149 0.0428 ...\n  ..@ colormode: int 2\n  ..$ dim: int [1:4] 64 64 3 268\n\nEğitim setindeki fotoğrafları bir araya getirdik. Bu fotoğraflar\n64x64x3 boyutunda ve 268 adet.\n\n\nfor(i in 1:length(imgs_test)){\n  \n  test[[i]] <- resize(test[[i]], 64, 64)\n  \n  if(i == length(imgs_test)){\n    \n    test <- combine(test)\n    \n  }\n  \n}\n\nstr(test)\n\n\nFormal class 'Image' [package \"EBImage\"] with 2 slots\n  ..@ .Data    : num [1:64, 1:64, 1:3, 1:2] 0.305 0.343 0.428 0.522 0.605 ...\n  ..@ colormode: int 2\n  ..$ dim: int [1:4] 64 64 3 2\n\nAynı şekilde, test setindeki fotoğrafları bir araya getirdik. Bu\nfotoğraflar 64x64x3 boyutunda ve 2 adet.\nFakat yukarıdaki format model aşamasında çalışmayacaktır. Aşağıdaki\ngibi yeniden sıralayıp çalışmasını sağlayacağız.\n\n\ntrain <- aperm(train, c(4,1,2,3)) # 268, 64, 64, 3\ntest <- aperm(test, c(4,1,2,3)) # 2, 64, 64, 3\n\n\n\nEtiketleri belirleyelim.\n\n\ntrain_y <- c(\n  rep(0,length(imgs_akp)), # AKP: 0\n  rep(1,length(imgs_chp)) # CHP: 1\n)\n\ntest_y <- c(\n  0, # AKP\n  1 # CHP\n)\n\n\n\nKategorik değişkenleri ikili olarak temsil eden One Hot Encoding\nyapacağız.\n\n\ntrainLabels <- to_categorical(train_y)\ntestLabels <- to_categorical(test_y)\n\n\n\nİlk kolonun AKP’yi; ikinci kolonun CHP’yi temsil ettiğini\ngörebiliriz.\n\n\nhead(trainLabels)\n\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    1    0\n[3,]    1    0\n[4,]    1    0\n[5,]    1    0\n[6,]    1    0\n\ntail(trainLabels)\n\n\n       [,1] [,2]\n[263,]    0    1\n[264,]    0    1\n[265,]    0    1\n[266,]    0    1\n[267,]    0    1\n[268,]    0    1\n\nModel kurma aşamasına geçmeden önce Keras ve Tensorflow’a\nbakalım.\nKeras, Python ile yazılmış etkili bir üst düzey sinir ağı API’dır.\nKeras kütüphanesi CNTK, TensorFlow ve Theano üzerinde çalışabilir.\nTensorFlow ise uçtan uca açık kaynak bir derin öğrenme framework’üdür.\nYani, derin öğrenme için geliştirilmiş açık kaynak bir matematik\nkütüphanesidir.\nŞimdi modele geçebiliriz.\n\n\nmodel <- keras_model_sequential(name = \"CNN\") %>%\n  layer_conv_2d(\n    filters = 8,\n    kernel_size = c(5, 5),\n    activation = \"relu\",\n    input_shape = c(64, 64, 3)\n  ) %>% # Convolution, ReLU\n  layer_max_pooling_2d(pool_size = c(3, 3)) %>% # Max Pooling\n  layer_flatten() %>% # Flattening\n  layer_dense(units = 16,\n              activation = \"relu\") %>% \n  layer_dense(units = 2,\n              activation = \"sigmoid\",\n              name = \"Output\") %>% \n  compile(\n    loss = \"binary_crossentropy\",\n    optimizer = optimizer_adam(), # Adam\n    metrics = \"accuracy\"\n  )\n\n\n\n\n\nhistory <- model %>% \n  fit(\n    train,\n    trainLabels,\n    epochs = 50,\n    batch_size = 64,\n    validation_split = 0.1\n  )\n\nplot(history)\n\n\n\n\nAşağıda hata matrisi (confusion matrix) oluşturalım.\n\n\nlibrary(tidyverse)\n\ntrain_prob <- model %>% \n  predict(train) %>% \n  as.data.frame() %>% \n  rename(\"AKP\"=1,\"CHP\"=2) %>% \n  mutate(\"Actual\" = if_else(train_y == 0, \"AKP\", \"CHP\")) %>% \n  mutate(\"Pred\" = if_else(AKP > 0.5, \"AKP\", \"CHP\"))\n\ntbl1 <- table(Predicted = train_prob$Pred, Actual = train_prob$Actual)\ntbl1\n\n\n         Actual\nPredicted AKP CHP\n      AKP 124  31\n      CHP  10 103\n\nHata matrisine göre 227 milletvekilinin partisini doğru; 41\nmilletvekilinin partisini yanlış tahmin etti. Buradan da Doğruluk oranı\n(Accuracy rate) 0.8470149 olur.\n\n\ntest_prob <- model %>% \n  predict(test) %>% \n  as.data.frame() %>% \n  rename(\"AKP\"=1,\"CHP\"=2) %>% \n  mutate(\"Actual\" = if_else(test_y == 0, \"AKP\", \"CHP\")) %>% \n  mutate(\"Pred\" = if_else(AKP > 0.5, \"AKP\", \"CHP\"))\n\ntbl2 <- table(Predicted = test_prob$Pred, Actual = test_prob$Actual)\ntbl2\n\n\n         Actual\nPredicted AKP CHP\n      AKP   1   0\n      CHP   0   1\n\nTest verilerini kullandığımızda karşımıza aşağıdaki gibi bir sonuç\nçıkmaktadır.\n\n\n\nErdoğan’ın AKP’li bir yüze sahip olma olasılığı 0.66 iken;\nKılıçdaroğlu’nun CHP’li bir yüze sahip olma olasılığı 0.63\nBu uygulamanın ardından güzel bir Shiny Web uygulaması yapıp\npaylaşmak isterdim ancak hem veri setinin yetersiz oluşu hem de etik\nsebepler ile böyle bir şeyi paylaşmayı uygun görmedim. Çalışmaya konu\nveri setindeki fotoğrafların bugün itibarıyla hangi\netiketin/sınıfın/kategorinin/partinin içinden olduğu net bir şekilde\nbellidir ancak dışarıdan uygulamaya girecek bir fotoğraf için bu tahmin\nşu şartlarda riskli olabilir. Çalışmanın eğitim amaçlı paylaşıldığını ve\nbu alanda neler yapılabileceği konusunda fikir vermeyi amaçladığını\nbelirtmek isterim.\nÇalışmanın diğer R kodlarına aşağıdan ulaşabilirsiniz.\n\n\n# Python entegrasyonu\n\nlibrary(reticulate)\nuse_python(\"C:/Users/.../Documents/.virtualenvs/myenv/Scripts/python.exe\")\n\n# Web kazıma ile verilerin elde edilmesi\n\nlibrary(rvest)\nlibrary(tidyverse)\n\nurlsList <- read_html(\"https://www.tbmm.gov.tr/Milletvekilleri/liste\") %>% \n  html_nodes(\"div.col-md-8 a\") %>% \n  html_attr(\"href\") %>% \n  as.data.frame() %>% \n  rename(\"urls\"=1) %>% \n  mutate(urls = paste0(\"https://www.tbmm.gov.tr\",urls))\n\npartyList <- read_html(\"https://www.tbmm.gov.tr/Milletvekilleri/liste\") %>% \n  html_nodes(\"div.col-md-4.text-right\") %>% \n  html_text() %>% \n  as.data.frame() %>% \n  rename(\"party\"=1) %>% \n  mutate(party = str_trim(party),\n         party = gsub(\"İ\",\"I\",party),\n         party = gsub(\"Ğ\",\"G\",party))\n\nmaster <- cbind(urlsList,partyList) %>% \n  filter(party %in% c(\"AK Parti\",\"CHP\")) %>% \n  mutate(party = if_else(party == \"AK Parti\",\"AKP\",party))\n\nfor(i in 1:nrow(master)){\n  \n  img_mop <- read_html(master$urls[i]) %>% \n    html_nodes(\"img\") %>% \n    .[[3]] %>% \n    html_attr(\"src\")\n  \n  download.file(img_mop,\n                paste0(\"C:/.../\", # fotoğrafların ineceği yer\n                       master$party[i],\"_\",i,\".jpg\"), mode = \"wb\")\n  \n}\n\n# Test görselleştirme\n\ndf <- data.frame(\n  Test = c(\"Recep Tayyip Erdoğan\", \"Kemal Kılıçdaroğlu\"),\n  AKP = c(test_prob[1,1],test_prob[2,1]),\n  CHP = c(test_prob[1,2],test_prob[2,2]),\n  Img = c(\"https://pbs.twimg.com/profile_images/1151410974240444416/yVvaD7hU_400x400.jpg\",\n          \"https://pbs.twimg.com/profile_images/1446836678832922629/UB3uap87_400x400.jpg\")\n) %>% \n  pivot_longer(!c(Test,Img), names_to = \"Party\", values_to = \"Prob\")\n\nggplot(df, aes(x = Test, y = Prob, fill = Party)) +\n  geom_bar(stat = \"identity\", position = \"dodge\", alpha = .5) +\n  ggimage::geom_image(aes(image = Img), size = 0.15,\n                      position = position_dodge(width = 0.9)) +\n  scale_fill_manual(values = c(\"orange\",\"red\")) +\n  theme_minimal() +\n  theme(plot.title = element_text(face = \"bold\", hjust = 0.5),\n        legend.title = element_blank()) +\n  scale_y_continuous(limits = c(0,1)) +\n  labs(title = \"İki Politik Liderin Yüzleri Hangi Olasılıkla Hangi Partiye Ait?\",\n       y = \"Olasılık\", x = \"\")\n\n\n\n\n\n\n",
    "preview": "posts/2022-07-03-post14/post14_files/figure-html5/unnamed-chunk-30-1.png",
    "last_modified": "2022-07-03T17:50:12+03:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-06-19-post13/",
    "title": "Çok Kriterli Karar Verme Yöntemleri ile Yaşanacak İl Seçimi",
    "description": "Yaşam endeksi verileri ve TOPSIS yöntemi kullanılarak illerin sıralanması.",
    "author": [
      {
        "name": "A. Uraz Akgül",
        "url": {}
      }
    ],
    "date": "2022-06-19",
    "categories": [
      "Decision Making"
    ],
    "contents": "\r\nYaşayacağımız yere karar verme süreci içerisinde olduğumuzu ve bunu\r\nda çok kriterli karar verme yöntemleri ile yapmak istediğimizi\r\nvarsayalım. Böyle bir süreç ve düşüncenin içinde en uygun kararı\r\nverebilmek için bir veri setine ihtiyacımız olacak. Bu veri setini\r\nTÜİK’ten alacağız. TÜİK, 2016 yılında İllerde\r\nYaşam Endeksi, 2015 başlıklı bir çalışma paylaşmıştı ve en yüksek\r\nendeks değerini Isparta almıştı. Çalışmaya konu endeks, yaşamın 11\r\nboyutunu 41 gösterge ile temsil eden bir bileşik endekstir.\r\nBiz karar verme sürecimizde bu 41 göstergeyi kullanmak zorunda\r\ndeğiliz. Hatta ağırlıkları da önem derecemize göre değiştirebiliriz.\r\nUygulamanın bundan sonraki kısmında subjektif değerlendirmelerim ile\r\ndevam edeceğim için sizler göstergeleri ve ağırlıkları\r\ndeğiştirebilirsiniz.\r\n11 boyut ve 41 gösterge içerisinden seçmiş olduğum boyut ve\r\ngöstergeler aşağıdadır. Listeye aynı zamanda ilgili göstergenin pozitif\r\nve negatif bilgisini ve ileride ihtiyacımız olacak ağırlıkları\r\n(toplamları 1 olacak) da ekledim.\r\n(Konut, -) Konutun kalitesinde problem yaşayanların oranı,\r\n0.02\r\n(Çalışma hayatı, -) İşsizlik oranı, 0.03\r\n(Sağlık, +) Kamunun sağlık hizmetlerinden memnuniyet oranı,\r\n0.05\r\n(Eğitim, +) Okul öncesi eğitimde (3-5 yaş) net okullaşma oranı,\r\n0.03\r\n(Eğitim, +) Kamunun eğitim hizmetlerinden memnuniyet oranı,\r\n0.04\r\n(Çevre, -) PM10 istasyon değerleri ortalaması (hava kirliliği),\r\n0.09\r\n(Çevre, +) Km2’ye düşen orman alanı, 0.09\r\n(Çevre, -) Sokaktan gelen gürültü problemi yaşayanların oranı,\r\n0.07\r\n(Çevre, +) Belediyenin temizlik hizmetlerinden memnuniyet oranı,\r\n0.06\r\n(Güvenlik, -) Cinayet oranı, 0.05\r\n(Güvenlik, +) Gece yalnız yürürken kendini güvende hissedenlerin\r\noranı, 0.08\r\n(Güvenlik, +) Kamunun asayiş hizmetlerinden memnuniyet oranı,\r\n0.05\r\n(Altyapı hizmetlerine erişim, +) İnternet abone sayısı (yüz\r\nkişide), 0.02\r\n(Altyapı hizmetlerine erişim, +) Kanalizasyon ve şebeke suyuna\r\nerişim oranı, 0.05\r\n(Altyapı hizmetlerine erişim, +) Havalimanına erişim oranı,\r\n0.03\r\n(Altyapı hizmetlerine erişim, +) Belediyenin toplu taşıma\r\nhizmetlerinden memnuniyet oranı, 0.05\r\n(Sosyal yaşam, +) Sinema ve tiyatro seyirci sayısı (yüz kişide),\r\n0.06\r\n(Sosyal yaşam, +) Bin kişi başına düşen alışveriş merkezi alanı,\r\n0.01\r\n(Sosyal yaşam, +) Sosyal hayatından memnuniyet oranı,\r\n0.06\r\n(Yaşam memnuniyeti, +) Mutluluk düzeyi, 0.06\r\n11 boyutu 9’a, 41 göstergeyi 20’ye düşürmüş oldum. Verilere\r\n(post13.xls) GitHub\r\nhesabımdan ulaşabilirsiniz. Eğer TÜİK üzerinden veri setine ulaşmak\r\nisterseniz Gelir, Yaşam, Tüketim ve Yoksulluk kategorisine\r\ngirdikten sonra İllerde Yaşam Endeksi alt kategorisini\r\nfiltreleyip İstatistiksel Tablolar bölümünden indirebilirsiniz.\r\nHedef dosya: İllerde Yaşam Endeksi Gösterge Değerleridir.\r\nAşağıda örnek olması açısından İstanbul ve Muğla’ya ait gösterge\r\ndeğerleri verilmiştir.\r\n\r\n\r\nindicators\r\n\r\n\r\nİstanbul\r\n\r\n\r\nMuğla\r\n\r\n\r\nkonut_kalite\r\n\r\n\r\n15.95000\r\n\r\n\r\n17.56500\r\n\r\n\r\nissizlik\r\n\r\n\r\n11.20000\r\n\r\n\r\n7.30000\r\n\r\n\r\nsaglik_hizmet\r\n\r\n\r\n67.43000\r\n\r\n\r\n71.48000\r\n\r\n\r\nokul_oncesi\r\n\r\n\r\n27.51277\r\n\r\n\r\n38.33912\r\n\r\n\r\negitim_hizmet\r\n\r\n\r\n61.59000\r\n\r\n\r\n66.79000\r\n\r\n\r\npm10_istasyon\r\n\r\n\r\n54.80000\r\n\r\n\r\n81.00000\r\n\r\n\r\norman_alan\r\n\r\n\r\n43.99341\r\n\r\n\r\n66.48696\r\n\r\n\r\nsokak_gurultu\r\n\r\n\r\n33.75000\r\n\r\n\r\n17.19000\r\n\r\n\r\ntemizlik_hizmet\r\n\r\n\r\n80.69000\r\n\r\n\r\n70.08000\r\n\r\n\r\ncinayet_oran\r\n\r\n\r\n22.53597\r\n\r\n\r\n52.54279\r\n\r\n\r\ngece_yalniz\r\n\r\n\r\n45.10000\r\n\r\n\r\n63.82000\r\n\r\n\r\nasayis_hizmet\r\n\r\n\r\n68.51000\r\n\r\n\r\n79.77000\r\n\r\n\r\ninternet_abone\r\n\r\n\r\n17.62955\r\n\r\n\r\n14.49276\r\n\r\n\r\nkanalizasyon_sebeke\r\n\r\n\r\n100.00000\r\n\r\n\r\n83.00000\r\n\r\n\r\nhavalimani_erisim\r\n\r\n\r\n9874.83432\r\n\r\n\r\n318.22520\r\n\r\n\r\ntoplu_tasima_hizmet\r\n\r\n\r\n69.89000\r\n\r\n\r\n53.33000\r\n\r\n\r\nsinema_tiyatro\r\n\r\n\r\n147.44083\r\n\r\n\r\n56.06305\r\n\r\n\r\navm_alani\r\n\r\n\r\n277.27405\r\n\r\n\r\n175.16313\r\n\r\n\r\nsosyal_hayat\r\n\r\n\r\n51.41000\r\n\r\n\r\n52.05000\r\n\r\n\r\nmutluluk\r\n\r\n\r\n58.40000\r\n\r\n\r\n52.40000\r\n\r\n\r\nTechnique for Order Preference by Similarity to Ideal Solution’ın\r\nkısaltması ve çok kriterli karar verme yöntemlerinden biri olan TOPSIS,\r\nİdeal Çözüme Dayalı Sıralama Tekniği olarak çevrilebilir. TOPSIS’in\r\ngizemli mantığı, seçilen alternatifin en iyi çözümden en kısa geometrik\r\nuzaklığa ve en kötü çözümden en uzun geometrik uzaklığa sahip olması\r\ngerektiği kavramına dayanmaktadır.\r\nTOPSIS yönteminin hem adımlarına bakalım hem de bu adımları\r\nuygulayalım.\r\nAdım 0) Veri seti: 81 ilin 9 boyut ve 20 göstergede değerleri\r\n\r\n\r\nlibrary(readxl)\r\nlibrary(tidyverse)\r\n\r\nendeks <- read_excel(\"yasam_endeksi.xls\")\r\n\r\n\r\n\r\nAdım 1) Karar matrisinin yaratılması\r\nKarar matrisinin satırları alternatiflerden (iller), sütunları\r\nkriterlerden (göstergeler) oluşmaktadır.\r\nAlternatifler M, kriterler N olsun. Karar matrisi:\r\n\\((a_{ij})_{MxN}\\)\r\n\r\n\r\nil <- endeks$il # kullanmak için il sütunu ayrıldı\r\nm <- as.matrix(endeks[,-1]) # matris oluşturuldu\r\n\r\n\r\n\r\nAdım 2) Karar Matrisinin Normalize Edilmesi\r\n\\(a_{ij} =\r\n\\frac{a_{ij}}{\\sqrt{\\sum_{i=1}^{M}(a_{ij}^2)}}\\)\r\n\r\n\r\nm_denominator <- apply(m, 2, function(x) sqrt(sum(x**2))) # payda burada hesaplandı\r\nm2 <- m # m matrisi haricinde ikinci bir matris oluşturuldu\r\n\r\nfor(col in 1:ncol(m2)){\r\n  \r\n  for(row in 1:nrow(m2)){\r\n    \r\n    m2[row,col] <- m2[row,col] / m_denominator[col]\r\n    \r\n  }\r\n  \r\n}\r\n\r\n\r\n\r\nÖrnek: konut_kalite sütunundaki ilk değer 22.73’tür. 22.73’ü ilgili\r\nsütunda yer alan değerlerin karelerinin toplamının kareköküne bölüyoruz.\r\nBu değer ise 203.314’tür. Sonuç olarak 22.73/203.314 =\r\n0.1117975’tir.\r\nAdım 3) Ağırlıklı Normalleştirilmiş Karar Matrisinin\r\nOluşturulması\r\n\\(x_{ij} = a_{ij} * w_j\\)\r\n\\(w_j =\r\n\\frac{w_j}{\\sum_{j=1}^{N}w_j}\\)\r\n\\(\\sum_{j=1}^{N}w_j = 1\\)\r\nGöstergelere ilişkin ağırlık değerlerini toplamı 1 olacak şekilde\r\nbelirleyelim. Eşit ağırlıklandırma yapsak 1/20 = 0.05 oluyor. Hepsine\r\n0.05 atadım ve aralarındaki karşılaştırmaya göre birini artırıp diğerini\r\nazalttım.\r\n\r\n\r\nmyWeights <- data.frame(\r\n  Indicator = names(endeks[,-1]),\r\n  Weight = c(0.02,0.03,0.05,0.03,0.04,0.09,0.09,0.07,0.06,0.05,\r\n             0.08,0.05,0.02,0.05,0.03,0.05,0.06,0.01,0.06,0.06)\r\n)\r\n\r\n\r\n\r\n\r\n\r\nIndicator\r\n\r\n\r\nWeight\r\n\r\n\r\nkonut_kalite\r\n\r\n\r\n0.02\r\n\r\n\r\nissizlik\r\n\r\n\r\n0.03\r\n\r\n\r\nsaglik_hizmet\r\n\r\n\r\n0.05\r\n\r\n\r\nokul_oncesi\r\n\r\n\r\n0.03\r\n\r\n\r\negitim_hizmet\r\n\r\n\r\n0.04\r\n\r\n\r\npm10_istasyon\r\n\r\n\r\n0.09\r\n\r\n\r\norman_alan\r\n\r\n\r\n0.09\r\n\r\n\r\nsokak_gurultu\r\n\r\n\r\n0.07\r\n\r\n\r\ntemizlik_hizmet\r\n\r\n\r\n0.06\r\n\r\n\r\ncinayet_oran\r\n\r\n\r\n0.05\r\n\r\n\r\ngece_yalniz\r\n\r\n\r\n0.08\r\n\r\n\r\nasayis_hizmet\r\n\r\n\r\n0.05\r\n\r\n\r\ninternet_abone\r\n\r\n\r\n0.02\r\n\r\n\r\nkanalizasyon_sebeke\r\n\r\n\r\n0.05\r\n\r\n\r\nhavalimani_erisim\r\n\r\n\r\n0.03\r\n\r\n\r\ntoplu_tasima_hizmet\r\n\r\n\r\n0.05\r\n\r\n\r\nsinema_tiyatro\r\n\r\n\r\n0.06\r\n\r\n\r\navm_alani\r\n\r\n\r\n0.01\r\n\r\n\r\nsosyal_hayat\r\n\r\n\r\n0.06\r\n\r\n\r\nmutluluk\r\n\r\n\r\n0.06\r\n\r\n\r\nŞimdi her bir ağırlığı o göstergeye ait sütunda yer alan değerler ile\r\nçarpacağız. Böylece Ağırlıklı Normalleştirilmiş Karar Matrisi\r\noluşacak.\r\n\r\n\r\nfor(col in 1:ncol(m2)){\r\n  \r\n  for(row in 1:nrow(m2)){\r\n    \r\n    m2[row,col] <- m2[row,col] * myWeights$Weight[col]\r\n    \r\n  }\r\n  \r\n}\r\n\r\n\r\n\r\nAdım 4) En İyi ve En Kötü Alternatifin Belirlenmesi\r\nTüm iller arasından her bir göstergenin maksimum ve minimum değeri\r\nbulunur.\r\n\\(x_j^b = max_{i=1}^{M} x_{ij}\\)\r\n\\(x_j^w = min_{i=1}^{M} x_{ij}\\)\r\nAğırlıklı Normalleştirilmiş Karar Matrisi’nde her bir göstergenin\r\nilgili sütunundan pozitif ideal çözüm için en büyük değer, negatif ideal\r\nçözüm için ise en küçük değer seçilmiştir. Dikkat etmemiz gereken nokta,\r\npozitif ideal çözüm için eğer gösterge pozitif ise maksimum; negatif ise\r\nminimum değer; negatif ideal çözüm için eğer gösterge pozitif ise\r\nminimum; negatif ise maksimum değer seçilir.\r\n\r\n\r\n# pos: positive\r\n# neg: negative\r\n\r\nmyDirections <- data.frame(\r\n  Indicator = names(endeks[,-1]),\r\n  Direction = c(\"neg\",\"neg\",\"pos\",\"pos\",\r\n                \"pos\",\"neg\",\"pos\",\"neg\",\r\n                \"pos\",\"neg\",\"pos\",\"pos\",\r\n                \"pos\",\"pos\",\"pos\",\"pos\",\r\n                \"pos\",\"pos\",\"pos\",\"pos\")\r\n)\r\n\r\n\r\n\r\n\r\n\r\nIndicator\r\n\r\n\r\nDirection\r\n\r\n\r\nkonut_kalite\r\n\r\n\r\nneg\r\n\r\n\r\nissizlik\r\n\r\n\r\nneg\r\n\r\n\r\nsaglik_hizmet\r\n\r\n\r\npos\r\n\r\n\r\nokul_oncesi\r\n\r\n\r\npos\r\n\r\n\r\negitim_hizmet\r\n\r\n\r\npos\r\n\r\n\r\npm10_istasyon\r\n\r\n\r\nneg\r\n\r\n\r\norman_alan\r\n\r\n\r\npos\r\n\r\n\r\nsokak_gurultu\r\n\r\n\r\nneg\r\n\r\n\r\ntemizlik_hizmet\r\n\r\n\r\npos\r\n\r\n\r\ncinayet_oran\r\n\r\n\r\nneg\r\n\r\n\r\ngece_yalniz\r\n\r\n\r\npos\r\n\r\n\r\nasayis_hizmet\r\n\r\n\r\npos\r\n\r\n\r\ninternet_abone\r\n\r\n\r\npos\r\n\r\n\r\nkanalizasyon_sebeke\r\n\r\n\r\npos\r\n\r\n\r\nhavalimani_erisim\r\n\r\n\r\npos\r\n\r\n\r\ntoplu_tasima_hizmet\r\n\r\n\r\npos\r\n\r\n\r\nsinema_tiyatro\r\n\r\n\r\npos\r\n\r\n\r\navm_alani\r\n\r\n\r\npos\r\n\r\n\r\nsosyal_hayat\r\n\r\n\r\npos\r\n\r\n\r\nmutluluk\r\n\r\n\r\npos\r\n\r\n\r\n\r\n\r\npos_ideal <- numeric()\r\nneg_ideal <- numeric()\r\ncriteria <- colnames(m2)\r\n\r\nfor(col in 1:ncol(m2)){\r\n  \r\n  # pozitif taraf için;\r\n  \r\n  p_name <- criteria[col]\r\n  p_filter <- myDirections %>% \r\n    filter(Indicator == p_name) %>% \r\n    pull(Direction)\r\n  \r\n  p_ideal <- if_else(p_filter == \"pos\", max(m2[,col]), min(m2[,col]))\r\n  pos_ideal <- c(pos_ideal,p_ideal)\r\n  \r\n  # negatif taraf için;\r\n  \r\n  n_name <- criteria[col]\r\n  n_filter <- myDirections %>% \r\n    filter(Indicator == n_name) %>% \r\n    pull(Direction)\r\n  \r\n  n_ideal <- if_else(n_filter == \"neg\", max(m2[,col]), min(m2[,col]))\r\n  neg_ideal <- c(neg_ideal,n_ideal)\r\n  \r\n}\r\n\r\npos_neg_ideal <- data.frame(\r\n  \"Indicator\" = myDirections$Indicator,\r\n  \"PositiveIdeal\" = pos_ideal,\r\n  \"NegativeIdeal\" = neg_ideal\r\n)\r\n\r\n\r\n\r\n\r\n\r\nIndicator\r\n\r\n\r\nPositiveIdeal\r\n\r\n\r\nNegativeIdeal\r\n\r\n\r\nkonut_kalite\r\n\r\n\r\n0.0009227\r\n\r\n\r\n0.0043996\r\n\r\n\r\nissizlik\r\n\r\n\r\n0.0014571\r\n\r\n\r\n0.0081181\r\n\r\n\r\nsaglik_hizmet\r\n\r\n\r\n0.0063640\r\n\r\n\r\n0.0038950\r\n\r\n\r\nokul_oncesi\r\n\r\n\r\n0.0049470\r\n\r\n\r\n0.0021901\r\n\r\n\r\negitim_hizmet\r\n\r\n\r\n0.0052983\r\n\r\n\r\n0.0028718\r\n\r\n\r\npm10_istasyon\r\n\r\n\r\n0.0030563\r\n\r\n\r\n0.0191867\r\n\r\n\r\norman_alan\r\n\r\n\r\n0.0192523\r\n\r\n\r\n0.0000121\r\n\r\n\r\nsokak_gurultu\r\n\r\n\r\n0.0029607\r\n\r\n\r\n0.0157113\r\n\r\n\r\ntemizlik_hizmet\r\n\r\n\r\n0.0089772\r\n\r\n\r\n0.0031494\r\n\r\n\r\ncinayet_oran\r\n\r\n\r\n0.0008939\r\n\r\n\r\n0.0138624\r\n\r\n\r\ngece_yalniz\r\n\r\n\r\n0.0113660\r\n\r\n\r\n0.0058765\r\n\r\n\r\nasayis_hizmet\r\n\r\n\r\n0.0062340\r\n\r\n\r\n0.0038695\r\n\r\n\r\ninternet_abone\r\n\r\n\r\n0.0041353\r\n\r\n\r\n0.0005065\r\n\r\n\r\nkanalizasyon_sebeke\r\n\r\n\r\n0.0073085\r\n\r\n\r\n0.0022736\r\n\r\n\r\nhavalimani_erisim\r\n\r\n\r\n0.0193698\r\n\r\n\r\n0.0000000\r\n\r\n\r\ntoplu_tasima_hizmet\r\n\r\n\r\n0.0073563\r\n\r\n\r\n0.0021898\r\n\r\n\r\nsinema_tiyatro\r\n\r\n\r\n0.0173789\r\n\r\n\r\n0.0000338\r\n\r\n\r\navm_alani\r\n\r\n\r\n0.0032476\r\n\r\n\r\n0.0000000\r\n\r\n\r\nsosyal_hayat\r\n\r\n\r\n0.0097283\r\n\r\n\r\n0.0025860\r\n\r\n\r\nmutluluk\r\n\r\n\r\n0.0084035\r\n\r\n\r\n0.0045426\r\n\r\n\r\nAdım 5) Öklid Uzaklığın Hesaplanması\r\nHedef alternatif ile en iyi ve en kötü alternatif arasındaki Öklid\r\nuzaklığı hesaplanır.\r\n\\(d_i^b =\r\n\\sqrt{\\sum_{j=1}^{N}(x_{ij}-x_j^b)^2}\\)\r\n\\(d_i^w =\r\n\\sqrt{\\sum_{j=1}^{N}(x_{ij}-x_j^w)^2}\\)\r\n\r\n\r\nb_euclidean <- m2\r\nw_euclidean <- m2\r\n\r\nfor(row in 1:nrow(m2)){\r\n  \r\n  for(col in 1:ncol(m2)){\r\n    \r\n    b_euclidean[row, col] <- (b_euclidean[row, col] - pos_neg_ideal$PositiveIdeal[col]) ** 2\r\n    w_euclidean[row, col] <- (w_euclidean[row, col] - pos_neg_ideal$NegativeIdeal[col]) ** 2\r\n    \r\n  }\r\n  \r\n}\r\n\r\nb_euclidean <- sqrt(rowSums(b_euclidean))\r\nw_euclidean <- sqrt(rowSums(w_euclidean))\r\n\r\ndf_euclidean <- data.frame(\r\n  prov_num = seq(1,length(il),1),\r\n  b_euclidean = b_euclidean,\r\n  w_euclidean = w_euclidean\r\n)\r\n\r\n\r\n\r\n\r\n\r\nprov_num\r\n\r\n\r\nb_euclidean\r\n\r\n\r\nw_euclidean\r\n\r\n\r\n1\r\n\r\n\r\n0.0201152\r\n\r\n\r\n0.0259480\r\n\r\n\r\n2\r\n\r\n\r\n0.0322445\r\n\r\n\r\n0.0180609\r\n\r\n\r\n3\r\n\r\n\r\n0.0303864\r\n\r\n\r\n0.0221779\r\n\r\n\r\n4\r\n\r\n\r\n0.0345631\r\n\r\n\r\n0.0185534\r\n\r\n\r\n5\r\n\r\n\r\n0.0271004\r\n\r\n\r\n0.0257582\r\n\r\n\r\n6\r\n\r\n\r\n0.0254241\r\n\r\n\r\n0.0239853\r\n\r\n\r\n7\r\n\r\n\r\n0.0217484\r\n\r\n\r\n0.0258145\r\n\r\n\r\n8\r\n\r\n\r\n0.0251323\r\n\r\n\r\n0.0285319\r\n\r\n\r\n9\r\n\r\n\r\n0.0257707\r\n\r\n\r\n0.0221523\r\n\r\n\r\n10\r\n\r\n\r\n0.0247963\r\n\r\n\r\n0.0254779\r\n\r\n\r\nAdım 6) TOPSIS Skorunun Elde Edilmesi\r\nHer alternatif için en kötü alternatife olan benzerliğinin\r\nhesaplandığı adımdır. Burada, TOPSIS skoru elde edilir.\r\n\\(s_i = \\frac{d_i^w}{d_i^w +\r\nd_i^b}\\)\r\n\r\n\r\ntopsis_score <- df_euclidean %>% \r\n  mutate(\r\n    score = w_euclidean / (w_euclidean + b_euclidean),\r\n    province = il\r\n  ) %>% \r\n  arrange(desc(score)) %>% \r\n  mutate(rank = seq(1,nrow(.),1))\r\n\r\n\r\n\r\n\r\n\r\nprov_num\r\n\r\n\r\nb_euclidean\r\n\r\n\r\nw_euclidean\r\n\r\n\r\nscore\r\n\r\n\r\nprovince\r\n\r\n\r\nrank\r\n\r\n\r\n34\r\n\r\n\r\n0.0182616\r\n\r\n\r\n0.0338457\r\n\r\n\r\n0.6495390\r\n\r\n\r\nİstanbul\r\n\r\n\r\n1\r\n\r\n\r\n77\r\n\r\n\r\n0.0181110\r\n\r\n\r\n0.0305289\r\n\r\n\r\n0.6276518\r\n\r\n\r\nYalova\r\n\r\n\r\n2\r\n\r\n\r\n61\r\n\r\n\r\n0.0194185\r\n\r\n\r\n0.0269682\r\n\r\n\r\n0.5813780\r\n\r\n\r\nTrabzon\r\n\r\n\r\n3\r\n\r\n\r\n1\r\n\r\n\r\n0.0201152\r\n\r\n\r\n0.0259480\r\n\r\n\r\n0.5633130\r\n\r\n\r\nAdana\r\n\r\n\r\n4\r\n\r\n\r\n78\r\n\r\n\r\n0.0234771\r\n\r\n\r\n0.0301615\r\n\r\n\r\n0.5623088\r\n\r\n\r\nKarabük\r\n\r\n\r\n5\r\n\r\n\r\n35\r\n\r\n\r\n0.0204943\r\n\r\n\r\n0.0258802\r\n\r\n\r\n0.5580705\r\n\r\n\r\nİzmir\r\n\r\n\r\n6\r\n\r\n\r\n17\r\n\r\n\r\n0.0225074\r\n\r\n\r\n0.0270572\r\n\r\n\r\n0.5458984\r\n\r\n\r\nÇanakkale\r\n\r\n\r\n7\r\n\r\n\r\n7\r\n\r\n\r\n0.0217484\r\n\r\n\r\n0.0258145\r\n\r\n\r\n0.5427442\r\n\r\n\r\nAntalya\r\n\r\n\r\n8\r\n\r\n\r\n37\r\n\r\n\r\n0.0254867\r\n\r\n\r\n0.0293201\r\n\r\n\r\n0.5349719\r\n\r\n\r\nKastamonu\r\n\r\n\r\n9\r\n\r\n\r\n8\r\n\r\n\r\n0.0251323\r\n\r\n\r\n0.0285319\r\n\r\n\r\n0.5316751\r\n\r\n\r\nArtvin\r\n\r\n\r\n10\r\n\r\n\r\nAdım 7) TOPSIS Skorunun Büyükten Küçüğe Doğru Sıralanması\r\nAşağıda ilk 10 il gösterilmiştir.\r\n\r\n\r\ntopsis_score_desc <- topsis_score %>% \r\n  select(province,score,rank)\r\n\r\n\r\n\r\n\r\n\r\nprovince\r\n\r\n\r\nscore\r\n\r\n\r\nrank\r\n\r\n\r\nİstanbul\r\n\r\n\r\n0.6495390\r\n\r\n\r\n1\r\n\r\n\r\nYalova\r\n\r\n\r\n0.6276518\r\n\r\n\r\n2\r\n\r\n\r\nTrabzon\r\n\r\n\r\n0.5813780\r\n\r\n\r\n3\r\n\r\n\r\nAdana\r\n\r\n\r\n0.5633130\r\n\r\n\r\n4\r\n\r\n\r\nKarabük\r\n\r\n\r\n0.5623088\r\n\r\n\r\n5\r\n\r\n\r\nİzmir\r\n\r\n\r\n0.5580705\r\n\r\n\r\n6\r\n\r\n\r\nÇanakkale\r\n\r\n\r\n0.5458984\r\n\r\n\r\n7\r\n\r\n\r\nAntalya\r\n\r\n\r\n0.5427442\r\n\r\n\r\n8\r\n\r\n\r\nKastamonu\r\n\r\n\r\n0.5349719\r\n\r\n\r\n9\r\n\r\n\r\nArtvin\r\n\r\n\r\n0.5316751\r\n\r\n\r\n10\r\n\r\n\r\nBirinci sırada yaşadığım il olan İstanbul çıktı. TÜİK’in endeksinde\r\nbirinci olan Isparta ise bu veri setine ve ağırlıklandırmalarıma göre\r\nyaşayacağım iller arasında 16. olmuştur.\r\nSon olarak, topsis paketi ile aşağıdaki gibi çok kısa bir\r\nsüre içerisinde sonuçlar elde edilebilir.\r\n\r\n\r\nlibrary(topsis)\r\n\r\nm <- as.matrix(endeks[,-1]) # matris\r\nw <- myWeights$Weight # ağırlıklar\r\ni <- myDirections %>% \r\n  mutate(Direction = ifelse(Direction == \"neg\", \"-\", \"+\")) %>% \r\n  pull(Direction) # yönler\r\n\r\ndf <- topsis(m,w,i) %>% \r\n  mutate(province = il) %>% \r\n  arrange(desc(score))\r\n\r\n\r\n\r\n\r\n\r\nggplot(df %>% arrange(desc(score)) %>% slice(1:10),\r\n       aes(x = reorder(province,score), y = score, fill = score)) +\r\n  geom_col() +\r\n  coord_flip() +\r\n  theme_minimal() +\r\n  theme(axis.title = element_blank(),\r\n        legend.title = element_blank()) +\r\n  scale_fill_gradient(low = \"orange\", high = \"blue\")\r\n\r\n\r\n\r\n\r\nYararlandığım Kaynaklar:\r\nWhat\r\nis TOPSIS?\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-06-19-post13/post13_files/figure-html5/unnamed-chunk-19-1.png",
    "last_modified": "2022-06-19T22:47:08+03:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-06-12-post12/",
    "title": "Haber Paylaşımı Yapan Twitter Hesaplarının Benzerliği",
    "description": "Haber paylaşımı yapan Twitter hesaplarının kosinüs benzerliği ile ölçülmesi.",
    "author": [
      {
        "name": "A. Uraz Akgül",
        "url": {}
      }
    ],
    "date": "2022-06-12",
    "categories": [
      "Machine Learning",
      "Text",
      "Social Media"
    ],
    "contents": "\r\nUygulama için belirlemiş olduğum haber paylaşımı yapan 25 adet\r\nTwitter hesabının benzerlik sonuçları aşağıdaki gibidir.\r\n\r\n\r\n\r\nTwitter hesaplarının benzerliğini ölçmek için atılan tweetleri ve\r\nyöntem olarak da kosinüs benzerliğini kullandım.\r\nKosinüs Benzerliği\r\nKosinüs benzerliği, metinler arasındaki vektörel uzaklığı ölçer.\r\nKosinüs benzerliği yöntemi ile aslında trigonometrideki kosinüs\r\nfonksiyonu kullanılıyor. Metinler birer vektör olarak dikkate alınıyor\r\nve iki vektörün birbirleri ile olan açısı ölçülüyor. Bu benzerlik, iki\r\nvektörün çarpımının iki vektörün boylarının çarpımına oranı olarak\r\nhesaplanır.\r\n\\(cos(\\theta) = \\frac{x.y}{||x||||y||} =\r\n\\frac{\\sum_{i=1}^{n}x_iy_i}{\\sqrt{\\sum_{i=1}^{n}x_i^2}\\sqrt{\\sum_{i=1}^{n}y_i^2}}\\)\r\nBasit bir örnek ile nasıl hesaplandığına bakalım.\r\n\r\n\r\nx <- c(3,4,1,0)\r\ny <- c(3,4,4,8)\r\n\r\n\r\n\r\n\\(x.y = 3*3 + 4*4 + 1*4 + 0*8 =\r\n29\\)\r\n\\(||x|| = \\sqrt{3^2 + 4^2 + 1^2 + 0^2} =\r\n5.09902\\)\r\n\\(||y|| = \\sqrt{3^2 + 4^2 + 4^2 + 8^2} =\r\n10.24695\\)\r\n\\(cos(x,y) = cos(\\theta) =\r\n\\frac{29}{5.09902*10.24695} = 0.5550303\\)\r\nKosinüs açısı 0 olursa (\\(cos(\\theta) =\r\n0\\)), iki vektör arasında tam benzerlik vardır (\\(cos(0) = 1\\)).\r\nKosinüs açısı 90 olursa (\\(cos(\\theta) =\r\n90\\)), iki vektör arasında benzerlik yoktur (\\(cos(90) = 0\\)).\r\nManuel yapmak yerine R’da lsa paketi yardımıyla da kosinüs\r\nbenzerliği bulunabilir.\r\n\r\n\r\nas.numeric(lsa::cosine(x,y))\r\n\r\n\r\n[1] 0.5550303\r\n\r\nPeki, metinlerde bu benzerlik nasıl hesaplanıyor? Kısa cevabı kelime\r\nfrekansları ile. Bir örnek ile inceleyelim.\r\n\r\n\r\ntext1 <- \"R programlama dilini öğreniyorum\"\r\ntext2 <- \"Python programalama dilini öğreniyorum\"\r\n\r\n\r\n\r\nYukarıdaki örnekte dilini, öğreniyorum, programlama\r\nkelimeleri ortak; Python ve R kelimeleri farklıdır. Kelime frekanslarını\r\nbirbirine denk gelecek şekilde yazarsak;\r\n\r\n\r\n# dilini, öğreniyorum, programlama, Python, R\r\n\r\ntext1_n <- c(1,1,1,0,1)\r\ntext2_n <- c(1,1,1,1,0)\r\n\r\nas.numeric(lsa::cosine(text1_n,text2_n))\r\n\r\n\r\n[1] 0.75\r\n\r\nGörüldüğü üzere kosinüs benzerliği 0.75 ya da 75% çıkmıştır.\r\nUygulama\r\n25 adet Twitter hesabına ait tweetlere (post12.xlsx) GitHub\r\nhesabımdan ulaşabilirsiniz. İlgili veri setinde sadece hesap adı ve\r\natılan tweetler bulunmaktadır. Retweetler ise çıkarılmıştır.\r\n\r\n\r\naccounts <- c(\r\n  \"@Ahaber\",\r\n  \"@Haberturk\",\r\n  \"@ntv\",\r\n  \"@cnnturk\",\r\n  \"@BirGun_Gazetesi\",\r\n  \"@t24comtr\",\r\n  \"@cumhuriyetgzt\",\r\n  \"@yeniakit\",\r\n  \"@yenisafak\",\r\n  \"@Sabah\",\r\n  \"@trthaber\",\r\n  \"@anadoluajansi\",\r\n  \"@halktvcomtr\",\r\n  \"@tele1comtr\",\r\n  \"@bbcturkce\",\r\n  \"@FOXhaber\",\r\n  \"@gazetesozcu\",\r\n  \"@tgrthabertv\",\r\n  \"@gazeteduvar\",\r\n  \"@euronews_tr\",\r\n  \"@sputnik_TR\",\r\n  \"@dw_turkce\",\r\n  \"@Hurriyet\",\r\n  \"@solhaberportali\",\r\n  \"@DikenComTr\"\r\n)\r\n\r\n\r\n\r\nVeriler aşağıdaki gibi çekilmiştir.\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(rtweet) # tweet\r\nlibrary(tidytext) # düzenli veri formatı\r\nlibrary(stopwords) # Türkçe stop wordler\r\nlibrary(widyr) # tf-idf, kosinüs benzerliği\r\n\r\ndf <- data.frame()\r\n\r\nfor(i in 1:length(accounts)){\r\n  \r\n  # n = Inf ile maksimum tweet sayısı çekilmiştir ki bu da 3250'dir.\r\n  # include_rts parametresi FALSE yapılarak retweetler analiz dışı bırakılmıştır.\r\n  \r\n  timeline <- get_timeline(\r\n    user = accounts[i], n = Inf, include_rts = FALSE\r\n  )\r\n  \r\n  df <- df %>% \r\n    bind_rows(timeline)\r\n  \r\n  print(paste0(accounts[i],\" hesabından \",nrow(timeline),\" adet tweet çekildi.\"))\r\n  \r\n  Sys.sleep(time = 1)\r\n  \r\n}\r\n\r\nmaster <- df %>% \r\n  select(screen_name,text)\r\n\r\n\r\n\r\nTekrar çalıştırmamak için (73930 satırlık bir veri seti) kaydettiğim\r\nveri setini kullanıyorum.\r\n\r\n\r\n\r\n\r\n\r\nmaster <- readxl::read_excel(\"tweets.xlsx\")\r\n\r\n\r\n\r\nBu veri setinden url’leri (http’li) çıkaracağız.\r\n\r\n\r\nmaster <- master %>% \r\n  mutate(text = gsub(\"http.+\",\"\",text))\r\n\r\n\r\n\r\nAynı zamanda, türkçe stop wordleri de çıkarmak faydalı olabilir. Stop\r\nwordler etkisiz kelimelerdir. Sık kullanılan kelimeleri de kapsar desek\r\nyanlış olmaz sanırım.\r\n\r\n\r\ntr_sw <- stopwords(language = \"tr\", source = \"stopwords-iso\")\r\n\r\n\r\n\r\nKosinüs benzerliğini iki yöntemi baz alarak hesaplayacağız: Kelime\r\nfrekansları ve TF-IDF.\r\n\r\n\r\nmaster <- master %>% \r\n  unnest_tokens(output = \"word\", input = \"text\") %>% # kelimeler ayıklandı\r\n  filter(!(word %in% tr_sw)) %>% # stop wordler kaldırıldı\r\n  count(word, screen_name) %>% # kelimeler hesaplara göre saydırıldı\r\n  bind_tf_idf(word, screen_name, n) # TF-IDF hesaplandı\r\n\r\n\r\n\r\nKosinüs Benzerliği - Kelime Frekansları\r\nYukarıda da örneğini gördüğümüz üzere bu başlık altında kosinüs\r\nbenzerliği kelimelerin frekansları kullanılarak bulunmuştur.\r\n\r\n\r\nmaster_freq <- master %>% \r\n  pairwise_similarity(screen_name, word, n, upper = FALSE, sort = TRUE) %>% # n seçildi\r\n  mutate(item = paste0(item1,\"-\",item2), .before = similarity) %>% \r\n  select(-c(item1,item2)) %>% \r\n  rename(\"similarity_freq\"=2)\r\n\r\n\r\n\r\nKosinüs Benzerliği - TF-IDF\r\nTerm Frequency-Inverse Document Frequency’nin kısaltması olan TF-IDF\r\niçin Terim Frekansı-Ters Doküman Frekansı diyebiliriz. Bu hesaplama, bir\r\nkelimenin doküman içerisindeki önemini gösteriyor. TF-IDF, TF ile IDF’in\r\nçarpımından oluşuyor.\r\nTF: Kelimenin dokümandaki tekrar sayısının dokümandaki toplam kelime\r\nsayısına oranıdır.\r\nIDF: Toplam doküman sayısının seçilen kelimenin geçtiği toplam\r\ndoküman sayısına oranının logaritmasıdır. İlgili kelimenin diğer\r\ndokümanlardaki sıklığının artması DF değerini artırır; IDF değerini\r\nazaltır.\r\n\\(w_{i,j} = tf_{i,j}\\ x\\\r\nlog(\\frac{N}{df_i})\\)\r\n\\(tf_{i,j}:\\) i’nin j’de geçme\r\noranı\r\n\\(df_i:\\) i içeren doküman\r\nsayısı\r\n\\(N:\\) Doküman sayısı\r\nÖrnek: “çalarsaat” kelimesini inceleyelim. FOXhaber’in 30,402\r\nkelimesi vardır. Seçtiğimiz kelime ise 261 defa geçmiştir. Bu durumda TF\r\ndeğeri 261/30,402 = 0.008584962 olur. 25 adet Twitter hesabı olduğu için\r\n25 adet doküman vardır. Seçtiğimiz kelime sadece 1 dokümanda (~hesapta)\r\ngeçmektedir. Bu durumda IDF değeri log(25/1) = 3.218876 olur. TF-IDF ise\r\nTF ile IDF’in çarpımıydı. Yani, 0.008584962 * 3.218876 = 0.02763393\r\nsonucuna ulaşılır.\r\n\r\n\r\nmaster_tf_idf <- master %>% \r\n  pairwise_similarity(screen_name, word, tf_idf, upper = FALSE, sort = TRUE) %>% # tf-idf seçildi\r\n  mutate(item = paste0(item1,\"-\",item2), .before = similarity) %>% \r\n  select(-c(item1,item2)) %>% \r\n  rename(\"similarity_tf_idf\"=2)\r\n\r\n\r\n\r\nSonuç\r\n\r\n\r\ndf_result <- master_freq %>% \r\n  inner_join(master_tf_idf, by = \"item\")\r\n\r\n\r\n\r\n\r\n\r\nggplot(df_result,\r\n       aes(x = similarity_freq, y = similarity_tf_idf, color  = similarity_freq)) +\r\n  geom_point(alpha = .2, size = 4) +\r\n  ggrepel::geom_text_repel(\r\n    data = df_result %>% \r\n      arrange(desc(similarity_freq)) %>% \r\n      slice(c(1:10)),\r\n    aes(label = item), size = 3\r\n  ) +\r\n  theme_minimal() +\r\n  theme(plot.title = element_text(face = \"bold\", hjust = 0.5),\r\n        plot.caption = element_text(face = \"italic\"),\r\n        legend.position = \"none\") +\r\n  scale_color_gradient(low = \"orange\", high = \"red\") +\r\n  labs(\r\n    title = \"Haber Paylaşımı Yapan Twitter Hesaplarının Benzerliği\",\r\n    caption = \"Top 10 gösterilmiştir.\\nSağ-üst köşeye gittikçe benzerlik artmaktadır.\",\r\n    x = \"Kelime Frekansları\",\r\n    y = \"TF-IDF\"\r\n  )\r\n\r\n\r\n\r\n\r\nListenin tamamına aşağıdan ulaşılabilir. Liste, benzerlik oranı en\r\nyüksekten en düşüğe doğru kelime frekansına göre hesaplanan benzerliğe\r\ngöre sıralanmıştır. En yüksek benzerliğe sahip hesaplar ~81% ile\r\ntrthaber ve anadoluajansi; en düşük benzerliğe sahip hesaplar ise ~19%\r\nile DikenComTr ve tgrthabertv olmuştur. Kelime frekanslarına göre\r\nbenzerliğin yanında TF-IDF’i de hesaplamıştık. Buna göre, en yüksek\r\nbenzerliğe sahip hesaplar ~21% ile Sabah ve yenisafak; en düşük\r\nbenzerliğe sahip hesaplar ise ~0.15% ile FOXhaber ve DikenComTr\r\nolmuştur.\r\n\r\n\r\nHesaplar\r\n\r\n\r\nKelimeFrekansı_Benzerlik\r\n\r\n\r\nTF-IDF_Benzerlik\r\n\r\n\r\ntrthaber-anadoluajansi\r\n\r\n\r\n0.8106289\r\n\r\n\r\n0.1943405\r\n\r\n\r\nHaberturk-yenisafak\r\n\r\n\r\n0.8063759\r\n\r\n\r\n0.1123132\r\n\r\n\r\nAhaber-Sabah\r\n\r\n\r\n0.8051288\r\n\r\n\r\n0.2083826\r\n\r\n\r\nHaberturk-Hurriyet\r\n\r\n\r\n0.8027018\r\n\r\n\r\n0.0912925\r\n\r\n\r\nBirGun_Gazetesi-t24comtr\r\n\r\n\r\n0.7787305\r\n\r\n\r\n0.1335979\r\n\r\n\r\nBirGun_Gazetesi-cumhuriyetgzt\r\n\r\n\r\n0.7663677\r\n\r\n\r\n0.1918478\r\n\r\n\r\ntrthaber-yenisafak\r\n\r\n\r\n0.7654727\r\n\r\n\r\n0.1761566\r\n\r\n\r\neuronews_tr-sputnik_TR\r\n\r\n\r\n0.7629162\r\n\r\n\r\n0.0656842\r\n\r\n\r\ndw_turkce-euronews_tr\r\n\r\n\r\n0.7596030\r\n\r\n\r\n0.0562945\r\n\r\n\r\ncnnturk-Hurriyet\r\n\r\n\r\n0.7585064\r\n\r\n\r\n0.0940787\r\n\r\n\r\nanadoluajansi-yenisafak\r\n\r\n\r\n0.7548191\r\n\r\n\r\n0.1543507\r\n\r\n\r\ncumhuriyetgzt-solhaberportali\r\n\r\n\r\n0.7538519\r\n\r\n\r\n0.1639537\r\n\r\n\r\nSabah-yenisafak\r\n\r\n\r\n0.7436745\r\n\r\n\r\n0.2103621\r\n\r\n\r\ntrthaber-Haberturk\r\n\r\n\r\n0.7422488\r\n\r\n\r\n0.0855117\r\n\r\n\r\nsputnik_TR-yenisafak\r\n\r\n\r\n0.7409369\r\n\r\n\r\n0.0847340\r\n\r\n\r\nbbcturkce-euronews_tr\r\n\r\n\r\n0.7355193\r\n\r\n\r\n0.0948756\r\n\r\n\r\nBirGun_Gazetesi-solhaberportali\r\n\r\n\r\n0.7351738\r\n\r\n\r\n0.1626351\r\n\r\n\r\nsolhaberportali-t24comtr\r\n\r\n\r\n0.7346799\r\n\r\n\r\n0.1221117\r\n\r\n\r\ncumhuriyetgzt-t24comtr\r\n\r\n\r\n0.7337696\r\n\r\n\r\n0.1520996\r\n\r\n\r\ndw_turkce-bbcturkce\r\n\r\n\r\n0.7332175\r\n\r\n\r\n0.0477881\r\n\r\n\r\ngazeteduvar-gazetesozcu\r\n\r\n\r\n0.7298423\r\n\r\n\r\n0.0537881\r\n\r\n\r\ncumhuriyetgzt-gazetesozcu\r\n\r\n\r\n0.7272367\r\n\r\n\r\n0.1105421\r\n\r\n\r\nHaberturk-sputnik_TR\r\n\r\n\r\n0.7272345\r\n\r\n\r\n0.0463835\r\n\r\n\r\nbbcturkce-sputnik_TR\r\n\r\n\r\n0.7268729\r\n\r\n\r\n0.0603327\r\n\r\n\r\nntv-cnnturk\r\n\r\n\r\n0.7229869\r\n\r\n\r\n0.0285233\r\n\r\n\r\nsputnik_TR-t24comtr\r\n\r\n\r\n0.7229612\r\n\r\n\r\n0.0836858\r\n\r\n\r\nyenisafak-Hurriyet\r\n\r\n\r\n0.7130531\r\n\r\n\r\n0.1291871\r\n\r\n\r\nanadoluajansi-Haberturk\r\n\r\n\r\n0.7054939\r\n\r\n\r\n0.0832394\r\n\r\n\r\nbbcturkce-t24comtr\r\n\r\n\r\n0.7049415\r\n\r\n\r\n0.0687628\r\n\r\n\r\nHaberturk-Sabah\r\n\r\n\r\n0.6978937\r\n\r\n\r\n0.0976829\r\n\r\n\r\nntv-sputnik_TR\r\n\r\n\r\n0.6973590\r\n\r\n\r\n0.0408761\r\n\r\n\r\nHaberturk-cnnturk\r\n\r\n\r\n0.6966393\r\n\r\n\r\n0.0410796\r\n\r\n\r\ncumhuriyetgzt-yeniakit\r\n\r\n\r\n0.6953032\r\n\r\n\r\n0.0675983\r\n\r\n\r\nntv-Hurriyet\r\n\r\n\r\n0.6948125\r\n\r\n\r\n0.0503384\r\n\r\n\r\ngazetesozcu-Hurriyet\r\n\r\n\r\n0.6938015\r\n\r\n\r\n0.0728135\r\n\r\n\r\ntrthaber-Sabah\r\n\r\n\r\n0.6937281\r\n\r\n\r\n0.1585761\r\n\r\n\r\ntrthaber-Hurriyet\r\n\r\n\r\n0.6930919\r\n\r\n\r\n0.1003859\r\n\r\n\r\ntrthaber-sputnik_TR\r\n\r\n\r\n0.6929987\r\n\r\n\r\n0.0655192\r\n\r\n\r\nt24comtr-gazetesozcu\r\n\r\n\r\n0.6917629\r\n\r\n\r\n0.1115391\r\n\r\n\r\nHaberturk-gazetesozcu\r\n\r\n\r\n0.6901779\r\n\r\n\r\n0.0439020\r\n\r\n\r\nbbcturkce-BirGun_Gazetesi\r\n\r\n\r\n0.6901495\r\n\r\n\r\n0.0693958\r\n\r\n\r\ndw_turkce-sputnik_TR\r\n\r\n\r\n0.6890468\r\n\r\n\r\n0.0305955\r\n\r\n\r\ntrthaber-ntv\r\n\r\n\r\n0.6888448\r\n\r\n\r\n0.0713711\r\n\r\n\r\ncnnturk-yeniakit\r\n\r\n\r\n0.6846588\r\n\r\n\r\n0.0549831\r\n\r\n\r\nHaberturk-ntv\r\n\r\n\r\n0.6830790\r\n\r\n\r\n0.0366840\r\n\r\n\r\ntrthaber-cnnturk\r\n\r\n\r\n0.6825293\r\n\r\n\r\n0.0552654\r\n\r\n\r\nsolhaberportali-gazetesozcu\r\n\r\n\r\n0.6774125\r\n\r\n\r\n0.0711323\r\n\r\n\r\nSabah-Hurriyet\r\n\r\n\r\n0.6766720\r\n\r\n\r\n0.1530826\r\n\r\n\r\nntv-yeniakit\r\n\r\n\r\n0.6758222\r\n\r\n\r\n0.0397374\r\n\r\n\r\nHaberturk-t24comtr\r\n\r\n\r\n0.6714056\r\n\r\n\r\n0.0911134\r\n\r\n\r\ntrthaber-bbcturkce\r\n\r\n\r\n0.6707776\r\n\r\n\r\n0.0823790\r\n\r\n\r\nsputnik_TR-cnnturk\r\n\r\n\r\n0.6704460\r\n\r\n\r\n0.0276909\r\n\r\n\r\nanadoluajansi-sputnik_TR\r\n\r\n\r\n0.6704114\r\n\r\n\r\n0.0696720\r\n\r\n\r\ncumhuriyetgzt-Hurriyet\r\n\r\n\r\n0.6695873\r\n\r\n\r\n0.0816854\r\n\r\n\r\nSabah-cnnturk\r\n\r\n\r\n0.6676695\r\n\r\n\r\n0.0797012\r\n\r\n\r\nsputnik_TR-Hurriyet\r\n\r\n\r\n0.6674735\r\n\r\n\r\n0.0532219\r\n\r\n\r\nanadoluajansi-Hurriyet\r\n\r\n\r\n0.6642287\r\n\r\n\r\n0.0781847\r\n\r\n\r\nAhaber-yenisafak\r\n\r\n\r\n0.6632437\r\n\r\n\r\n0.0754914\r\n\r\n\r\nBirGun_Gazetesi-sputnik_TR\r\n\r\n\r\n0.6618473\r\n\r\n\r\n0.0634015\r\n\r\n\r\nyenisafak-cnnturk\r\n\r\n\r\n0.6605355\r\n\r\n\r\n0.0708626\r\n\r\n\r\nanadoluajansi-ntv\r\n\r\n\r\n0.6577930\r\n\r\n\r\n0.0467949\r\n\r\n\r\nSabah-sputnik_TR\r\n\r\n\r\n0.6545151\r\n\r\n\r\n0.0697620\r\n\r\n\r\nbbcturkce-Haberturk\r\n\r\n\r\n0.6528864\r\n\r\n\r\n0.0382478\r\n\r\n\r\ngazeteduvar-solhaberportali\r\n\r\n\r\n0.6506590\r\n\r\n\r\n0.1014805\r\n\r\n\r\nHurriyet-yeniakit\r\n\r\n\r\n0.6480956\r\n\r\n\r\n0.0748645\r\n\r\n\r\nbbcturkce-ntv\r\n\r\n\r\n0.6480399\r\n\r\n\r\n0.0479609\r\n\r\n\r\nt24comtr-yenisafak\r\n\r\n\r\n0.6479050\r\n\r\n\r\n0.0682962\r\n\r\n\r\ngazeteduvar-t24comtr\r\n\r\n\r\n0.6477091\r\n\r\n\r\n0.0832924\r\n\r\n\r\nhalktvcomtr-t24comtr\r\n\r\n\r\n0.6471755\r\n\r\n\r\n0.0524686\r\n\r\n\r\nBirGun_Gazetesi-gazetesozcu\r\n\r\n\r\n0.6457721\r\n\r\n\r\n0.1196845\r\n\r\n\r\nbbcturkce-yenisafak\r\n\r\n\r\n0.6439317\r\n\r\n\r\n0.0717316\r\n\r\n\r\nntv-t24comtr\r\n\r\n\r\n0.6432950\r\n\r\n\r\n0.0339556\r\n\r\n\r\nt24comtr-yeniakit\r\n\r\n\r\n0.6432487\r\n\r\n\r\n0.0619541\r\n\r\n\r\nt24comtr-Hurriyet\r\n\r\n\r\n0.6421443\r\n\r\n\r\n0.0522538\r\n\r\n\r\ncumhuriyetgzt-cnnturk\r\n\r\n\r\n0.6413744\r\n\r\n\r\n0.0456347\r\n\r\n\r\nBirGun_Gazetesi-Sabah\r\n\r\n\r\n0.6388411\r\n\r\n\r\n0.0855551\r\n\r\n\r\ncumhuriyetgzt-Haberturk\r\n\r\n\r\n0.6387961\r\n\r\n\r\n0.0517547\r\n\r\n\r\nSabah-t24comtr\r\n\r\n\r\n0.6385868\r\n\r\n\r\n0.0646894\r\n\r\n\r\nanadoluajansi-bbcturkce\r\n\r\n\r\n0.6382396\r\n\r\n\r\n0.0677710\r\n\r\n\r\nbbcturkce-Hurriyet\r\n\r\n\r\n0.6375017\r\n\r\n\r\n0.0436395\r\n\r\n\r\ncumhuriyetgzt-ntv\r\n\r\n\r\n0.6365525\r\n\r\n\r\n0.0336408\r\n\r\n\r\nBirGun_Gazetesi-ntv\r\n\r\n\r\n0.6358817\r\n\r\n\r\n0.0342174\r\n\r\n\r\nBirGun_Gazetesi-Haberturk\r\n\r\n\r\n0.6323996\r\n\r\n\r\n0.0482418\r\n\r\n\r\nanadoluajansi-cnnturk\r\n\r\n\r\n0.6315114\r\n\r\n\r\n0.0386135\r\n\r\n\r\ncumhuriyetgzt-gazeteduvar\r\n\r\n\r\n0.6285823\r\n\r\n\r\n0.0579139\r\n\r\n\r\nanadoluajansi-Sabah\r\n\r\n\r\n0.6280131\r\n\r\n\r\n0.1232532\r\n\r\n\r\ndw_turkce-t24comtr\r\n\r\n\r\n0.6272307\r\n\r\n\r\n0.0342632\r\n\r\n\r\nBirGun_Gazetesi-yeniakit\r\n\r\n\r\n0.6267010\r\n\r\n\r\n0.0458171\r\n\r\n\r\nBirGun_Gazetesi-Hurriyet\r\n\r\n\r\n0.6257641\r\n\r\n\r\n0.0641389\r\n\r\n\r\nntv-yenisafak\r\n\r\n\r\n0.6244915\r\n\r\n\r\n0.0516657\r\n\r\n\r\nsputnik_TR-yeniakit\r\n\r\n\r\n0.6213267\r\n\r\n\r\n0.0469178\r\n\r\n\r\nntv-Sabah\r\n\r\n\r\n0.6211794\r\n\r\n\r\n0.0582493\r\n\r\n\r\ndw_turkce-BirGun_Gazetesi\r\n\r\n\r\n0.6146793\r\n\r\n\r\n0.0434032\r\n\r\n\r\nBirGun_Gazetesi-cnnturk\r\n\r\n\r\n0.6084926\r\n\r\n\r\n0.0340565\r\n\r\n\r\nbbcturkce-cnnturk\r\n\r\n\r\n0.6080996\r\n\r\n\r\n0.0276098\r\n\r\n\r\nSabah-yeniakit\r\n\r\n\r\n0.6076181\r\n\r\n\r\n0.0930308\r\n\r\n\r\ntele1comtr-BirGun_Gazetesi\r\n\r\n\r\n0.6067541\r\n\r\n\r\n0.0772375\r\n\r\n\r\ndw_turkce-yenisafak\r\n\r\n\r\n0.6064737\r\n\r\n\r\n0.0489489\r\n\r\n\r\nBirGun_Gazetesi-yenisafak\r\n\r\n\r\n0.6063298\r\n\r\n\r\n0.0940432\r\n\r\n\r\nsolhaberportali-yeniakit\r\n\r\n\r\n0.6053319\r\n\r\n\r\n0.0400433\r\n\r\n\r\ntrthaber-euronews_tr\r\n\r\n\r\n0.6052005\r\n\r\n\r\n0.0775859\r\n\r\n\r\nt24comtr-cnnturk\r\n\r\n\r\n0.6013539\r\n\r\n\r\n0.0247482\r\n\r\n\r\nAhaber-Haberturk\r\n\r\n\r\n0.6003299\r\n\r\n\r\n0.0381220\r\n\r\n\r\nhalktvcomtr-cumhuriyetgzt\r\n\r\n\r\n0.5992940\r\n\r\n\r\n0.0550906\r\n\r\n\r\nBirGun_Gazetesi-gazeteduvar\r\n\r\n\r\n0.5981839\r\n\r\n\r\n0.0730252\r\n\r\n\r\ntrthaber-t24comtr\r\n\r\n\r\n0.5979022\r\n\r\n\r\n0.0476076\r\n\r\n\r\ndw_turkce-trthaber\r\n\r\n\r\n0.5975492\r\n\r\n\r\n0.0404801\r\n\r\n\r\ntele1comtr-t24comtr\r\n\r\n\r\n0.5971600\r\n\r\n\r\n0.0620738\r\n\r\n\r\neuronews_tr-t24comtr\r\n\r\n\r\n0.5969724\r\n\r\n\r\n0.0477570\r\n\r\n\r\nbbcturkce-yeniakit\r\n\r\n\r\n0.5969629\r\n\r\n\r\n0.0377484\r\n\r\n\r\nbbcturkce-Sabah\r\n\r\n\r\n0.5960016\r\n\r\n\r\n0.0588932\r\n\r\n\r\ntrthaber-BirGun_Gazetesi\r\n\r\n\r\n0.5957208\r\n\r\n\r\n0.0663263\r\n\r\n\r\nsolhaberportali-Hurriyet\r\n\r\n\r\n0.5949732\r\n\r\n\r\n0.0308396\r\n\r\n\r\ngazetesozcu-yeniakit\r\n\r\n\r\n0.5942259\r\n\r\n\r\n0.0638034\r\n\r\n\r\ntele1comtr-cumhuriyetgzt\r\n\r\n\r\n0.5931553\r\n\r\n\r\n0.0793969\r\n\r\n\r\ncumhuriyetgzt-Sabah\r\n\r\n\r\n0.5925846\r\n\r\n\r\n0.0886826\r\n\r\n\r\neuronews_tr-yenisafak\r\n\r\n\r\n0.5920227\r\n\r\n\r\n0.0892231\r\n\r\n\r\nhalktvcomtr-BirGun_Gazetesi\r\n\r\n\r\n0.5889909\r\n\r\n\r\n0.0551861\r\n\r\n\r\ndw_turkce-anadoluajansi\r\n\r\n\r\n0.5876155\r\n\r\n\r\n0.0369139\r\n\r\n\r\nanadoluajansi-t24comtr\r\n\r\n\r\n0.5872263\r\n\r\n\r\n0.0490416\r\n\r\n\r\nbbcturkce-gazetesozcu\r\n\r\n\r\n0.5868224\r\n\r\n\r\n0.0508068\r\n\r\n\r\nhalktvcomtr-yenisafak\r\n\r\n\r\n0.5864419\r\n\r\n\r\n0.0284972\r\n\r\n\r\ntrthaber-yeniakit\r\n\r\n\r\n0.5846644\r\n\r\n\r\n0.0541077\r\n\r\n\r\neuronews_tr-Haberturk\r\n\r\n\r\n0.5836692\r\n\r\n\r\n0.0443140\r\n\r\n\r\nntv-solhaberportali\r\n\r\n\r\n0.5834627\r\n\r\n\r\n0.0216584\r\n\r\n\r\nsolhaberportali-sputnik_TR\r\n\r\n\r\n0.5831984\r\n\r\n\r\n0.0450597\r\n\r\n\r\nbbcturkce-cumhuriyetgzt\r\n\r\n\r\n0.5818368\r\n\r\n\r\n0.0547193\r\n\r\n\r\ngazeteduvar-Haberturk\r\n\r\n\r\n0.5798512\r\n\r\n\r\n0.0253730\r\n\r\n\r\nntv-gazetesozcu\r\n\r\n\r\n0.5775730\r\n\r\n\r\n0.0491491\r\n\r\n\r\nHaberturk-solhaberportali\r\n\r\n\r\n0.5768975\r\n\r\n\r\n0.0267129\r\n\r\n\r\nhalktvcomtr-Haberturk\r\n\r\n\r\n0.5767789\r\n\r\n\r\n0.0161725\r\n\r\n\r\nAhaber-t24comtr\r\n\r\n\r\n0.5766286\r\n\r\n\r\n0.0281784\r\n\r\n\r\nhalktvcomtr-solhaberportali\r\n\r\n\r\n0.5765503\r\n\r\n\r\n0.0450955\r\n\r\n\r\ncumhuriyetgzt-sputnik_TR\r\n\r\n\r\n0.5763907\r\n\r\n\r\n0.0488487\r\n\r\n\r\ndw_turkce-Haberturk\r\n\r\n\r\n0.5761907\r\n\r\n\r\n0.0273289\r\n\r\n\r\nHaberturk-yeniakit\r\n\r\n\r\n0.5759016\r\n\r\n\r\n0.0419655\r\n\r\n\r\nsputnik_TR-gazetesozcu\r\n\r\n\r\n0.5742517\r\n\r\n\r\n0.0550270\r\n\r\n\r\nbbcturkce-solhaberportali\r\n\r\n\r\n0.5722210\r\n\r\n\r\n0.0396403\r\n\r\n\r\ntele1comtr-solhaberportali\r\n\r\n\r\n0.5716931\r\n\r\n\r\n0.0602841\r\n\r\n\r\ngazeteduvar-Hurriyet\r\n\r\n\r\n0.5713483\r\n\r\n\r\n0.0293130\r\n\r\n\r\nanadoluajansi-BirGun_Gazetesi\r\n\r\n\r\n0.5694263\r\n\r\n\r\n0.0557753\r\n\r\n\r\nAhaber-cnnturk\r\n\r\n\r\n0.5664703\r\n\r\n\r\n0.0347852\r\n\r\n\r\nAhaber-Hurriyet\r\n\r\n\r\n0.5664411\r\n\r\n\r\n0.0584761\r\n\r\n\r\nAhaber-trthaber\r\n\r\n\r\n0.5659846\r\n\r\n\r\n0.0719299\r\n\r\n\r\nanadoluajansi-euronews_tr\r\n\r\n\r\n0.5652975\r\n\r\n\r\n0.0691299\r\n\r\n\r\nBirGun_Gazetesi-euronews_tr\r\n\r\n\r\n0.5637798\r\n\r\n\r\n0.0590051\r\n\r\n\r\nhalktvcomtr-gazeteduvar\r\n\r\n\r\n0.5635129\r\n\r\n\r\n0.0248399\r\n\r\n\r\neuronews_tr-cnnturk\r\n\r\n\r\n0.5633697\r\n\r\n\r\n0.0359805\r\n\r\n\r\nAhaber-BirGun_Gazetesi\r\n\r\n\r\n0.5626208\r\n\r\n\r\n0.0392925\r\n\r\n\r\nAhaber-sputnik_TR\r\n\r\n\r\n0.5552218\r\n\r\n\r\n0.0238154\r\n\r\n\r\ngazetesozcu-cnnturk\r\n\r\n\r\n0.5546777\r\n\r\n\r\n0.0318466\r\n\r\n\r\nhalktvcomtr-gazetesozcu\r\n\r\n\r\n0.5522672\r\n\r\n\r\n0.0292085\r\n\r\n\r\neuronews_tr-Hurriyet\r\n\r\n\r\n0.5477599\r\n\r\n\r\n0.0539274\r\n\r\n\r\nSabah-solhaberportali\r\n\r\n\r\n0.5470097\r\n\r\n\r\n0.0384828\r\n\r\n\r\neuronews_tr-ntv\r\n\r\n\r\n0.5460006\r\n\r\n\r\n0.0334140\r\n\r\n\r\nsolhaberportali-cnnturk\r\n\r\n\r\n0.5397424\r\n\r\n\r\n0.0160335\r\n\r\n\r\ntrthaber-cumhuriyetgzt\r\n\r\n\r\n0.5379678\r\n\r\n\r\n0.0563175\r\n\r\n\r\nhalktvcomtr-Hurriyet\r\n\r\n\r\n0.5325830\r\n\r\n\r\n0.0245928\r\n\r\n\r\ntgrthabertv-yenisafak\r\n\r\n\r\n0.5317700\r\n\r\n\r\n0.0356498\r\n\r\n\r\nAhaber-cumhuriyetgzt\r\n\r\n\r\n0.5316974\r\n\r\n\r\n0.0447653\r\n\r\n\r\ngazeteduvar-sputnik_TR\r\n\r\n\r\n0.5315661\r\n\r\n\r\n0.0441302\r\n\r\n\r\ndw_turkce-ntv\r\n\r\n\r\n0.5305560\r\n\r\n\r\n0.0154338\r\n\r\n\r\ndw_turkce-Sabah\r\n\r\n\r\n0.5305229\r\n\r\n\r\n0.0332714\r\n\r\n\r\ndw_turkce-Hurriyet\r\n\r\n\r\n0.5286587\r\n\r\n\r\n0.0207836\r\n\r\n\r\ntrthaber-gazetesozcu\r\n\r\n\r\n0.5278540\r\n\r\n\r\n0.0533354\r\n\r\n\r\neuronews_tr-Sabah\r\n\r\n\r\n0.5275381\r\n\r\n\r\n0.0671365\r\n\r\n\r\nSabah-gazetesozcu\r\n\r\n\r\n0.5255846\r\n\r\n\r\n0.0786883\r\n\r\n\r\nhalktvcomtr-tele1comtr\r\n\r\n\r\n0.5253747\r\n\r\n\r\n0.0327451\r\n\r\n\r\neuronews_tr-yeniakit\r\n\r\n\r\n0.5241909\r\n\r\n\r\n0.0385380\r\n\r\n\r\nanadoluajansi-yeniakit\r\n\r\n\r\n0.5237885\r\n\r\n\r\n0.0558375\r\n\r\n\r\nhalktvcomtr-sputnik_TR\r\n\r\n\r\n0.5223739\r\n\r\n\r\n0.0181744\r\n\r\n\r\ndw_turkce-cnnturk\r\n\r\n\r\n0.5220954\r\n\r\n\r\n0.0141125\r\n\r\n\r\nyenisafak-yeniakit\r\n\r\n\r\n0.5215068\r\n\r\n\r\n0.0657442\r\n\r\n\r\ntele1comtr-gazeteduvar\r\n\r\n\r\n0.5205275\r\n\r\n\r\n0.0270762\r\n\r\n\r\nhalktvcomtr-Sabah\r\n\r\n\r\n0.5197517\r\n\r\n\r\n0.0264668\r\n\r\n\r\nhalktvcomtr-Ahaber\r\n\r\n\r\n0.5190580\r\n\r\n\r\n0.0140692\r\n\r\n\r\nbbcturkce-gazeteduvar\r\n\r\n\r\n0.5172236\r\n\r\n\r\n0.0408893\r\n\r\n\r\ndw_turkce-yeniakit\r\n\r\n\r\n0.5156929\r\n\r\n\r\n0.0153667\r\n\r\n\r\nAhaber-yeniakit\r\n\r\n\r\n0.5156580\r\n\r\n\r\n0.0405285\r\n\r\n\r\ncumhuriyetgzt-yenisafak\r\n\r\n\r\n0.5152230\r\n\r\n\r\n0.0772845\r\n\r\n\r\ndw_turkce-solhaberportali\r\n\r\n\r\n0.5114312\r\n\r\n\r\n0.0228503\r\n\r\n\r\ngazetesozcu-yenisafak\r\n\r\n\r\n0.5113475\r\n\r\n\r\n0.0752942\r\n\r\n\r\ndw_turkce-gazetesozcu\r\n\r\n\r\n0.5080183\r\n\r\n\r\n0.0267702\r\n\r\n\r\nanadoluajansi-gazetesozcu\r\n\r\n\r\n0.5075900\r\n\r\n\r\n0.0515435\r\n\r\n\r\nsolhaberportali-yenisafak\r\n\r\n\r\n0.5070632\r\n\r\n\r\n0.0449267\r\n\r\n\r\ngazeteduvar-yeniakit\r\n\r\n\r\n0.5064414\r\n\r\n\r\n0.0397979\r\n\r\n\r\nAhaber-ntv\r\n\r\n\r\n0.5035523\r\n\r\n\r\n0.0346341\r\n\r\n\r\nAhaber-bbcturkce\r\n\r\n\r\n0.5016460\r\n\r\n\r\n0.0244870\r\n\r\n\r\nhalktvcomtr-bbcturkce\r\n\r\n\r\n0.4999610\r\n\r\n\r\n0.0193332\r\n\r\n\r\nAhaber-anadoluajansi\r\n\r\n\r\n0.4996122\r\n\r\n\r\n0.0421809\r\n\r\n\r\ndw_turkce-cumhuriyetgzt\r\n\r\n\r\n0.4983784\r\n\r\n\r\n0.0254060\r\n\r\n\r\nhalktvcomtr-ntv\r\n\r\n\r\n0.4914800\r\n\r\n\r\n0.0138667\r\n\r\n\r\nhalktvcomtr-yeniakit\r\n\r\n\r\n0.4862986\r\n\r\n\r\n0.0243008\r\n\r\n\r\ncumhuriyetgzt-euronews_tr\r\n\r\n\r\n0.4830498\r\n\r\n\r\n0.0500718\r\n\r\n\r\nhalktvcomtr-cnnturk\r\n\r\n\r\n0.4825188\r\n\r\n\r\n0.0115618\r\n\r\n\r\ntele1comtr-yeniakit\r\n\r\n\r\n0.4801900\r\n\r\n\r\n0.0305528\r\n\r\n\r\ngazeteduvar-ntv\r\n\r\n\r\n0.4792814\r\n\r\n\r\n0.0248216\r\n\r\n\r\neuronews_tr-gazetesozcu\r\n\r\n\r\n0.4784038\r\n\r\n\r\n0.0415197\r\n\r\n\r\ntrthaber-solhaberportali\r\n\r\n\r\n0.4775124\r\n\r\n\r\n0.0278770\r\n\r\n\r\neuronews_tr-solhaberportali\r\n\r\n\r\n0.4775024\r\n\r\n\r\n0.0302210\r\n\r\n\r\nanadoluajansi-cumhuriyetgzt\r\n\r\n\r\n0.4771810\r\n\r\n\r\n0.0362971\r\n\r\n\r\ntele1comtr-gazetesozcu\r\n\r\n\r\n0.4740713\r\n\r\n\r\n0.0489467\r\n\r\n\r\ntele1comtr-Ahaber\r\n\r\n\r\n0.4708324\r\n\r\n\r\n0.0291057\r\n\r\n\r\nhalktvcomtr-tgrthabertv\r\n\r\n\r\n0.4689181\r\n\r\n\r\n0.0068348\r\n\r\n\r\nAhaber-solhaberportali\r\n\r\n\r\n0.4648964\r\n\r\n\r\n0.0169447\r\n\r\n\r\nanadoluajansi-solhaberportali\r\n\r\n\r\n0.4646816\r\n\r\n\r\n0.0306466\r\n\r\n\r\ntele1comtr-bbcturkce\r\n\r\n\r\n0.4639462\r\n\r\n\r\n0.0186551\r\n\r\n\r\ntele1comtr-Sabah\r\n\r\n\r\n0.4572505\r\n\r\n\r\n0.0343725\r\n\r\n\r\ntrthaber-gazeteduvar\r\n\r\n\r\n0.4535852\r\n\r\n\r\n0.0350991\r\n\r\n\r\ngazeteduvar-Sabah\r\n\r\n\r\n0.4528567\r\n\r\n\r\n0.0369867\r\n\r\n\r\nhalktvcomtr-trthaber\r\n\r\n\r\n0.4516245\r\n\r\n\r\n0.0185267\r\n\r\n\r\ntele1comtr-Hurriyet\r\n\r\n\r\n0.4508242\r\n\r\n\r\n0.0306197\r\n\r\n\r\ngazeteduvar-yenisafak\r\n\r\n\r\n0.4498436\r\n\r\n\r\n0.0439440\r\n\r\n\r\ngazeteduvar-cnnturk\r\n\r\n\r\n0.4476540\r\n\r\n\r\n0.0150541\r\n\r\n\r\nHaberturk-tgrthabertv\r\n\r\n\r\n0.4460957\r\n\r\n\r\n0.0184105\r\n\r\n\r\ndw_turkce-gazeteduvar\r\n\r\n\r\n0.4450478\r\n\r\n\r\n0.0196793\r\n\r\n\r\ntele1comtr-Haberturk\r\n\r\n\r\n0.4442116\r\n\r\n\r\n0.0155230\r\n\r\n\r\nAhaber-gazetesozcu\r\n\r\n\r\n0.4429511\r\n\r\n\r\n0.0307758\r\n\r\n\r\ntele1comtr-sputnik_TR\r\n\r\n\r\n0.4427018\r\n\r\n\r\n0.0171524\r\n\r\n\r\ndw_turkce-Ahaber\r\n\r\n\r\n0.4394215\r\n\r\n\r\n0.0115221\r\n\r\n\r\nSabah-tgrthabertv\r\n\r\n\r\n0.4335623\r\n\r\n\r\n0.0429504\r\n\r\n\r\ntele1comtr-ntv\r\n\r\n\r\n0.4329672\r\n\r\n\r\n0.0128951\r\n\r\n\r\nAhaber-gazeteduvar\r\n\r\n\r\n0.4310360\r\n\r\n\r\n0.0170058\r\n\r\n\r\nAhaber-euronews_tr\r\n\r\n\r\n0.4306591\r\n\r\n\r\n0.0283555\r\n\r\n\r\ndw_turkce-halktvcomtr\r\n\r\n\r\n0.4268255\r\n\r\n\r\n0.0103629\r\n\r\n\r\ntele1comtr-yenisafak\r\n\r\n\r\n0.4254835\r\n\r\n\r\n0.0300456\r\n\r\n\r\nFOXhaber-t24comtr\r\n\r\n\r\n0.4253660\r\n\r\n\r\n0.0076325\r\n\r\n\r\nFOXhaber-sputnik_TR\r\n\r\n\r\n0.4252895\r\n\r\n\r\n0.0063732\r\n\r\n\r\nhalktvcomtr-anadoluajansi\r\n\r\n\r\n0.4249324\r\n\r\n\r\n0.0157424\r\n\r\n\r\ntele1comtr-cnnturk\r\n\r\n\r\n0.4239025\r\n\r\n\r\n0.0169703\r\n\r\n\r\ntgrthabertv-Hurriyet\r\n\r\n\r\n0.4223124\r\n\r\n\r\n0.0299972\r\n\r\n\r\neuronews_tr-gazeteduvar\r\n\r\n\r\n0.4198134\r\n\r\n\r\n0.0288717\r\n\r\n\r\nanadoluajansi-gazeteduvar\r\n\r\n\r\n0.4183067\r\n\r\n\r\n0.0332509\r\n\r\n\r\nt24comtr-DikenComTr\r\n\r\n\r\n0.4168956\r\n\r\n\r\n0.0223397\r\n\r\n\r\ntgrthabertv-cnnturk\r\n\r\n\r\n0.4167897\r\n\r\n\r\n0.0186526\r\n\r\n\r\nt24comtr-tgrthabertv\r\n\r\n\r\n0.4116710\r\n\r\n\r\n0.0189317\r\n\r\n\r\ndw_turkce-tele1comtr\r\n\r\n\r\n0.4111918\r\n\r\n\r\n0.0109860\r\n\r\n\r\nsputnik_TR-tgrthabertv\r\n\r\n\r\n0.4089963\r\n\r\n\r\n0.0156813\r\n\r\n\r\ntrthaber-tgrthabertv\r\n\r\n\r\n0.4073556\r\n\r\n\r\n0.0255372\r\n\r\n\r\nBirGun_Gazetesi-DikenComTr\r\n\r\n\r\n0.4065819\r\n\r\n\r\n0.0238453\r\n\r\n\r\nAhaber-tgrthabertv\r\n\r\n\r\n0.4058075\r\n\r\n\r\n0.0229085\r\n\r\n\r\ntele1comtr-trthaber\r\n\r\n\r\n0.4057580\r\n\r\n\r\n0.0174285\r\n\r\n\r\nhalktvcomtr-FOXhaber\r\n\r\n\r\n0.4021386\r\n\r\n\r\n0.0027647\r\n\r\n\r\nhalktvcomtr-euronews_tr\r\n\r\n\r\n0.3983322\r\n\r\n\r\n0.0156261\r\n\r\n\r\nBirGun_Gazetesi-FOXhaber\r\n\r\n\r\n0.3962450\r\n\r\n\r\n0.0098530\r\n\r\n\r\nFOXhaber-yenisafak\r\n\r\n\r\n0.3960814\r\n\r\n\r\n0.0090183\r\n\r\n\r\nbbcturkce-DikenComTr\r\n\r\n\r\n0.3929745\r\n\r\n\r\n0.0123792\r\n\r\n\r\nbbcturkce-tgrthabertv\r\n\r\n\r\n0.3923173\r\n\r\n\r\n0.0137057\r\n\r\n\r\ntgrthabertv-yeniakit\r\n\r\n\r\n0.3909191\r\n\r\n\r\n0.0227032\r\n\r\n\r\nntv-tgrthabertv\r\n\r\n\r\n0.3896238\r\n\r\n\r\n0.0132043\r\n\r\n\r\nanadoluajansi-FOXhaber\r\n\r\n\r\n0.3848416\r\n\r\n\r\n0.0101080\r\n\r\n\r\nbbcturkce-FOXhaber\r\n\r\n\r\n0.3846752\r\n\r\n\r\n0.0064830\r\n\r\n\r\ncumhuriyetgzt-tgrthabertv\r\n\r\n\r\n0.3827390\r\n\r\n\r\n0.0243898\r\n\r\n\r\nFOXhaber-Haberturk\r\n\r\n\r\n0.3781859\r\n\r\n\r\n0.0056567\r\n\r\n\r\nBirGun_Gazetesi-tgrthabertv\r\n\r\n\r\n0.3772591\r\n\r\n\r\n0.0213794\r\n\r\n\r\ntrthaber-FOXhaber\r\n\r\n\r\n0.3702974\r\n\r\n\r\n0.0086941\r\n\r\n\r\nDikenComTr-gazetesozcu\r\n\r\n\r\n0.3690939\r\n\r\n\r\n0.0297316\r\n\r\n\r\nanadoluajansi-tgrthabertv\r\n\r\n\r\n0.3661290\r\n\r\n\r\n0.0183070\r\n\r\n\r\ntele1comtr-anadoluajansi\r\n\r\n\r\n0.3592216\r\n\r\n\r\n0.0122300\r\n\r\n\r\nsolhaberportali-DikenComTr\r\n\r\n\r\n0.3585451\r\n\r\n\r\n0.0148973\r\n\r\n\r\neuronews_tr-tgrthabertv\r\n\r\n\r\n0.3582541\r\n\r\n\r\n0.0191940\r\n\r\n\r\ntele1comtr-euronews_tr\r\n\r\n\r\n0.3581601\r\n\r\n\r\n0.0149665\r\n\r\n\r\ndw_turkce-tgrthabertv\r\n\r\n\r\n0.3565172\r\n\r\n\r\n0.0091259\r\n\r\n\r\ndw_turkce-FOXhaber\r\n\r\n\r\n0.3527138\r\n\r\n\r\n0.0051071\r\n\r\n\r\nFOXhaber-Sabah\r\n\r\n\r\n0.3516677\r\n\r\n\r\n0.0075679\r\n\r\n\r\ncumhuriyetgzt-DikenComTr\r\n\r\n\r\n0.3508239\r\n\r\n\r\n0.0186730\r\n\r\n\r\nFOXhaber-ntv\r\n\r\n\r\n0.3504955\r\n\r\n\r\n0.0064411\r\n\r\n\r\nFOXhaber-solhaberportali\r\n\r\n\r\n0.3492678\r\n\r\n\r\n0.0056070\r\n\r\n\r\nFOXhaber-Hurriyet\r\n\r\n\r\n0.3471565\r\n\r\n\r\n0.0058412\r\n\r\n\r\nAhaber-FOXhaber\r\n\r\n\r\n0.3456809\r\n\r\n\r\n0.0046989\r\n\r\n\r\neuronews_tr-FOXhaber\r\n\r\n\r\n0.3443768\r\n\r\n\r\n0.0063400\r\n\r\n\r\ncumhuriyetgzt-FOXhaber\r\n\r\n\r\n0.3433734\r\n\r\n\r\n0.0065443\r\n\r\n\r\nFOXhaber-cnnturk\r\n\r\n\r\n0.3364997\r\n\r\n\r\n0.0028187\r\n\r\n\r\nFOXhaber-gazetesozcu\r\n\r\n\r\n0.3361759\r\n\r\n\r\n0.0082618\r\n\r\n\r\nFOXhaber-yeniakit\r\n\r\n\r\n0.3353517\r\n\r\n\r\n0.0055062\r\n\r\n\r\nDikenComTr-Hurriyet\r\n\r\n\r\n0.3345587\r\n\r\n\r\n0.0081378\r\n\r\n\r\ngazeteduvar-DikenComTr\r\n\r\n\r\n0.3342991\r\n\r\n\r\n0.0110154\r\n\r\n\r\nsputnik_TR-DikenComTr\r\n\r\n\r\n0.3337218\r\n\r\n\r\n0.0087047\r\n\r\n\r\nsolhaberportali-tgrthabertv\r\n\r\n\r\n0.3293375\r\n\r\n\r\n0.0082908\r\n\r\n\r\ntgrthabertv-gazetesozcu\r\n\r\n\r\n0.3252408\r\n\r\n\r\n0.0172451\r\n\r\n\r\nFOXhaber-gazeteduvar\r\n\r\n\r\n0.3245054\r\n\r\n\r\n0.0048890\r\n\r\n\r\ntele1comtr-FOXhaber\r\n\r\n\r\n0.3230278\r\n\r\n\r\n0.0040446\r\n\r\n\r\ndw_turkce-DikenComTr\r\n\r\n\r\n0.3216916\r\n\r\n\r\n0.0056046\r\n\r\n\r\ntele1comtr-tgrthabertv\r\n\r\n\r\n0.3199987\r\n\r\n\r\n0.0106059\r\n\r\n\r\ntele1comtr-DikenComTr\r\n\r\n\r\n0.3153672\r\n\r\n\r\n0.0089188\r\n\r\n\r\nHaberturk-DikenComTr\r\n\r\n\r\n0.3149255\r\n\r\n\r\n0.0099406\r\n\r\n\r\nDikenComTr-yeniakit\r\n\r\n\r\n0.3140351\r\n\r\n\r\n0.0098138\r\n\r\n\r\nhalktvcomtr-DikenComTr\r\n\r\n\r\n0.3060827\r\n\r\n\r\n0.0073108\r\n\r\n\r\nSabah-DikenComTr\r\n\r\n\r\n0.3049851\r\n\r\n\r\n0.0114251\r\n\r\n\r\nntv-DikenComTr\r\n\r\n\r\n0.3047723\r\n\r\n\r\n0.0062080\r\n\r\n\r\nanadoluajansi-DikenComTr\r\n\r\n\r\n0.3037381\r\n\r\n\r\n0.0090901\r\n\r\n\r\ngazeteduvar-tgrthabertv\r\n\r\n\r\n0.3028468\r\n\r\n\r\n0.0117258\r\n\r\n\r\nDikenComTr-yenisafak\r\n\r\n\r\n0.3014180\r\n\r\n\r\n0.0112483\r\n\r\n\r\ntrthaber-DikenComTr\r\n\r\n\r\n0.2979784\r\n\r\n\r\n0.0110424\r\n\r\n\r\neuronews_tr-DikenComTr\r\n\r\n\r\n0.2971398\r\n\r\n\r\n0.0085442\r\n\r\n\r\nFOXhaber-tgrthabertv\r\n\r\n\r\n0.2853207\r\n\r\n\r\n0.0239892\r\n\r\n\r\nDikenComTr-cnnturk\r\n\r\n\r\n0.2845894\r\n\r\n\r\n0.0038484\r\n\r\n\r\nAhaber-DikenComTr\r\n\r\n\r\n0.2715798\r\n\r\n\r\n0.0047246\r\n\r\n\r\nFOXhaber-DikenComTr\r\n\r\n\r\n0.2178037\r\n\r\n\r\n0.0015388\r\n\r\n\r\nDikenComTr-tgrthabertv\r\n\r\n\r\n0.1910289\r\n\r\n\r\n0.0028210\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-06-12-post12/post12_files/figure-html5/unnamed-chunk-16-1.png",
    "last_modified": "2022-06-12T17:28:26+03:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-06-11-post11/",
    "title": "JSON Formatındaki Veriye Ulaşmak: İBB WiFi Yeni Kullanıcı Verisi",
    "description": "İBB Açık Veri Portalı'ndaki JSON formatındaki bir veri setinin çekilmesi ve haritalandırılması.",
    "author": [
      {
        "name": "A. Uraz Akgül",
        "url": {}
      }
    ],
    "date": "2022-06-11",
    "categories": [
      "Web",
      "Map"
    ],
    "contents": "\r\n2020 yılının Ocak ayında İBB, Açık Veri Portalı’nı hizmete\r\nsundu. Geçen sürede 253 veri seti paylaşmışlar. Bu veri setlerinin çoğu\r\nxlsx, csv gibi formatlardan oluşuyor. Her ne kadar bu formatlar yaygın\r\nolarak kullanılsa da direkt web üzerinden verilere ulaşabileceğimiz\r\nyollar da var. Daha çok kullanılması dileğiyle demek isterim.\r\nBu uygulamada hem JSON formatını tanıyacağız hem de çektiğimiz\r\nveriler üzerinden bir haritalandırma yapacağız.\r\nUygulamaya konu İBB WiFi Yeni Kullanıcı Verisi, İstanbul Büyük\r\nŞehir Belediyesi tarafından sağlanan kablosuz internet hizmetini\r\nkullanan kullanıcı verilerini içermektedir. İşbu veri kaynağının\r\niçerisinde, Abonelik tarihi, Abone olunan ilçe, Abonenin Yerli/Yabancı,\r\nAbone olunan lokasyon bilgisi ve kullanıcı sayısı bilgilerini\r\niçerir. -İBB\r\nBu veri setinin 6 tane değişkeni bulunmaktadır.\r\nSUBSCRIPTION_DATE: Aboneliğin gerçekleştiği tarih bilgisini\r\niçerir.\r\nSUBSCRIPTION_COUNTY: Aboneliğin gerçekleştiği ilçe bilgisini\r\niçerir.\r\nSUBSCRIBER_DOMESTIC_FOREIGN: Abonenin Türk vatandaşı ise yerli,\r\ndeğilse yabancı bilgisini içerir.\r\nLONGITUDE: Aboneliğin gerçekleştiği boylam bilgisini\r\niçerir.\r\nLATITUDE: Aboneliğin gerçekleştiği enlem bilgisini\r\niçerir.\r\nNUMBER_OF_SUBSCRIBER: İlgili tarihteki abone sayısı bilgisini\r\niçerir.\r\nVeri API butonuna tıklarsak eğer karşımıza aşağıdaki ekran\r\nçıkacaktır.\r\n\r\nAPI, iki yazılım ve bir sunucunun olduğu yerde bağlantı kurma görevi\r\nüstlenen bir protokoldür. Biz bu API’ı kullanarak talebimizi sunucuya\r\naktarıyoruz. Talebi alan sunucu kaynağa başvuruyor. Başvurduğu kaynak\r\nsunucuya geri dönüş sağlıyor. Sunucu gelen bu geri dönüşü API ile\r\nkullanıcıya iletiyor.\r\nhttps://www.altexsoft.com/blog/engineering/what-is-api-definition-types-specifications-documentation/Endpointler, API üzerinde belli bir amaca yönelik oluşturulmuş\r\nmetotlardır.\r\nİlk görselde Sorgulama başlığını görüyoruz. Buraya tıklarsak\r\neğer OData örneği (ilk 5 sonuç, OData JSON’ı olarak dön) alt\r\nbaşlığını göreceğiz. Open Data Protocol anlamına gelen OData, veri\r\nkaynaklarına HTTP (sunucu ve istemci arasındaki haberleşme protokolü)\r\nüzerinden sorgu atılmasını sağlar. Yani HTTP tabanlı olması, tüm\r\nsorguların url (web sitesi erişiminde kullanılan adres) üzerinden\r\ngerçekleştirilmesidir.\r\nSon olarak, JavaScript Object Notation’ın kısaltması olan JSON,\r\nverileri depolamak ve veri alışverişi yapmak için kullanılan kolay ve\r\nhafif bir yapıdır. JSON, veriyi okuyabilmemizi sağlar. Nasıl?\r\nGelin R’da örnek bir JSON yapısı oluşturalım. Bunun için\r\njsonlite paketini kullanacağız. rjson da kullandığım\r\npaketler arasındadır.\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(leaflet) # harita\r\nlibrary(httr) # API\r\n\r\n\r\n\r\n\r\n\r\nlibrary(jsonlite) # JSON\r\n\r\nid <- c(100,101,102)\r\ntitle <- c(\"X\",\"Y\",\"Z\")\r\n\r\ndf <- data.frame(id, title)\r\nsomedata <- data.frame(Category_A = c(\"1\",\"0\",\"0\"),\r\n                       Category_B = c(\"0\",\"0\",\"1\"),\r\n                       Category_C = c(\"1\",\"1\",\"1\"))\r\ndf$somedata <- somedata\r\n\r\ntoJSON(df, pretty = TRUE)\r\n\r\n\r\n[\r\n  {\r\n    \"id\": 100,\r\n    \"title\": \"X\",\r\n    \"somedata\": {\r\n      \"Category_A\": \"1\",\r\n      \"Category_B\": \"0\",\r\n      \"Category_C\": \"1\"\r\n    }\r\n  },\r\n  {\r\n    \"id\": 101,\r\n    \"title\": \"Y\",\r\n    \"somedata\": {\r\n      \"Category_A\": \"0\",\r\n      \"Category_B\": \"0\",\r\n      \"Category_C\": \"1\"\r\n    }\r\n  },\r\n  {\r\n    \"id\": 102,\r\n    \"title\": \"Z\",\r\n    \"somedata\": {\r\n      \"Category_A\": \"0\",\r\n      \"Category_B\": \"1\",\r\n      \"Category_C\": \"1\"\r\n    }\r\n  }\r\n] \r\n\r\nAPI ile çalışırken httr paketini kullanacağız.\r\nİlk fonksiyon GET(), veri çekmek için kullanılır.\r\n\r\n\r\nres <- GET(\r\n  url = \"https://data.ibb.gov.tr/en/datastore/odata3.0/12f5bc23-224a-43cb-b60d-3f36f83ffd33?$format=json\"\r\n)\r\n\r\n\r\n\r\nYukarıda oluşturduğumuz res değişkeni (response), API\r\nsunucusunun isteğimize verdiği yanıtı içerir.\r\n\r\n\r\nres\r\n\r\n\r\nResponse [https://data.ibb.gov.tr/en/datastore/odata3.0/12f5bc23-224a-43cb-b60d-3f36f83ffd33?$format=json]\r\n  Date: 2022-06-11 11:54\r\n  Status: 200\r\n  Content-Type: text/html; charset=utf-8\r\n  Size: 92.3 kB\r\n\r\nStatus’ın 200 olması yanıtın başarılı olduğunu gösterir.\r\nYani, istemci ile sunucu arasındaki iletişim herhangi bir hata olmadan\r\ngerçekleşmiştir.\r\nhttps://dev.to/bisrategebriel/http-status-codes-101-6jhİçeriği aşağıdaki gibi JSON formatına çevirmemiz gerekiyor.\r\n\r\n\r\n# rawToChar(res$content)\r\n\r\n\r\n\r\nHer ne kadar dağınık görünse de aslında JSON formatına çevirmiş olduk\r\nancak bu format character yapısında oldu. Bu formatı list yapısına\r\nçevirebiliriz.\r\n\r\n\r\ndt <- fromJSON(rawToChar(res$content))\r\nnames(dt)\r\n\r\n\r\n[1] \"odata.metadata\" \"value\"         \r\n\r\nİhtiyacımız olan veriler value’nun içindedir. Koordinat\r\nbazlı haritalandırma yapacağımız için kullanacağımız değişkenler\r\nSUBSCRIBER_DOMESTIC_FOREIGN, LONGITUDE, LATITUDE ve NUMBER_OF_SUBSCRIBER\r\nolacaktır.\r\n\r\n\r\nmaster <- dt$value\r\n\r\n\r\n\r\n\r\n\r\nSUBSCRIBER_DOMESTIC_FOREIGN\r\n\r\n\r\nLONGITUDE\r\n\r\n\r\nLATITUDE\r\n\r\n\r\nNUMBER_OF_SUBSCRIBER\r\n\r\n\r\nYabancÄ±\r\n\r\n\r\n29.02325\r\n\r\n\r\n40.99270\r\n\r\n\r\n1\r\n\r\n\r\nYerli\r\n\r\n\r\n29.10552\r\n\r\n\r\n41.01701\r\n\r\n\r\n2\r\n\r\n\r\nYerli\r\n\r\n\r\n29.08863\r\n\r\n\r\n40.95404\r\n\r\n\r\n2\r\n\r\n\r\nYerli\r\n\r\n\r\n29.09082\r\n\r\n\r\n41.24324\r\n\r\n\r\n2\r\n\r\n\r\nYerli\r\n\r\n\r\n29.14665\r\n\r\n\r\n40.90128\r\n\r\n\r\n1\r\n\r\n\r\nYerli\r\n\r\n\r\n28.65662\r\n\r\n\r\n41.00984\r\n\r\n\r\n1\r\n\r\n\r\nYerli\r\n\r\n\r\n29.11825\r\n\r\n\r\n41.04074\r\n\r\n\r\n32\r\n\r\n\r\nYerli\r\n\r\n\r\n28.94929\r\n\r\n\r\n41.01124\r\n\r\n\r\n3\r\n\r\n\r\nYerli\r\n\r\n\r\n28.72508\r\n\r\n\r\n40.98418\r\n\r\n\r\n2\r\n\r\n\r\nYabancÄ±\r\n\r\n\r\n28.97837\r\n\r\n\r\n41.00851\r\n\r\n\r\n2\r\n\r\n\r\nBoom!\r\nKüçük bir düzeltme yapmamız gerekiyor. SUBSCRIBER_DOMESTIC_FOREIGN\r\ndeğişkenine ait değerlerde Yabancı yerine YabancÄ± yazılmış. Bunu\r\ngsub() fonksiyonu ile aşağıdaki gibi düzeltebiliriz.\r\n\r\n\r\nmaster <- master %>% \r\n  mutate(\r\n    SUBSCRIBER_DOMESTIC_FOREIGN = gsub(\"YabancÄ±\",\"Yabancı\",SUBSCRIBER_DOMESTIC_FOREIGN)\r\n  )\r\n\r\n\r\n\r\n\r\n\r\nSUBSCRIBER_DOMESTIC_FOREIGN\r\n\r\n\r\nLONGITUDE\r\n\r\n\r\nLATITUDE\r\n\r\n\r\nNUMBER_OF_SUBSCRIBER\r\n\r\n\r\nYabancı\r\n\r\n\r\n29.02325\r\n\r\n\r\n40.99270\r\n\r\n\r\n1\r\n\r\n\r\nYerli\r\n\r\n\r\n29.10552\r\n\r\n\r\n41.01701\r\n\r\n\r\n2\r\n\r\n\r\nYerli\r\n\r\n\r\n29.08863\r\n\r\n\r\n40.95404\r\n\r\n\r\n2\r\n\r\n\r\nYerli\r\n\r\n\r\n29.09082\r\n\r\n\r\n41.24324\r\n\r\n\r\n2\r\n\r\n\r\nYerli\r\n\r\n\r\n29.14665\r\n\r\n\r\n40.90128\r\n\r\n\r\n1\r\n\r\n\r\nYerli\r\n\r\n\r\n28.65662\r\n\r\n\r\n41.00984\r\n\r\n\r\n1\r\n\r\n\r\nYerli\r\n\r\n\r\n29.11825\r\n\r\n\r\n41.04074\r\n\r\n\r\n32\r\n\r\n\r\nYerli\r\n\r\n\r\n28.94929\r\n\r\n\r\n41.01124\r\n\r\n\r\n3\r\n\r\n\r\nYerli\r\n\r\n\r\n28.72508\r\n\r\n\r\n40.98418\r\n\r\n\r\n2\r\n\r\n\r\nYabancı\r\n\r\n\r\n28.97837\r\n\r\n\r\n41.00851\r\n\r\n\r\n2\r\n\r\n\r\nHaritaya geçebiliriz.\r\nBu bölümde JavaScript’in bir kütüphanesi olan ve web tabanlı\r\ninteraktif haritalamada tercih edilebilecek leaflet’i\r\nkullanacağız. En basit haliyle aşağıdaki gibi bir örnek verebiliriz.\r\n\r\n\r\nleaflet() %>% \r\n  addTiles() %>% \r\n  addMarkers(lng = 32.836944, lat = 39.925, popup = \"Anıtkabir\")\r\n\r\n\r\n\r\n{\"x\":{\"options\":{\"crs\":{\"crsClass\":\"L.CRS.EPSG3857\",\"code\":null,\"proj4def\":null,\"projectedBounds\":null,\"options\":{}}},\"calls\":[{\"method\":\"addTiles\",\"args\":[\"https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png\",null,null,{\"minZoom\":0,\"maxZoom\":18,\"tileSize\":256,\"subdomains\":\"abc\",\"errorTileUrl\":\"\",\"tms\":false,\"noWrap\":false,\"zoomOffset\":0,\"zoomReverse\":false,\"opacity\":1,\"zIndex\":1,\"detectRetina\":false,\"attribution\":\"&copy; <a href=\\\"https://openstreetmap.org\\\">OpenStreetMap<\\/a> contributors, <a href=\\\"https://creativecommons.org/licenses/by-sa/2.0/\\\">CC-BY-SA<\\/a>\"}]},{\"method\":\"addMarkers\",\"args\":[39.925,32.836944,null,null,null,{\"interactive\":true,\"draggable\":false,\"keyboard\":true,\"title\":\"\",\"alt\":\"\",\"zIndexOffset\":0,\"opacity\":1,\"riseOnHover\":false,\"riseOffset\":250},\"Anıtkabir\",null,null,null,null,{\"interactive\":false,\"permanent\":false,\"direction\":\"auto\",\"opacity\":1,\"offset\":[0,0],\"textsize\":\"10px\",\"textOnly\":false,\"className\":\"\",\"sticky\":true},null]}],\"limits\":{\"lat\":[39.925,39.925],\"lng\":[32.836944,32.836944]}},\"evals\":[],\"jsHooks\":[]}\r\nÇektiğimiz verileri artık haritaya aktarabiliriz.\r\n\r\n\r\npalet <- colorFactor(palette = c(\"gray\",\"orange\",\"red\"),\r\n                     domain = master$SUBSCRIBER_DOMESTIC_FOREIGN) # renklendirme\r\n\r\nmaster %>% \r\n  leaflet() %>% \r\n  addTiles() %>% \r\n  addProviderTiles(provider = providers$CartoDB.DarkMatterNoLabels) %>% # harita tipi\r\n  addCircles(lng = ~LONGITUDE,\r\n             lat = ~LATITUDE,\r\n             weight = 3,\r\n             color = ~palet(SUBSCRIBER_DOMESTIC_FOREIGN),\r\n             popup=paste(\"Abonelik Tarihi:\", master$SUBSCRIPTION_DATE, \"<br>\",\r\n                         \"Abone Tipi:\", master$SUBSCRIBER_DOMESTIC_FOREIGN, \"<br>\", \r\n                         \"Abone Sayısı:\", master$NUMBER_OF_SUBSCRIBER, \"<br>\")) %>% # nokta\r\n  addLegend(position = \"bottomright\",\r\n            pal = palet,\r\n            values = master$SUBSCRIBER_DOMESTIC_FOREIGN,\r\n            title = \"\",\r\n            opacity = 1) # lejant\r\n\r\n\r\n\r\n{\"x\":{\"options\":{\"crs\":{\"crsClass\":\"L.CRS.EPSG3857\",\"code\":null,\"proj4def\":null,\"projectedBounds\":null,\"options\":{}}},\"calls\":[{\"method\":\"addTiles\",\"args\":[\"https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png\",null,null,{\"minZoom\":0,\"maxZoom\":18,\"tileSize\":256,\"subdomains\":\"abc\",\"errorTileUrl\":\"\",\"tms\":false,\"noWrap\":false,\"zoomOffset\":0,\"zoomReverse\":false,\"opacity\":1,\"zIndex\":1,\"detectRetina\":false,\"attribution\":\"&copy; <a href=\\\"https://openstreetmap.org\\\">OpenStreetMap<\\/a> contributors, <a href=\\\"https://creativecommons.org/licenses/by-sa/2.0/\\\">CC-BY-SA<\\/a>\"}]},{\"method\":\"addProviderTiles\",\"args\":[\"CartoDB.DarkMatterNoLabels\",null,null,{\"errorTileUrl\":\"\",\"noWrap\":false,\"detectRetina\":false}]},{\"method\":\"addCircles\",\"args\":[[40.9927,41.017006,40.954044,41.243239,40.90128,41.009845,41.040737,41.01124,40.984185,41.008506,41.037058,41.068341,40.978551,41.044715,40.98844,41.008841,41.011242,40.978484,41.01762,41.029178,40.892454,40.96082,41.08589,40.96995,40.9999,41.041025,41.010555,41.034462,40.92453,41.0176,40.990604,41.00019,40.99213,41.0085,41.04874,41.00683,40.98075,41.057795,41.02604,41.1341,40.96544,40.975596,41.108199,41.0101,40.98656,41.16718,41.0081,40.99213,40.988251,41.029852,41.029178,41.080041,41.03932,40.920918,41.02985,41.00884,41.04638,40.978551,41.007712,41.082908,41.041218,41.02727,41.021925,41.047359,41.00894,40.929618,40.960154,41.018207,41.10443,40.985936,41.021925,40.978484,41.01418,40.91249,40.868536,41.072225,41.053895,41.05301,40.99545,41.10459,41.392387,41.00683,41.01231,41.03172,41.01475,41.024939,40.99253,41.104596,41.040605,41.005342,40.978551,41.017514,41.09774,41.09255,41.01181,41.040117,41.03908,41.053895,41.035982,41.013451,40.88494,41.067514,41.003475,41.00894,40.99213,41.03274,40.892454,40.992707,41.068341,41.01411,40.978551,41.183378,41.02055,40.985878,41.0536,40.98687,41.075281,40.978484,40.924531,41.02727,40.99353,41.035648,40.99203,41.037058,40.892454,41.072225,41.105707,40.96714,41.0182,41.017572,41.00249,41.06751,41.01552,41.243239,41.022063,41.055284,40.9906,41.002993,41.039082,41.061808,40.992531,41.080041,41.00148,41.015653,41.02979,41.07742,40.98844,41.057795,41.108199,40.999602,41.01496,41.01565,41.04744,41.017609,41.030498,40.99213,40.992755,41.068341,41.01758,41.002499,41.02224,41.06034,41.001919,40.901284,41.01757,40.968617,41.082908,41.06409,41.0536,41.03834,41.00788,41.04477,41.087819,41.00919,41.02224,41.075095,41.010197,41.00683,40.987307,41.048105,40.978484,41.108199,40.93897,41.0696,41.050219,41.01285,41.009663,41.0176,41.0081,41.014892,41.00683,41.00919,40.978551,41.036929,41.02881,41.011816,41.012278,41.060332,41.01181,40.90657,41.014079,40.978551,41.009331,41.022246,41.051604,40.975509,41.104805,40.96024,41.02556,41.0182,41.047359,41.02396,40.963136,41.00012,41.038348,41.143438,41.058001,41.053734,41.082908,41.064094,40.97088,40.992531,40.978551,41.01211,41.037058,41.024776,41.064094,41.05983,41.01751,41.17954,41.04121,41.01211,40.892454,40.99353,41.027853,41.07324,41.065278,41.22773,40.892454,41.1057,41.187223,40.99453,41.10402,41.061599,40.99213,40.892454,40.978551,41.083861,41.02371,41.054031,41.0182,41.031331,41.310704,41.1075,41.047446,40.99845,41.162284,41.004253,41.0118,41.031331,41.0118,41.03274,40.593952,41.008899,40.924531,41.00148,41.07407,41.057795,41.064818,40.947455,40.954044,40.912915,40.995034,40.9927,40.999602,41.10119,41.0118,41.01654,41.056851,41.03992,41.01019,41.080995,41.1341,41.083861,40.593952,41.083861,40.91249,41.017758,41.207132,41.179543,41.08854,41.007323,40.99845,41.01475,41.08781,41.017572,41.01654,41.01654,41.03423,41.00534,40.905838,41.03345,41.107116,41.0696,41.009543,40.99017,41.068341,40.978484,41.01489,41.03148,40.990178,40.990178,40.988449,41.097747,40.96219,41.046827,41.04725,41.03274,41.00956,41.028811,41.035648,40.995149,41.049054,41.047359,40.99048,40.978551,41.068341,41.047446,41.012684,41.014079,41.01124,40.980151,40.93462,40.954095,41.06481,41.04744,41.044715,41.080041,41.02206,41.017609,41.027275,40.995052,41.01654,41.014188,41.082908,41.034326,41.00191,41.048105,40.97979,41.03697,41.111189,41.048207,41.01758,41.00954,40.924531,40.92091,41.015525,40.89103,40.99048,40.99537,41.0081,41.000016,41.064094,41.09806,40.96024,40.87441,41.0182,41.068858,40.960824,40.995034,41.083861,41.0274,41.03368,41.108199,41.105707,41.01418,41.04638,41.033384,41.031482,41.03404,41.032534,41.02949,41.080041,41.085679,41.044715,41.01552,40.888024,41.016151,41.00683,41.00534,41.048105,41.03432,41.047359,41.01751,40.97643,41.243239,40.93897,41.009561,41.033193,41.058,41.033458,41.07914,41.02917,41.04247,41.059831,41.02558,40.967629,41.06409,41.04725,41.01285,41.04644,40.978484,41.069605,41.0696,41.028379,41.029331,41.00683,41.031331,41.015149,41.03432,41.0518,40.960154,41.023941,41.04247,40.99704,41.03329,41.13554,41.033384,40.99203,40.99253,41.082908,41.083861,41.032315,41.16003,40.892454,41.007712,41.0696,41.0331,41.14402,41.047359,41.09394,41.03345,40.999602,41.047359,40.969123,40.920918,41.01489,41.082908,40.963136,41.0081,41.082908,41.02689,41.00012,41.017758,41.03705,41.00001,41.155494,41.04007,41.007712,40.88982,41.0182,41.01171,41.01418,40.977902,41.07254,40.978551,41.009197,40.83319,41.024776,41.01418,41.04725,41.033458,41.066865,41.080041,40.95253,41.072875,40.888024,40.994132,41.04644,41.009485,41.0573,41.097747,41.00805,40.96861,41.083229,41.03253,41.07914,40.924531,41.0101,40.888024,41.035648,41.02036,40.976437,40.994532,41.024225,41.01345,41.012137,41.037058,41.012535,40.97848],[29.02325,29.105516,29.08863,29.090822,29.14665,28.656623,29.118248,28.94929,28.725085,28.978365,28.985666,28.981998,28.820556,28.796872,28.78062,28.657735,28.949292,29.233057,29.04378,28.86084,29.189967,28.80734,28.96352,29.05786,28.7653,28.896005,28.909518,28.856627,29.12119,28.97397,28.77166,28.95307,29.02399,28.97836,28.88683,28.97778,28.79174,28.989035,28.93474,29.09185,29.1044,29.099479,29.053011,28.94658,28.86382,29.05751,28.97549,29.023992,28.790419,28.9668,28.86084,29.011473,28.89344,29.16611,28.9668,28.65773,28.98871,28.820556,28.972482,28.948922,29.00736,29.01488,29.063421,28.933511,28.97119,29.145264,28.812574,28.972923,28.78554,28.786787,29.063421,29.233057,28.58272,29.18304,29.256022,28.72489,28.873136,28.87899,28.86388,28.86224,28.453073,28.97778,28.86444,28.79054,29.24496,29.008469,29.22955,28.862245,28.894728,28.976958,28.820556,28.973529,28.8767,28.92277,28.97821,28.893607,28.98954,28.873136,28.59289,28.955328,29.20128,29.044642,28.922089,28.97119,29.02399,28.860657,29.189967,28.59777,28.981998,28.82949,28.820556,29.456303,29.1164,28.896656,28.75983,29.22317,29.138382,29.233057,29.121198,29.01488,28.71296,28.573054,28.845483,28.985666,29.189967,28.72489,28.860871,29.266654,28.97292,28.970257,28.96279,29.04464,28.95411,29.090822,28.921655,28.949284,28.77166,28.890811,28.989542,28.994621,29.229558,29.011473,28.89022,28.932981,28.96927,29.33885,28.78062,28.989035,29.053011,28.884828,29.03821,28.93298,29.02574,28.973976,28.951463,29.02399,28.835378,28.981998,28.97389,28.962793,28.97478,28.912558,28.767776,29.146659,28.97025,29.258734,28.948922,28.99246,28.75983,28.88197,28.8977,28.79624,28.944987,28.96677,28.97478,29.798666,28.97256,28.97778,28.788922,28.932298,29.233057,29.053011,29.31137,28.97888,29.053364,28.81876,28.945011,28.97397,28.97549,28.956406,28.977784,28.96677,28.820556,28.985969,28.93199,28.97821,29.074823,28.987285,28.97821,29.18905,29.190271,28.820556,28.966282,28.974782,28.976831,28.858946,29.073978,28.80753,28.93176,28.97292,28.933511,28.969,28.80068,28.95117,28.881976,29.055617,28.919846,28.932578,28.948922,28.992465,29.05788,29.229558,28.820556,29.011697,28.985666,28.929394,28.992465,29.00767,28.97352,28.75104,29.00736,29.011697,29.189967,28.71296,29.069071,28.25016,28.994942,28.45338,29.189967,28.86087,29.471361,28.90399,28.80075,28.902185,29.02399,29.189967,28.820556,28.926066,28.86374,28.867578,28.97292,28.989684,28.612294,28.80125,29.025741,29.05554,28.88337,28.922037,28.94026,28.989684,28.94026,28.860657,29.023297,29.020952,29.121198,28.89022,28.80074,28.989035,28.886847,29.10161,29.08863,29.188055,28.705535,29.02325,28.884828,28.9942,28.94026,28.951,28.907544,28.46788,28.97256,28.921595,29.09185,28.926066,29.023297,28.926066,29.18304,28.941171,29.131822,28.751046,28.76499,28.843902,29.05554,29.24496,28.94498,28.970257,28.951006,28.951006,29.15253,29.196707,29.161455,28.97738,28.876316,28.97888,28.953944,28.9037,28.981998,29.233057,28.9564,28.98943,28.9037,28.9037,28.780627,28.876702,28.80794,29.051149,28.94027,28.86065,28.95976,28.931995,28.573054,28.706011,29.015456,28.933511,28.92595,28.820556,28.981998,29.025741,28.938473,29.190271,28.94929,28.731161,29.15512,29.276736,28.88684,29.02574,28.796872,29.011473,28.92165,28.973976,29.014881,28.863467,28.951,28.98077,28.948922,28.992646,28.76777,28.932298,29.10319,28.92342,28.858182,28.990258,28.97389,28.95394,29.121198,29.16611,28.954114,29.25202,28.92595,29.06317,28.975497,28.888716,28.992465,28.87652,28.80753,29.12765,28.97292,28.897455,28.807347,28.705535,28.926066,29.0679,28.92817,29.053011,28.860871,28.58272,28.98871,28.870054,28.989436,28.63119,29.026143,28.930938,29.011473,28.982117,28.796872,28.87721,29.187151,28.971066,28.97778,29.1967,28.932298,28.99264,28.933511,28.97352,28.74381,29.090822,29.31137,28.959765,28.912704,28.91984,28.977381,28.89354,28.86084,29.00768,29.007671,28.92922,29.053326,28.99246,28.94027,28.81876,28.9328,29.233057,28.978885,28.97888,28.982257,28.77561,28.97778,28.989684,28.933466,28.99264,28.88765,28.812574,28.921474,29.00768,28.82524,28.86979,28.466635,28.870054,28.845483,29.22955,28.948922,28.926066,29.02601,29.0389,29.189967,28.972482,28.97888,28.472931,28.46099,28.933511,28.905229,28.97738,28.884828,28.933511,29.266941,29.16611,28.9564,28.948922,28.80068,28.975497,28.948922,28.98061,28.95117,28.941171,28.98566,28.88871,29.025047,28.88419,28.972482,29.18934,28.97292,28.650483,28.58272,29.147233,29.01554,28.820556,28.966779,29.29923,28.929394,28.98077,28.94027,28.977381,28.991766,29.011473,29.27685,29.455337,29.187151,29.026507,28.9328,28.965208,28.99127,28.876702,28.79335,29.25873,29.066626,29.02614,28.89354,29.121198,28.946583,29.187151,28.573054,28.57817,28.743816,28.903993,28.960271,28.95532,29.254897,28.985666,28.649426,29.23305],10,null,null,{\"interactive\":true,\"className\":\"\",\"stroke\":true,\"color\":[\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FFA500\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#BEBEBE\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FFA500\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FFA500\",\"#FFA500\",\"#FFA500\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FFA500\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FFA500\",\"#FFA500\",\"#FF0000\",\"#FFA500\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FFA500\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FFA500\",\"#FFA500\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FFA500\",\"#FFA500\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\"],\"weight\":3,\"opacity\":0.5,\"fill\":true,\"fillColor\":[\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FFA500\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#BEBEBE\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FFA500\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FFA500\",\"#FFA500\",\"#FFA500\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FFA500\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FFA500\",\"#FFA500\",\"#FF0000\",\"#FFA500\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FFA500\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FFA500\",\"#FFA500\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FFA500\",\"#FFA500\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\",\"#FFA500\",\"#FF0000\",\"#FF0000\"],\"fillOpacity\":0.2},[\"Abonelik Tarihi: 2020-12-27 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-04-16 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-06-25 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2019-01-23 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-03-01 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-07-08 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-07-13 <br> Abone Tipi: Yerli <br> Abone Sayısı: 32 <br>\",\"Abonelik Tarihi: 2021-02-21 <br> Abone Tipi: Yerli <br> Abone Sayısı: 3 <br>\",\"Abonelik Tarihi: 2021-07-08 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-04-22 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-05-25 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2019-06-30 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2019-02-16 <br> Abone Tipi: Yerli <br> Abone Sayısı: 5 <br>\",\"Abonelik Tarihi: 2019-08-04 <br> Abone Tipi: Yerli <br> Abone Sayısı: 8 <br>\",\"Abonelik Tarihi: 2020-11-30 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-05-25 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-06-29 <br> Abone Tipi: Yerli <br> Abone Sayısı: 11 <br>\",\"Abonelik Tarihi: 2018-05-11 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2020-12-05 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-06-10 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2020-01-10 <br> Abone Tipi: Yerli <br> Abone Sayısı: 4 <br>\",\"Abonelik Tarihi: 2020-11-17 <br> Abone Tipi: Yerli <br> Abone Sayısı: 3 <br>\",\"Abonelik Tarihi: 2020-11-18 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-03-05 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2020-11-10 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-06-02 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-06-20 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-07-06 <br> Abone Tipi: Yerli <br> Abone Sayısı: 18 <br>\",\"Abonelik Tarihi: 2021-01-15 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2020-12-11 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-06-30 <br> Abone Tipi: Yerli <br> Abone Sayısı: 3 <br>\",\"Abonelik Tarihi: 2021-03-09 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-03-29 <br> Abone Tipi: Yerli <br> Abone Sayısı: 32 <br>\",\"Abonelik Tarihi: 2021-02-25 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-03-30 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2020-12-03 <br> Abone Tipi: Yerli <br> Abone Sayısı: 7 <br>\",\"Abonelik Tarihi: 2020-11-17 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2019-07-20 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-01-20 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2020-12-26 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2020-12-15 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2020-10-28 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-07-07 <br> Abone Tipi: Yerli <br> Abone Sayısı: 16 <br>\",\"Abonelik Tarihi: 2021-03-01 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-01-12 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-02-05 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2020-11-25 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-05-28 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-07-08 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-06-30 <br> Abone Tipi: Yerli <br> Abone Sayısı: 3 <br>\",\"Abonelik Tarihi: 2021-04-15 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2019-08-05 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2020-10-29 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-06-07 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2020-11-10 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-01-05 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-01-22 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2017-10-16 <br> Abone Tipi: Yerli <br> Abone Sayısı: 9 <br>\",\"Abonelik Tarihi: 2019-03-08 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2018-03-12 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-05-01 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-01-14 <br> Abone Tipi: Yerli <br> Abone Sayısı: 5 <br>\",\"Abonelik Tarihi: 2021-04-28 <br> Abone Tipi: Yerli <br> Abone Sayısı: 6 <br>\",\"Abonelik Tarihi: 2018-11-23 <br> Abone Tipi: Yerli <br> Abone Sayısı: 3 <br>\",\"Abonelik Tarihi: 2021-01-24 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2020-10-27 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-04-18 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-04-26 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2020-12-09 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-05-13 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-06-07 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2018-11-27 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2018-07-22 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-04-10 <br> Abone Tipi: Yerli <br> Abone Sayısı: 4 <br>\",\"Abonelik Tarihi: 2021-07-10 <br> Abone Tipi: Yerli <br> Abone Sayısı: 67 <br>\",\"Abonelik Tarihi: 2021-06-11 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2019-07-07 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-01-25 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-04-05 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-01-18 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-07-19 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2020-11-01 <br> Abone Tipi: Yerli <br> Abone Sayısı: 11 <br>\",\"Abonelik Tarihi: 2021-01-07 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-01-05 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-01-27 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-07-13 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 10 <br>\",\"Abonelik Tarihi: 2021-02-28 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-06-19 <br> Abone Tipi: Yerli <br> Abone Sayısı: 6 <br>\",\"Abonelik Tarihi: 2021-06-26 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-05-24 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2020-07-04 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-04-30 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-01-18 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-02-28 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-02-01 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-05-25 <br> Abone Tipi: Yerli <br> Abone Sayısı: 5 <br>\",\"Abonelik Tarihi: 2020-12-12 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-06-10 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-07-17 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-04-26 <br> Abone Tipi: Yerli <br> Abone Sayısı: 4 <br>\",\"Abonelik Tarihi: 2021-02-05 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-06-14 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-07-06 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-02-25 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-01-16 <br> Abone Tipi: Yerli <br> Abone Sayısı: 3 <br>\",\"Abonelik Tarihi: 2021-05-19 <br> Abone Tipi: Yerli <br> Abone Sayısı: 4 <br>\",\"Abonelik Tarihi: 2019-02-24 <br> Abone Tipi: Yerli <br> Abone Sayısı: 4 <br>\",\"Abonelik Tarihi: 2021-06-22 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2019-09-12 <br> Abone Tipi: Bilinmiyor <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2020-12-30 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2018-02-21 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-06-24 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-02-03 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-06-12 <br> Abone Tipi: Yerli <br> Abone Sayısı: 3 <br>\",\"Abonelik Tarihi: 2021-07-08 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2020-12-02 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2018-06-12 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2019-09-27 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2019-09-15 <br> Abone Tipi: Yerli <br> Abone Sayısı: 44 <br>\",\"Abonelik Tarihi: 2020-12-13 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2020-11-18 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2016-08-27 <br> Abone Tipi: Yerli <br> Abone Sayısı: 4 <br>\",\"Abonelik Tarihi: 2021-07-02 <br> Abone Tipi: Yerli <br> Abone Sayısı: 9 <br>\",\"Abonelik Tarihi: 2021-06-06 <br> Abone Tipi: Yerli <br> Abone Sayısı: 10 <br>\",\"Abonelik Tarihi: 2019-10-14 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-07-13 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-04-16 <br> Abone Tipi: Yerli <br> Abone Sayısı: 4 <br>\",\"Abonelik Tarihi: 2021-05-07 <br> Abone Tipi: Yerli <br> Abone Sayısı: 4 <br>\",\"Abonelik Tarihi: 2020-11-17 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-06-25 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 12 <br>\",\"Abonelik Tarihi: 2021-01-12 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-02-18 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-04-06 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2018-11-16 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-06-12 <br> Abone Tipi: Yerli <br> Abone Sayısı: 5 <br>\",\"Abonelik Tarihi: 2021-06-30 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-04-11 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-04-13 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-05-05 <br> Abone Tipi: Yerli <br> Abone Sayısı: 4 <br>\",\"Abonelik Tarihi: 2021-04-28 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-06-04 <br> Abone Tipi: Yerli <br> Abone Sayısı: 4 <br>\",\"Abonelik Tarihi: 2019-10-16 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2020-10-31 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-05-28 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-06-06 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2020-12-03 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2020-11-10 <br> Abone Tipi: Yerli <br> Abone Sayısı: 6 <br>\",\"Abonelik Tarihi: 2019-04-07 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-04-26 <br> Abone Tipi: Yerli <br> Abone Sayısı: 26 <br>\",\"Abonelik Tarihi: 2019-12-25 <br> Abone Tipi: Yerli <br> Abone Sayısı: 3 <br>\",\"Abonelik Tarihi: 2020-12-13 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2020-12-03 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2020-11-06 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-04-22 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 8 <br>\",\"Abonelik Tarihi: 2021-05-04 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2020-12-15 <br> Abone Tipi: Yerli <br> Abone Sayısı: 4 <br>\",\"Abonelik Tarihi: 2021-07-02 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2019-12-19 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2020-12-03 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-07-03 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 3 <br>\",\"Abonelik Tarihi: 2021-03-02 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2018-12-02 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-06-26 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 3 <br>\",\"Abonelik Tarihi: 2021-06-29 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-02-09 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 4 <br>\",\"Abonelik Tarihi: 2021-05-07 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2019-01-10 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-02-11 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-01-05 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-02-26 <br> Abone Tipi: Yerli <br> Abone Sayısı: 6 <br>\",\"Abonelik Tarihi: 2021-04-07 <br> Abone Tipi: Yerli <br> Abone Sayısı: 4 <br>\",\"Abonelik Tarihi: 2021-03-02 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-06-04 <br> Abone Tipi: Yerli <br> Abone Sayısı: 6 <br>\",\"Abonelik Tarihi: 2020-11-01 <br> Abone Tipi: Yerli <br> Abone Sayısı: 6 <br>\",\"Abonelik Tarihi: 2021-02-02 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 3 <br>\",\"Abonelik Tarihi: 2021-07-02 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-07-11 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2020-12-31 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 51 <br>\",\"Abonelik Tarihi: 2019-05-24 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-05-20 <br> Abone Tipi: Yerli <br> Abone Sayısı: 6 <br>\",\"Abonelik Tarihi: 2018-04-25 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-06-25 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 5 <br>\",\"Abonelik Tarihi: 2020-12-28 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-02-22 <br> Abone Tipi: Yerli <br> Abone Sayısı: 5 <br>\",\"Abonelik Tarihi: 2021-05-06 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-02-16 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2019-09-12 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-02-25 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2020-11-13 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 8 <br>\",\"Abonelik Tarihi: 2021-07-03 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-05-16 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 64 <br>\",\"Abonelik Tarihi: 2020-11-29 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 11 <br>\",\"Abonelik Tarihi: 2018-05-04 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-06-14 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2020-12-18 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-07-03 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 6 <br>\",\"Abonelik Tarihi: 2021-06-15 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-05-26 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2020-12-31 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2019-02-24 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-05-17 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2018-03-09 <br> Abone Tipi: Yerli <br> Abone Sayısı: 3 <br>\",\"Abonelik Tarihi: 2021-07-07 <br> Abone Tipi: Yerli <br> Abone Sayısı: 10 <br>\",\"Abonelik Tarihi: 2021-05-09 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 10 <br>\",\"Abonelik Tarihi: 2021-04-27 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-06-05 <br> Abone Tipi: Yerli <br> Abone Sayısı: 17 <br>\",\"Abonelik Tarihi: 2021-05-19 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2020-11-01 <br> Abone Tipi: Yerli <br> Abone Sayısı: 7 <br>\",\"Abonelik Tarihi: 2021-06-21 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2020-11-05 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 5 <br>\",\"Abonelik Tarihi: 2018-08-27 <br> Abone Tipi: Yerli <br> Abone Sayısı: 10 <br>\",\"Abonelik Tarihi: 2021-01-29 <br> Abone Tipi: Yerli <br> Abone Sayısı: 4 <br>\",\"Abonelik Tarihi: 2021-07-15 <br> Abone Tipi: Yerli <br> Abone Sayısı: 4 <br>\",\"Abonelik Tarihi: 2021-01-12 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-07-04 <br> Abone Tipi: Yerli <br> Abone Sayısı: 18 <br>\",\"Abonelik Tarihi: 2020-02-19 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-05-08 <br> Abone Tipi: Yerli <br> Abone Sayısı: 4 <br>\",\"Abonelik Tarihi: 2021-05-31 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2017-10-19 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-04-24 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-03-05 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-07-06 <br> Abone Tipi: Yerli <br> Abone Sayısı: 3 <br>\",\"Abonelik Tarihi: 2018-11-24 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-01-14 <br> Abone Tipi: Yerli <br> Abone Sayısı: 3 <br>\",\"Abonelik Tarihi: 2021-05-05 <br> Abone Tipi: Yerli <br> Abone Sayısı: 10 <br>\",\"Abonelik Tarihi: 2021-07-06 <br> Abone Tipi: Yerli <br> Abone Sayısı: 3 <br>\",\"Abonelik Tarihi: 2021-04-20 <br> Abone Tipi: Yerli <br> Abone Sayısı: 20 <br>\",\"Abonelik Tarihi: 2021-02-18 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2020-11-05 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 3 <br>\",\"Abonelik Tarihi: 2021-03-05 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-04-11 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 8 <br>\",\"Abonelik Tarihi: 2021-05-22 <br> Abone Tipi: Yerli <br> Abone Sayısı: 4 <br>\",\"Abonelik Tarihi: 2018-07-23 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-01-29 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-07-09 <br> Abone Tipi: Yerli <br> Abone Sayısı: 10 <br>\",\"Abonelik Tarihi: 2020-12-03 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-07-09 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2020-12-31 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2018-08-08 <br> Abone Tipi: Yerli <br> Abone Sayısı: 8 <br>\",\"Abonelik Tarihi: 2020-12-09 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-06-24 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-02-04 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2020-11-23 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-05-24 <br> Abone Tipi: Yerli <br> Abone Sayısı: 4 <br>\",\"Abonelik Tarihi: 2021-02-25 <br> Abone Tipi: Yerli <br> Abone Sayısı: 11 <br>\",\"Abonelik Tarihi: 2019-05-07 <br> Abone Tipi: Yerli <br> Abone Sayısı: 3 <br>\",\"Abonelik Tarihi: 2018-09-25 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2019-10-17 <br> Abone Tipi: Yerli <br> Abone Sayısı: 3 <br>\",\"Abonelik Tarihi: 2021-03-06 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-05-25 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-03-29 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 12 <br>\",\"Abonelik Tarihi: 2018-01-21 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-07-11 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-02-11 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-06-18 <br> Abone Tipi: Yerli <br> Abone Sayısı: 4 <br>\",\"Abonelik Tarihi: 2020-11-13 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-05-27 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2019-05-16 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-02-05 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2018-08-09 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-02-21 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-04-17 <br> Abone Tipi: Yerli <br> Abone Sayısı: 4 <br>\",\"Abonelik Tarihi: 2015-12-02 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-06-23 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-06-12 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2020-11-16 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-04-08 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2019-10-16 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-05-17 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-06-16 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-05-24 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-05-20 <br> Abone Tipi: Yerli <br> Abone Sayısı: 4 <br>\",\"Abonelik Tarihi: 2019-10-07 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2020-12-11 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 4 <br>\",\"Abonelik Tarihi: 2019-09-28 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-01-05 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2020-12-11 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2020-12-07 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-05-25 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2018-05-20 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-03-01 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2018-11-15 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2020-11-16 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2018-03-04 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2017-04-19 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2018-03-20 <br> Abone Tipi: Yerli <br> Abone Sayısı: 4 <br>\",\"Abonelik Tarihi: 2021-02-13 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2018-10-03 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-06-04 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-07-03 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2019-02-22 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-07-02 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-01-08 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2020-10-31 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-04-05 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-04-14 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 22 <br>\",\"Abonelik Tarihi: 2021-04-18 <br> Abone Tipi: Yerli <br> Abone Sayısı: 4 <br>\",\"Abonelik Tarihi: 2021-05-04 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-02-11 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-05-11 <br> Abone Tipi: Yerli <br> Abone Sayısı: 26 <br>\",\"Abonelik Tarihi: 2019-10-14 <br> Abone Tipi: Yerli <br> Abone Sayısı: 3 <br>\",\"Abonelik Tarihi: 2020-11-14 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 34 <br>\",\"Abonelik Tarihi: 2021-07-13 <br> Abone Tipi: Yerli <br> Abone Sayısı: 4 <br>\",\"Abonelik Tarihi: 2020-12-28 <br> Abone Tipi: Yerli <br> Abone Sayısı: 4 <br>\",\"Abonelik Tarihi: 2021-04-26 <br> Abone Tipi: Yerli <br> Abone Sayısı: 8 <br>\",\"Abonelik Tarihi: 2020-10-31 <br> Abone Tipi: Yerli <br> Abone Sayısı: 9 <br>\",\"Abonelik Tarihi: 2019-05-17 <br> Abone Tipi: Yerli <br> Abone Sayısı: 4 <br>\",\"Abonelik Tarihi: 2018-03-16 <br> Abone Tipi: Yerli <br> Abone Sayısı: 3 <br>\",\"Abonelik Tarihi: 2021-03-01 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2020-11-02 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-05-27 <br> Abone Tipi: Yerli <br> Abone Sayısı: 9 <br>\",\"Abonelik Tarihi: 2021-07-10 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 4 <br>\",\"Abonelik Tarihi: 2021-04-19 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-06-11 <br> Abone Tipi: Yerli <br> Abone Sayısı: 3 <br>\",\"Abonelik Tarihi: 2021-04-02 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-07-17 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2020-01-03 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-02-28 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2020-11-01 <br> Abone Tipi: Yerli <br> Abone Sayısı: 3 <br>\",\"Abonelik Tarihi: 2021-07-14 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2017-01-18 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-07-08 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-05-05 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2018-04-21 <br> Abone Tipi: Yerli <br> Abone Sayısı: 14 <br>\",\"Abonelik Tarihi: 2021-02-01 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2019-07-14 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2020-02-29 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-06-22 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 3 <br>\",\"Abonelik Tarihi: 2020-10-28 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 3 <br>\",\"Abonelik Tarihi: 2020-10-29 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2020-11-25 <br> Abone Tipi: Yerli <br> Abone Sayısı: 3 <br>\",\"Abonelik Tarihi: 2019-09-29 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-01-15 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-07-02 <br> Abone Tipi: Yerli <br> Abone Sayısı: 4 <br>\",\"Abonelik Tarihi: 2020-10-29 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2020-11-10 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 3 <br>\",\"Abonelik Tarihi: 2020-01-11 <br> Abone Tipi: Yerli <br> Abone Sayısı: 3 <br>\",\"Abonelik Tarihi: 2019-05-13 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2020-12-02 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-07-19 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-06-11 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 4 <br>\",\"Abonelik Tarihi: 2021-06-08 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-01-04 <br> Abone Tipi: Yerli <br> Abone Sayısı: 8 <br>\",\"Abonelik Tarihi: 2021-07-15 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2019-08-14 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-05-17 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2020-11-18 <br> Abone Tipi: Yerli <br> Abone Sayısı: 3 <br>\",\"Abonelik Tarihi: 2021-07-03 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-02-01 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-02-20 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-05-10 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-07-17 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2020-11-05 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2020-12-11 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2018-05-23 <br> Abone Tipi: Yerli <br> Abone Sayısı: 19 <br>\",\"Abonelik Tarihi: 2020-11-27 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-04-14 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2020-12-08 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-02-17 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-02-17 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-07-07 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-04-29 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-07-05 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 3 <br>\",\"Abonelik Tarihi: 2021-01-26 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2020-11-05 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-02-07 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-04-10 <br> Abone Tipi: Yerli <br> Abone Sayısı: 10 <br>\",\"Abonelik Tarihi: 2021-06-01 <br> Abone Tipi: Yerli <br> Abone Sayısı: 5 <br>\",\"Abonelik Tarihi: 2021-07-11 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2019-10-23 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2019-12-12 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2020-11-19 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-02-04 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-07-11 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 4 <br>\",\"Abonelik Tarihi: 2021-07-13 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2018-08-23 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2020-11-27 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2018-05-27 <br> Abone Tipi: Yerli <br> Abone Sayısı: 6 <br>\",\"Abonelik Tarihi: 2021-04-27 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2020-12-29 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-07-01 <br> Abone Tipi: Yerli <br> Abone Sayısı: 4 <br>\",\"Abonelik Tarihi: 2021-06-28 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2019-04-15 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-07-08 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2019-03-13 <br> Abone Tipi: Yerli <br> Abone Sayısı: 5 <br>\",\"Abonelik Tarihi: 2020-11-06 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2017-09-22 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-06-25 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 3 <br>\",\"Abonelik Tarihi: 2021-02-13 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2020-10-31 <br> Abone Tipi: Yerli <br> Abone Sayısı: 14 <br>\",\"Abonelik Tarihi: 2021-07-15 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-01-21 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2018-03-20 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-01-28 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-01-05 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2019-04-05 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-02-18 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-07-15 <br> Abone Tipi: Yerli <br> Abone Sayısı: 4 <br>\",\"Abonelik Tarihi: 2019-06-13 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2020-12-31 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-06-06 <br> Abone Tipi: Yerli <br> Abone Sayısı: 13 <br>\",\"Abonelik Tarihi: 2021-03-05 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-02-10 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-01-05 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 5 <br>\",\"Abonelik Tarihi: 2021-04-23 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-02-28 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-06-17 <br> Abone Tipi: Yerli <br> Abone Sayısı: 6 <br>\",\"Abonelik Tarihi: 2020-12-29 <br> Abone Tipi: Yerli <br> Abone Sayısı: 7 <br>\",\"Abonelik Tarihi: 2018-12-27 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-03-04 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-01-19 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2020-03-05 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-06-22 <br> Abone Tipi: Yerli <br> Abone Sayısı: 7 <br>\",\"Abonelik Tarihi: 2020-11-02 <br> Abone Tipi: Yerli <br> Abone Sayısı: 3 <br>\",\"Abonelik Tarihi: 2021-07-12 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-06-22 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-03-29 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 140 <br>\",\"Abonelik Tarihi: 2018-04-07 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-05-20 <br> Abone Tipi: Yerli <br> Abone Sayısı: 4 <br>\",\"Abonelik Tarihi: 2020-11-14 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-03-30 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-07-15 <br> Abone Tipi: Yerli <br> Abone Sayısı: 7 <br>\",\"Abonelik Tarihi: 2021-05-01 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2020-12-08 <br> Abone Tipi: Yerli <br> Abone Sayısı: 4 <br>\",\"Abonelik Tarihi: 2021-02-24 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-01-11 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-07-03 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2018-08-07 <br> Abone Tipi: Yerli <br> Abone Sayısı: 3 <br>\",\"Abonelik Tarihi: 2021-05-31 <br> Abone Tipi: Yerli <br> Abone Sayısı: 8 <br>\",\"Abonelik Tarihi: 2020-11-16 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2017-11-04 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2018-07-26 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-07-01 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-04-02 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2019-08-03 <br> Abone Tipi: Yerli <br> Abone Sayısı: 7 <br>\",\"Abonelik Tarihi: 2017-12-31 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2020-11-30 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-06-06 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-03-29 <br> Abone Tipi: Yerli <br> Abone Sayısı: 4 <br>\",\"Abonelik Tarihi: 2019-08-22 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-06-03 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-01-21 <br> Abone Tipi: Yerli <br> Abone Sayısı: 9 <br>\",\"Abonelik Tarihi: 2018-05-28 <br> Abone Tipi: Yerli <br> Abone Sayısı: 6 <br>\",\"Abonelik Tarihi: 2018-12-21 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-04-13 <br> Abone Tipi: Yerli <br> Abone Sayısı: 4 <br>\",\"Abonelik Tarihi: 2021-06-23 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-01-04 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2019-01-26 <br> Abone Tipi: Yerli <br> Abone Sayısı: 4 <br>\",\"Abonelik Tarihi: 2021-06-05 <br> Abone Tipi: Yerli <br> Abone Sayısı: 7 <br>\",\"Abonelik Tarihi: 2021-04-14 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 8 <br>\",\"Abonelik Tarihi: 2018-08-19 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-03-02 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-03-09 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2018-06-13 <br> Abone Tipi: Yerli <br> Abone Sayısı: 4 <br>\",\"Abonelik Tarihi: 2021-02-18 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2020-11-08 <br> Abone Tipi: Yerli <br> Abone Sayısı: 17 <br>\",\"Abonelik Tarihi: 2021-05-28 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-01-25 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2019-07-18 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-01-12 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2020-12-31 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 4 <br>\",\"Abonelik Tarihi: 2021-05-25 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2019-04-24 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2019-06-28 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-02-07 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2019-03-04 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-07-07 <br> Abone Tipi: Yerli <br> Abone Sayısı: 10 <br>\",\"Abonelik Tarihi: 2021-01-07 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-06-24 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-02-17 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2020-02-16 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-05-09 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 16 <br>\",\"Abonelik Tarihi: 2021-07-09 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2019-12-15 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2020-11-28 <br> Abone Tipi: Yerli <br> Abone Sayısı: 3 <br>\",\"Abonelik Tarihi: 2021-07-10 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2018-09-21 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-07-07 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2020-11-08 <br> Abone Tipi: Yerli <br> Abone Sayısı: 7 <br>\",\"Abonelik Tarihi: 2021-06-05 <br> Abone Tipi: Yerli <br> Abone Sayısı: 4 <br>\",\"Abonelik Tarihi: 2020-11-03 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-04-16 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-02-22 <br> Abone Tipi: Yerli <br> Abone Sayısı: 4 <br>\",\"Abonelik Tarihi: 2021-01-23 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-06-04 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2020-11-03 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2020-11-13 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-06-24 <br> Abone Tipi: Yerli <br> Abone Sayısı: 26 <br>\",\"Abonelik Tarihi: 2021-04-30 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2019-02-08 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2019-12-18 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2020-11-17 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-04-19 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-06-12 <br> Abone Tipi: Yerli <br> Abone Sayısı: 5 <br>\",\"Abonelik Tarihi: 2021-07-03 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-01-04 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2021-06-28 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\",\"Abonelik Tarihi: 2021-05-09 <br> Abone Tipi: Yabancı <br> Abone Sayısı: 10 <br>\",\"Abonelik Tarihi: 2021-06-30 <br> Abone Tipi: Yerli <br> Abone Sayısı: 2 <br>\",\"Abonelik Tarihi: 2020-12-02 <br> Abone Tipi: Yerli <br> Abone Sayısı: 1 <br>\"],null,null,{\"interactive\":false,\"permanent\":false,\"direction\":\"auto\",\"opacity\":1,\"offset\":[0,0],\"textsize\":\"10px\",\"textOnly\":false,\"className\":\"\",\"sticky\":true},null,null]},{\"method\":\"addLegend\",\"args\":[{\"colors\":[\"#BEBEBE\",\"#FFA500\",\"#FF0000\"],\"labels\":[\"Bilinmiyor\",\"Yabancı\",\"Yerli\"],\"na_color\":null,\"na_label\":\"NA\",\"opacity\":1,\"position\":\"bottomright\",\"type\":\"factor\",\"title\":\"\",\"extra\":null,\"layerId\":null,\"className\":\"info legend\",\"group\":null}]}],\"limits\":{\"lat\":[40.593952,41.392387],\"lng\":[28.25016,29.798666]}},\"evals\":[],\"jsHooks\":[]}\r\nYukarıdaki harita zoomlanabilir; haritadan noktaların üzerine\r\ntıklanarak bilgi alınabilir.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-06-11-post11/wifi.png",
    "last_modified": "2022-06-11T14:54:18+03:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-06-08-post10/",
    "title": "TCMB Başkanlarının Kendi Döneminde Döviz Kuru Volatilitesi ve Değişimi",
    "description": "TCMB başkanlarının kurdaki volatilite ve değişim bazlı performansı.",
    "author": [
      {
        "name": "A. Uraz Akgül",
        "url": {}
      }
    ],
    "date": "2022-06-08",
    "categories": [
      "Finance"
    ],
    "contents": "\r\nÇalışma boyunca kurdaki değişim, eşit ağırlıklandırılmış volatilite\r\nve özellikle açıklayacağım eşit ağırlıklandırılmamış volatiliteden\r\n(EWMA) bahsedeceğim.\r\nExponential Weighted Moving Average kelimelerinin kısaltmasından\r\noluşan ve Üssel Hareketli Ortalamalar olarak çevrilen EWMA, geçmiş\r\nvolatilitenin ortalama hareketiyle gelecekteki volatiliteyi hesaplamak\r\niçin zaman ve volatiliteyi ilişkilendiren en popüler volatilite\r\nmodellerinden birisidir. EWMA’da, geçmiş gözlemler üssel olarak\r\nağırlıklandırılmak ile beraber yakın geçmişteki gözlemlere daha çok;\r\nuzak geçmişteki gözlemlere ise daha az ağırlık verilir. EWMA, varlık\r\ngetirilerinin simetrik ve bağımsız olarak dağıldığı prensibi üzerine\r\nkurulmuştur.\r\nEWMA modeli şöyle yazılabilir:\r\n\\(\\sigma^2_n = \\lambda\\sigma^2_{n-1} + (1 -\r\n\\lambda)u^2_{n-1}\\)\r\n\\(\\lambda\\), 0 ile 1 arasında değer\r\nalır (\\(0 < \\lambda < 1\\)). \\(\\lambda\\) değeri 1’e yaklaştıkça her\r\ngözleme eşit ağırlık verilmesi yönündeki eğilim artar. \\(\\lambda = 1\\) hareketli ortalama (Moving\r\nAverage, MA) ile aynıdır. Diğer bir açıdan, yüksek \\(\\lambda\\) güncel piyasa hareketlerine daha\r\nzayıf tepki demektir. Literatürde, günlük verilerde \\(\\lambda = 0.94\\), aylık verilerde \\(\\lambda = 0.97\\) önerilmiştir. Tabi bu\r\ndeğerler değişebilmektedir.\r\n\\(\\lambda\\) dışında modelde;\r\n\\(\\sigma^2_n:\\) Bugünün\r\nvaryansı,\r\n\\(\\sigma^2_{n-1}:\\) Dünün\r\nvaryansı,\r\n\\(u^2_{n-1}:\\) Dünün getirisinin\r\nkaresidir.\r\nYukarıda gördüğümüz EWMA modelinde iki bileşen vardır. \\((1-\\lambda)u^2_{n-1}\\) bileşeni,\r\nvolatilitenin finansal piyasalarda meydana gelen fiyat hareketlerine\r\nkarşı hassasiyetini gösterirken; \\(\\lambda\\sigma^2_{n-1}\\) bileşeni ise piyasa\r\nhareketlerinden bağımsız olarak önceki dönemdeki volatilitenin cari\r\ndönem volatilitesi üzerindeki etkisini göstermektedir. Bu bileşen aynı\r\nzamanda volatilite direncidir. Buradan hareketle \\(\\lambda\\) parametresi için şunu\r\ndiyebiliriz: Parametre 1’e yakın değer alırsa volatilite direnci piyasa\r\nhareketlerine baskın gelmekte; 0’a yakın değer alırsa piyasa hareketleri\r\nbelirleyici olmaktadır.\r\nUygulamada, TCMB başkanları dönemine ait USDTRY kur volatilitesini\r\nEWMA ile hesaplayacağız ki bu volatilite bir başkanın diğerine\r\ndevrettiği volatilite olacak. Bunun yanında kendi döneminde yarattığı\r\nvolatilite ile kurdaki değişimi de hesaplayacağız.\r\nVikipedi’den\r\naldığım başkanlar ve görev tarihlerine ait bilgiler aşağıdadır.\r\nSerdengeçti ile Yılmaz arasındaki boşlukta atama ile ilgili bir veto\r\nyaşanmıştır.\r\n\r\n\r\nNo\r\n\r\n\r\nGovernor\r\n\r\n\r\nStart\r\n\r\n\r\nEnd\r\n\r\n\r\n19\r\n\r\n\r\nSüreyya Serdengeçti\r\n\r\n\r\n2001-03-14\r\n\r\n\r\n2006-03-14\r\n\r\n\r\n20\r\n\r\n\r\nDurmuş Yılmaz\r\n\r\n\r\n2006-04-18\r\n\r\n\r\n2011-04-13\r\n\r\n\r\n21\r\n\r\n\r\nErdem Başçı\r\n\r\n\r\n2011-04-14\r\n\r\n\r\n2016-04-19\r\n\r\n\r\n22\r\n\r\n\r\nMurat Çetinkaya\r\n\r\n\r\n2016-04-19\r\n\r\n\r\n2019-07-06\r\n\r\n\r\n23\r\n\r\n\r\nMurat Uysal\r\n\r\n\r\n2019-07-06\r\n\r\n\r\n2020-11-07\r\n\r\n\r\n24\r\n\r\n\r\nNaci Ağbal\r\n\r\n\r\n2020-11-07\r\n\r\n\r\n2021-03-20\r\n\r\n\r\n25\r\n\r\n\r\nŞahap Kavcıoğlu\r\n\r\n\r\n2021-03-20\r\n\r\n\r\nNA\r\n\r\n\r\n7 başkan döneminde USDTRY kur değerleri ve getirileri (\\(u_t = log(\\frac{P_t}{P_{t-1}})\\)) aşağıdaki\r\ngibidir. Investing’ten alınan verilere (post10.xlsx) GitHub\r\nhesabımdan ulaşabilirsiniz.\r\n\r\n\r\n\r\nEWMA hesaplaması için aşağıdaki adımlar izleniyor.\r\nEn güncel tarihten en eski tarihe göre sıralama yapılır. Örneğin,\r\n07.06.2022, 06.06.2022, 03.06.2022, … gibi.\r\nGetiriler hesaplanır. Örneğin, \\(log(\\frac{16.7383}{16.6135}) =\r\n0.007483889\\) getiri 07.06.2022’ye aittir.\r\nGetirilerin karesi alınır. Örneğin, \\(0.007483889^2 = 5.600859e-05\\)\r\ngibi.\r\n\\(\\lambda\\) parametresi\r\nbelirlenir. Ağırlıklandırmada ilk ağırlık \\(1\r\n- \\lambda\\) şeklinde yazılır ve sonrakiler hesaplanan ağırlık ile\r\n\\(\\lambda\\)’nın çarpımı şeklinde\r\nilerletilir. Yani;\r\n\\((1-\\lambda)\\lambda^0,\r\n(1-\\lambda)\\lambda^1, (1-\\lambda)\\lambda^2, ...\\) gibi. \\(w_0\\) başlangıçtaki ağırlık; m gün sayısı\r\nolsun. Şöyle özetleyebiliriz:\r\n\\(w_0\\sum_{i=0}^{\\infty}\\lambda^m\\)\r\nÖrneğin, \\(\\lambda = 0.94\\) olsun.\r\nİlk ağırlık \\((1-0.94)0.94^0 = 0.06\\)\r\nolur. İkinci ağırlık, \\(0.94*0.06 =\r\n0.0564\\) ya da \\((1-0.94)0.94^1 =\r\n0.0564\\) olur ve bu şekilde ilerletilir.\r\nGetiri karesi ile ağırlık çarpılır. Örneğin, \\(5.600859e-05*0.06 = 3.360515e-06\\) gibi. Bu\r\nşekilde elde edilen tüm değerler toplanır ve EWMA varyans değerine\r\nulaşılır.\r\nÖnceki adımda elde edilen EWMA varyansın kare kökü (standart\r\nsapma) volatilite değeridir.\r\nEşit ağırlıklandırılmamış EWMA’dan bağımsız olarak, eşit\r\nağırlıklandırılmış şekilde tarihsel volatilite için getirilerin\r\nkaresinin ortalamasının (varyans) karekökü (standart sapma) alınır.\r\nNedeni ise şudur:\r\nm tane gün; n de varyansını hesaplamak istediğimiz gün olsun.\r\nVaryans;\r\n\\(\\sigma^2_n =\r\n\\frac{1}{m-1}\\sum_{i=1}^{m}(u_{n-i}-\\bar u)^2\\)\r\nStandart sapma (volatilite), \\(\\sqrt{\\sigma^2_n}\\) ya da \\(\\sigma_n\\) olur.\r\nYukarıda yazdığımız \\(\\bar u\\), m\r\ngünün ortalama getirisidir. Aşağıdaki gibi yazılabilir.\r\n\\(\\bar u =\r\n\\frac{1}{m}\\sum_{i=1}^{m}u_{n-i}\\)\r\nBiz ortalama getiriyi sıfır bekliyoruz: \\(\\bar u = 0\\). Bu durumda varyans aşağıdaki\r\ngibi olur.\r\n\\(\\sigma^2_n =\r\n\\frac{1}{m}\\sum_{i=1}^{m}u_{n-i}^2\\)\r\nAynı şekilde, standart sapma (volatilite), \\(\\sqrt{\\sigma^2_n}\\) ya da \\(\\sigma_n\\) olur.\r\nÇalışmada, \\(\\lambda\\) değeri 0.94\r\nalınmıştır.\r\n\r\n\r\n\r\nBaşta da belirttiğim üzere iki tane volatilite hesaplayacağız.\r\nBaşkanın devrettiği volatilitede son görev günlerinin daha çok ağırlığı\r\nolacak. Dönemi boyunca elde ettiği volatilite de görev başlangıç ve\r\nbitiş tarihleri arasındaki volatilite olacak. Mantığını şuna\r\ndayandırdım: EWMA yakın döneme ağırlık verdiği için bu, görevin bitiş\r\ntarihine doğru ağırlıklandırmanın artması demektir. Eşit\r\nağırlıklandırılmışta ise görevi boyunca elde ettiği volatilite elde\r\nedilecek.\r\n\r\n\r\n\r\n\r\n\r\n\r\nGörevini en yüksek volatilite ile devreden Naci Ağbal; en düşük\r\nvolatilite ile devreden Erdem Başçı olmuştur.\r\nGörev süresi boyunca en yüksek volatilite Şahap Kavcıoğlu; en düşük\r\nvolatilite Murat Uysal döneminde olmuştur.\r\nHer iki volatilitenin ortalamasını aldığımızda ise sıralama:\r\n\r\n\r\n\r\nSon olarak, başkanların döneminde elde ettiği volatilite ile kuru\r\naldıkları ve getirdikleri yerin seviyesine (değişim) bakalım.\r\n\r\n\r\n\r\nSon 7 başkan arasında kuru aldığı seviyenin aşağısında bırakan tek\r\nisim Naci Ağbal olmuş. Kur en çok Şahap Kavcıoğlu döneminde artmış.\r\nBu tip performans değerlendirmelerinde TCMB’nin bağımsızlığı önem\r\nkazanmaktadır diye düşünüyorum. Çünkü çalışma isim bazlı performansa\r\nbakıyor ve aslında bağımsızlığı kusursuz varsayıyor. Üzerinizde bir\r\nbaskı varsa (elinizdeki araçları en iyi şekilde kullanıp kararları bu\r\nşekilde alma isteğinizin olduğu birisiniz) kendinizi gösteremezsiniz ki\r\nErdem Başçı’dan sonra (Başçı’da da olmuştu) Merkez’in üzerindeki artan\r\nbaskı ortadadır.\r\nÇalışmanın R kodlarına aşağıdan ulaşılabilir.\r\n\r\n\r\nlibrary(rvest)\r\nlibrary(tidyverse)\r\nlibrary(lubridate)\r\n\r\nurl <- \"https://tr.wikipedia.org/wiki/T%C3%BCrkiye_Cumhuriyet_Merkez_Bankas%C4%B1_ba%C5%9Fkanlar%C4%B1_listesi\"\r\n\r\ncbrt_governor <- read_html(url) %>% \r\n  html_table() %>% \r\n  .[[2]] %>% \r\n  slice(-nrow(.)) %>% \r\n  rename(\"No\"=1,\"Governor\"=2,\"Start\"=3,\"End\"=4) %>% \r\n  mutate(No = as.numeric(No),\r\n         Start = dmy(Start),\r\n         End = dmy(End)) %>% \r\n  filter(No >= 19)\r\n\r\nusdtry <- readxl::read_excel(\"data.xlsx\") %>% \r\n  mutate(\r\n    Date = as.Date(Date),\r\n    Governor = case_when(\r\n      Date <= cbrt_governor$End[1] ~ \"Süreyya Serdengeçti\",\r\n      Date >= cbrt_governor$Start[2] & Date <= cbrt_governor$End[2] ~ \"Durmuş Yılmaz\",\r\n      Date >= cbrt_governor$Start[3] & Date <= cbrt_governor$End[3] ~ \"Erdem Başçı\",\r\n      Date >= cbrt_governor$Start[4] & Date <= cbrt_governor$End[4] ~ \"Murat Çetinkaya\",\r\n      Date >= cbrt_governor$Start[5] & Date <= cbrt_governor$End[5] ~ \"Murat Uysal\",\r\n      Date >= cbrt_governor$Start[6] & Date <= cbrt_governor$End[6] ~ \"Naci Ağbal\",\r\n      Date >= cbrt_governor$Start[7] ~ \"Şahap Kavcıoğlu\"\r\n    ),\r\n    LClose = log(Close),\r\n    Return = lag(log(lead(Close)/Close))\r\n  ) %>% \r\n  na.omit()\r\n\r\nggplot(usdtry, aes(x = Date, y = LClose)) +\r\n  geom_line() +\r\n  theme_minimal() +\r\n  theme(axis.title = element_blank(),\r\n        plot.title = element_text(face = \"bold\"),\r\n        plot.subtitle = element_text(face = \"italic\"),\r\n        plot.caption = element_text(face = \"italic\")) +\r\n  labs(title = \"USDTRY\",\r\n       subtitle = \"14.03.2001-07.06.2022\",\r\n       caption = \"Verilerin logaritması alınmıştır.\") +\r\n  annotate(geom = \"rect\",\r\n           xmin = cbrt_governor$Start[1],\r\n           xmax = cbrt_governor$End[1],\r\n           ymin = -Inf,\r\n           ymax = Inf,\r\n           fill = \"red\",\r\n           alpha = .3) +\r\n  annotate(geom = \"rect\",\r\n           xmin = cbrt_governor$Start[2],\r\n           xmax = cbrt_governor$End[2],\r\n           ymin = -Inf,\r\n           ymax = Inf,\r\n           fill = \"blue\",\r\n           alpha = .3) +\r\n  annotate(geom = \"rect\",\r\n           xmin = cbrt_governor$Start[3],\r\n           xmax = cbrt_governor$End[3],\r\n           ymin = -Inf,\r\n           ymax = Inf,\r\n           fill = \"red\",\r\n           alpha = .3) +\r\n  annotate(geom = \"rect\",\r\n           xmin = cbrt_governor$Start[4],\r\n           xmax = cbrt_governor$End[4],\r\n           ymin = -Inf,\r\n           ymax = Inf,\r\n           fill = \"blue\",\r\n           alpha = .3) +\r\n  annotate(geom = \"rect\",\r\n           xmin = cbrt_governor$Start[5],\r\n           xmax = cbrt_governor$End[5],\r\n           ymin = -Inf,\r\n           ymax = Inf,\r\n           fill = \"red\",\r\n           alpha = .3) +\r\n  annotate(geom = \"rect\",\r\n           xmin = cbrt_governor$Start[6],\r\n           xmax = cbrt_governor$End[6],\r\n           ymin = -Inf,\r\n           ymax = Inf,\r\n           fill = \"blue\",\r\n           alpha = .3) +\r\n  annotate(geom = \"rect\",\r\n           xmin = cbrt_governor$Start[7],\r\n           xmax = as.Date(\"2022-06-07\"),\r\n           ymin = -Inf,\r\n           ymax = Inf,\r\n           fill = \"red\",\r\n           alpha = .3)\r\n\r\nggplot(usdtry, aes(x = Date, y = Return)) +\r\n  geom_line() +\r\n  theme_minimal() +\r\n  theme(axis.title = element_blank(),\r\n        plot.title = element_text(face = \"bold\"),\r\n        plot.subtitle = element_text(face = \"italic\"),\r\n        plot.caption = element_text(face = \"italic\")) +\r\n  labs(title = \"USDTRY\",\r\n       subtitle = \"14.03.2001-07.06.2022\",\r\n       caption = \"Logaritmik getiridir.\") +\r\n  annotate(geom = \"rect\",\r\n           xmin = cbrt_governor$Start[1],\r\n           xmax = cbrt_governor$End[1],\r\n           ymin = -Inf,\r\n           ymax = Inf,\r\n           fill = \"red\",\r\n           alpha = .3) +\r\n  annotate(geom = \"rect\",\r\n           xmin = cbrt_governor$Start[2],\r\n           xmax = cbrt_governor$End[2],\r\n           ymin = -Inf,\r\n           ymax = Inf,\r\n           fill = \"blue\",\r\n           alpha = .3) +\r\n  annotate(geom = \"rect\",\r\n           xmin = cbrt_governor$Start[3],\r\n           xmax = cbrt_governor$End[3],\r\n           ymin = -Inf,\r\n           ymax = Inf,\r\n           fill = \"red\",\r\n           alpha = .3) +\r\n  annotate(geom = \"rect\",\r\n           xmin = cbrt_governor$Start[4],\r\n           xmax = cbrt_governor$End[4],\r\n           ymin = -Inf,\r\n           ymax = Inf,\r\n           fill = \"blue\",\r\n           alpha = .3) +\r\n  annotate(geom = \"rect\",\r\n           xmin = cbrt_governor$Start[5],\r\n           xmax = cbrt_governor$End[5],\r\n           ymin = -Inf,\r\n           ymax = Inf,\r\n           fill = \"red\",\r\n           alpha = .3) +\r\n  annotate(geom = \"rect\",\r\n           xmin = cbrt_governor$Start[6],\r\n           xmax = cbrt_governor$End[6],\r\n           ymin = -Inf,\r\n           ymax = Inf,\r\n           fill = \"blue\",\r\n           alpha = .3) +\r\n  annotate(geom = \"rect\",\r\n           xmin = cbrt_governor$Start[7],\r\n           xmax = as.Date(\"2022-06-07\"),\r\n           ymin = -Inf,\r\n           ymax = Inf,\r\n           fill = \"red\",\r\n           alpha = .3)\r\n\r\ncurr <- usdtry %>% \r\n  select(Date,Governor,Return) %>% \r\n  arrange(desc(Date))\r\n\r\nlambda <- 0.94\r\n\r\ncbrt_governor_vol <- data.frame(\r\n  \"Governor\" = cbrt_governor$Governor,\r\n  \"UnequalWeighted\" = NA,\r\n  \"EqualWeighted\" = NA\r\n)\r\n\r\nfor(i in 1:nrow(cbrt_governor_vol)){\r\n  \r\n  filteredgov <- curr %>% \r\n    filter(Governor == cbrt_governor$Governor[i]) %>% \r\n    slice(-1) %>% \r\n    mutate(\r\n      Return2 = Return^2,\r\n      t = seq(1,nrow(.),1),\r\n      Weight = (1 - lambda) * lambda^(t-1),\r\n      Return2Weight = Return2 * Weight\r\n  ) %>% \r\n    arrange(Date)\r\n  \r\n  unequal_vol <- sqrt(sum(filteredgov$Return2Weight)) * 100\r\n  equal_vol <- sqrt(mean(filteredgov$Return2))\r\n  \r\n  cbrt_governor_vol$UnequalWeighted[i] <- unequal_vol\r\n  cbrt_governor_vol$EqualWeighted[i] <- equal_vol\r\n  \r\n}\r\n\r\nggplot(cbrt_governor_vol, aes(x = UnequalWeighted, y = EqualWeighted)) +\r\n  geom_point(size = 5, alpha = .2) +\r\n  geom_text(aes(label = Governor), vjust = -0.2, hjust = 0.47, size = 4) +\r\n  theme_minimal() +\r\n  theme(plot.title = element_text(face = \"bold\", hjust = 0.5),\r\n        plot.caption = element_text(face = \"italic\", size = 7)) +\r\n  labs(\r\n    title = \"TCMB Başkanlarının Kendi Döneminde Döviz Kuru Volatilitesi\",\r\n    x = \"Devrettiği Volatilite (EWMA)\",\r\n    y = \"Görev Dönemindeki Volatilite (Standart Sapma)\",\r\n    caption = \"Kavcıoğlu'nun görevi devrettiği varsayılmıştır.\"\r\n  ) +\r\n  scale_x_continuous(limits = c(min(cbrt_governor_vol$UnequalWeighted-0.1),\r\n                                max(cbrt_governor_vol$UnequalWeighted+0.1)))\r\n\r\ncbrt_governor_vol <- cbrt_governor_vol %>% \r\n  mutate(AvgVol = (UnequalWeighted + EqualWeighted)/2)\r\n\r\nggplot(cbrt_governor_vol, aes(x = reorder(Governor, AvgVol), y = AvgVol, fill = AvgVol)) +\r\n  geom_col() +\r\n  coord_flip() +\r\n  theme_minimal() +\r\n  theme(legend.position = \"none\",\r\n        axis.title = element_blank(),\r\n        plot.title = element_text(face = \"bold\"),\r\n        plot.caption = element_text(face = \"italic\", size = 7)) +\r\n  scale_fill_gradient(low = \"orange\", high = \"red\") +\r\n  labs(title = \"Ortalama Volatilite*\",\r\n       caption = \"*Devrettiği volatilite ile döneminde elde ettiği volatilite ortalamasıdır.\")\r\n\r\nret_vol <- usdtry %>% \r\n  select(Date,Close,Governor) %>% \r\n  group_by(Governor) %>% \r\n  filter(Date == min(Date) | Date == max(Date)) %>% \r\n  mutate(\r\n    Return = log(lead(Close)/Close)\r\n  ) %>% \r\n  select(Governor,Return) %>% \r\n  na.omit() %>% \r\n  left_join(cbrt_governor_vol, by = \"Governor\")\r\n\r\nggplot(ret_vol, aes(x = Return, y = EqualWeighted)) +\r\n  geom_point(size = 5, alpha = .2) +\r\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\r\n  geom_text(aes(label = Governor), vjust = -0.2, hjust = 0.47, size = 4) +\r\n  theme_minimal() +\r\n  theme(plot.title = element_text(face = \"bold\", hjust = 0.5),\r\n        plot.caption = element_text(face = \"italic\", size = 7)) +\r\n  labs(\r\n    title = \"TCMB Başkanlarının Kendi Döneminde Kur Değişimi ve Volatilite\",\r\n    x = \"Kur Değişimi (Log Değişim)\",\r\n    y = \"Görev Dönemindeki Volatilite (Standart Sapma)\",\r\n    caption = \"Kavcıoğlu'nun görevi devrettiği varsayılmıştır.\"\r\n  ) +\r\n  scale_x_continuous(limits = c(min(ret_vol$Return-0.1),\r\n                                max(ret_vol$Return+0.1)))\r\n\r\n\r\n\r\nYararlandığım Kaynaklar:\r\nFinansal Ekonometri; N.Ç.Yavuz\r\nTürev Piyasalar ve Yapılandırılmış Ürünler; M.B.Akçay, M.Kasap,\r\nT.Doğuç, G.Kasap\r\nVolatility:\r\nExponentially weighted moving average, EWMA (FRM T2-22)\r\nMeasuring\r\nand Monitoring Volatility\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-06-08-post10/post10_files/figure-html5/unnamed-chunk-7-1.png",
    "last_modified": "2022-06-08T19:20:12+03:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-06-05-post9/",
    "title": "Bir Sürecin Otomatize Edilmesi: Temel Gıda Fiyatlarının Takibi",
    "description": "Fiyat algısının kaybolduğu bir ortamda temel gıda fiyatlarının takibini otomatikleştirmek.",
    "author": [
      {
        "name": "A. Uraz Akgül",
        "url": {}
      }
    ],
    "date": "2022-06-05",
    "categories": [
      "Automation"
    ],
    "contents": "\r\nMayıs 2022 verilerine göre Türkiye’de enflasyon TÜİK’e göre 73.5%;\r\nENAG’a göre ise 160.76% oldu. Trading\r\nEconomics verilerine göre dünyada 6., Avrupa’da 1.\r\nsıradayız.\r\nR’da bir süreci otomatize etmeyi eğlenceli bir konu ile anlatmak\r\nisterdim ancak içinde bulunduğumuz ortam maalesef buna izin\r\nvermiyor.\r\nBu uygulamada, Cimri’den temel gıda kategorisini baz alarak aşağıdaki\r\nişlemleri yapacağız.\r\nWeb kazıma ile istenen kategoride verilerin çekilmesi (web kazıma\r\nile ilgili temel bilginizin olduğunu varsayacağım).\r\nÇekilen verilerin veri tabanına kaydedilmesi.\r\nGüncel veriler ile bir önceki zamana ait verilerin\r\nkarşılaştırılması ve karşılaştırmanın kaydedilmesi.\r\nTüm bu sürecin otomatik olarak yapılması için görev\r\nzamanlayıcının ayarlanması ve script’in belli bir frekansta\r\nçalıştırılması.\r\nCimri.com, farklı çevrimiçi alışveriş sitelerinde yer alan\r\nürünleri listeleyen ve aralarında karşılaştırmalar yapan İstanbul,\r\nTürkiye merkezli web sitesidir. -Vikipedi\r\n\r\n\r\nlibrary(rvest) # web kazıma\r\nlibrary(DBI); library(RSQLite) # sqlite\r\nlibrary(taskscheduleR) # görev zamanlayıcı\r\nlibrary(openxlsx) # excel olarak kaydetmek için\r\nlibrary(tidyverse)\r\nlibrary(lubridate)\r\nlibrary(kableExtra) # zorunlu değil\r\n\r\n\r\n\r\nWeb kazıma ile istenen kategoride verilerin\r\nçekilmesi\r\nAşağıdaki link’i kullanarak verileri çekeceğiz.\r\n\r\n\r\nurl <- \"https://www.cimri.com/market/temel-gida\"\r\n\r\n\r\n\r\nTemel gıda kategorisi 50 sayfadan oluşuyor. Eğer link’in yanına\r\n?page=1 yazarsak bu ilk sayfa olduğu anlamına gelecek.\r\nYani;\r\n\r\n\r\nurl <- \"https://www.cimri.com/market/temel-gida?page=1\"\r\n\r\n\r\n\r\nİlk sayfada yer alan ürünleri ve fiyatları çekelim.\r\n\r\n\r\n# ürün\r\nitem <- read_html(url) %>% \r\n  html_nodes(\"div.ProductCard_productName__35zi5\") %>% \r\n  html_text()\r\n\r\n# fiyat\r\nprice <- read_html(url) %>% \r\n  html_nodes(\"div.ProductCard_footer__Fc9OL span.ProductCard_price__10UHp\") %>% \r\n  html_text()\r\n\r\ndf <- data.frame(\r\n  Item = item,\r\n  Price = price\r\n)\r\n\r\n\r\n\r\nİlk sayfada yer alan 32 ürünün ilk 10’u aşağıdadır.\r\n\r\n\r\nItem\r\n\r\n\r\nPrice\r\n\r\n\r\nBiryağ 5 lt Ayçiçek Yağı\r\n\r\n\r\n175,00 TL\r\n\r\n\r\nKomili Riviera Sıcak Lezzetler 5 lt Zeytinyağı\r\n\r\n\r\n328,06 TL\r\n\r\n\r\nYudum 5 lt Teneke Ayçiçek Yağı\r\n\r\n\r\n192,50 TL\r\n\r\n\r\nBizim 5 lt Teneke Ayçiçek Yağı\r\n\r\n\r\n189,90 TL\r\n\r\n\r\nVera 5 lt Pet Ayçiçek Yağı\r\n\r\n\r\n145,00 TL\r\n\r\n\r\nYudum 1 lt Ayçiçek Yağı\r\n\r\n\r\n42,90 TL\r\n\r\n\r\nYudum 18 lt Ayçiçek Yağı\r\n\r\n\r\n739,95 TL\r\n\r\n\r\nSinangil 5 kg Un\r\n\r\n\r\n52,90 TL\r\n\r\n\r\nTorku Altın Ekin 5 lt Ayçiçek Yağı\r\n\r\n\r\n175,00 TL\r\n\r\n\r\nOna 5 lt Ayçiçek Teneke Yağı\r\n\r\n\r\n164,80 TL\r\n\r\n\r\nİlk sayfayı çektiğimize göre artık diğer sayfaları da bir döngü ile\r\nçekebiliriz (total sayfa sayısı: 50). Son sayfa sayısını belirlemeyi\r\ndinamik hale getirebiliriz.\r\n\r\n\r\n# son sayfa\r\nlastPage <- read_html(url) %>% \r\n  html_nodes(\"div.Pagination_pagination__6kvLO li\") %>% \r\n  html_text()\r\n\r\nlastPage <- as.numeric(tail(lastPage[lastPage != \"\"], 1))\r\n\r\nurls <- str_c(\r\n  \"https://www.cimri.com/market/temel-gida?page=\",\r\n  seq(1,lastPage,1)\r\n) # tüm linkler\r\n\r\n\r\n\r\nDöngüye geçebiliriz.\r\n\r\n\r\n######### global #########\r\n\r\nmaster <- data.frame()\r\ntime <- format(Sys.time(), \"%Y-%m-%d %H:%M:%S\")\r\n\r\n##########################\r\n\r\nfor(i in 1:2){ # 1:2 yazdım ancak siz 1:lastPage yazabilirsiniz\r\n  \r\n  thepage <- read_html(urls[i]) # sayfa okundu\r\n  \r\n  # ürün\r\n  item <- thepage %>% \r\n    html_nodes(\"div.ProductCard_productName__35zi5\") %>% \r\n    html_text()\r\n  \r\n  # fiyat\r\n  price <- thepage %>% \r\n    html_nodes(\"div.ProductCard_footer__Fc9OL span.ProductCard_price__10UHp\") %>% \r\n    html_text()\r\n  \r\n  # ürün ve fiyatın birleştirilmesi\r\n  tbl <- data.frame(\r\n    Item = item,\r\n    Price = price\r\n  )\r\n  \r\n  # birleştirilen tablonun globalde yaratılan master ile birleştirilmesi\r\n  master <- master %>% \r\n    bind_rows(tbl)\r\n  \r\n  # sürekli istek göndermemek için sistem bir süre (ör: 3 saniye) uyutulabilir\r\n  Sys.sleep(time = 3)\r\n  \r\n  if(i == 2){ # 2 yerine lastPage yazabilirsiniz\r\n    \r\n    # eğer i değişkeni son sayfaya eşitse ki döngü bitecek;\r\n    # globalde yaratılan time değişkeni master veri çerçevesine eklenecek.\r\n    # time değişkeni zamanın sabit olması için globalde yaratıldı.\r\n    \r\n    master$Time <- time\r\n    \r\n  }\r\n  \r\n}\r\n\r\n\r\n\r\nVarsayımsal olarak 50; şimdilik 2 sayfanın tamamını çektik. Son 10\r\nürüne ve fiyatına bakalım.\r\n\r\n\r\n\r\n\r\nItem\r\n\r\n\r\nPrice\r\n\r\n\r\nTime\r\n\r\n\r\n55\r\n\r\n\r\nDuru 1 kg Karabuğday Greçka\r\n\r\n\r\n67,80 TL\r\n\r\n\r\n2022-06-05 18:40:24\r\n\r\n\r\n56\r\n\r\n\r\nNestle 1927 2.5 kg Kuvertür Sütlü Çikolata\r\n\r\n\r\n215,13 TL\r\n\r\n\r\n2022-06-05 18:40:24\r\n\r\n\r\n57\r\n\r\n\r\nSinangil 8 kg Un\r\n\r\n\r\n148,47 TL\r\n\r\n\r\n2022-06-05 18:40:24\r\n\r\n\r\n58\r\n\r\n\r\nÖzgün 5 lt Naturel Sızma Zeytinyağı\r\n\r\n\r\n306,90 TL\r\n\r\n\r\n2022-06-05 18:40:24\r\n\r\n\r\n59\r\n\r\n\r\nPakmaya 100 gr Aktif Kuru Hamur Mayası\r\n\r\n\r\n6,49 TL\r\n\r\n\r\n2022-06-05 18:40:24\r\n\r\n\r\n60\r\n\r\n\r\nYudum 1 lt Egemden Riviera Zeytinyağı\r\n\r\n\r\n59,39 TL\r\n\r\n\r\n2022-06-05 18:40:24\r\n\r\n\r\n61\r\n\r\n\r\nTmo Teneke Ayçiçek Yağı 5 lt\r\n\r\n\r\n198,00 TL\r\n\r\n\r\n2022-06-05 18:40:24\r\n\r\n\r\n62\r\n\r\n\r\nAmasya Un 50 kg Çeşitlik Unu\r\n\r\n\r\n639,00 TL\r\n\r\n\r\n2022-06-05 18:40:24\r\n\r\n\r\n63\r\n\r\n\r\nİndomie 80 Gr Gurme Acı Ve Baharatlı Hazır Noodle\r\n\r\n\r\n4,13 TL\r\n\r\n\r\n2022-06-05 18:40:24\r\n\r\n\r\n64\r\n\r\n\r\nOlin 18 lt Ayçiçek Yağı\r\n\r\n\r\n673,41 TL\r\n\r\n\r\n2022-06-05 18:40:24\r\n\r\n\r\nÇekilen verilerin veri tabanına kaydedilmesi\r\nÇektiğimiz verileri veri tabanına kaydetme vakti. Bunun için SQLite’ı\r\nkullanacağız. Bu uygulama için tercih etme nedenim, herhangi bir\r\nyazılım/sunucu kurulumuna ihtiyacımız olmayacak. Bunun yanında sade ve\r\nbasit bir yapısı vardır.\r\nSQLite için DBI ve RSQLite paketini; veri tabanını\r\n(geçici değil; sürekli) yaratmak için aşağıdaki komutu kullanacağız.\r\nDosya yolu neredeyse oraya kaydedecektir.\r\n\r\n\r\n# dosya yolu\r\n# getwd()\r\n\r\nmyDB <- dbConnect(SQLite(), \"marketDB.sqlite\") # Dosya yolunu adres göstermenizi tavsiye ederim.\r\n# Ör: \"C:/.../marketDB.sqlite\"\r\n\r\n\r\n\r\nVerileri veri tabanına kaydedelim. Bu işlemi belli bir frekansta ya\r\nda uygulamamızda olduğu gibi saat başı yapacağımız zaman yeni gelen\r\nverilerin veri tabanındaki tablonun üzerine yazmaması önemli olacak.\r\nBunun için append parametresi TRUE olarak\r\nbelirlendi.\r\n\r\n\r\ndbWriteTable(myDB, \"master\", master, append = TRUE)\r\n\r\n\r\n\r\nVeri tabanına kaydettiğimiz verileri SQL sorgusu ile çekelim.\r\n\r\n\r\nmastertbl <- dbGetQuery(myDB, \"SELECT * FROM master\")\r\n\r\n\r\n\r\n\r\n\r\nItem\r\n\r\n\r\nPrice\r\n\r\n\r\nTime\r\n\r\n\r\nBiryağ 5 lt Ayçiçek Yağı\r\n\r\n\r\n175,00 TL\r\n\r\n\r\n2022-06-05 18:24:16\r\n\r\n\r\nKomili Riviera Sıcak Lezzetler 5 lt Zeytinyağı\r\n\r\n\r\n328,06 TL\r\n\r\n\r\n2022-06-05 18:24:16\r\n\r\n\r\nYudum 5 lt Teneke Ayçiçek Yağı\r\n\r\n\r\n192,50 TL\r\n\r\n\r\n2022-06-05 18:24:16\r\n\r\n\r\nBizim 5 lt Teneke Ayçiçek Yağı\r\n\r\n\r\n189,90 TL\r\n\r\n\r\n2022-06-05 18:24:16\r\n\r\n\r\nVera 5 lt Pet Ayçiçek Yağı\r\n\r\n\r\n145,00 TL\r\n\r\n\r\n2022-06-05 18:24:16\r\n\r\n\r\nYudum 1 lt Ayçiçek Yağı\r\n\r\n\r\n42,90 TL\r\n\r\n\r\n2022-06-05 18:24:16\r\n\r\n\r\nYudum 18 lt Ayçiçek Yağı\r\n\r\n\r\n739,95 TL\r\n\r\n\r\n2022-06-05 18:24:16\r\n\r\n\r\nSinangil 5 kg Un\r\n\r\n\r\n52,90 TL\r\n\r\n\r\n2022-06-05 18:24:16\r\n\r\n\r\nTorku Altın Ekin 5 lt Ayçiçek Yağı\r\n\r\n\r\n175,00 TL\r\n\r\n\r\n2022-06-05 18:24:16\r\n\r\n\r\nOna 5 lt Ayçiçek Teneke Yağı\r\n\r\n\r\n164,80 TL\r\n\r\n\r\n2022-06-05 18:24:16\r\n\r\n\r\nİşlemler bittikten sonra aşağıdaki kod ile veri tabanından\r\nçıkılabilir. Başta yazılan kod (aşağıdaki yorum satırı) ile tekrar\r\nbağlanacaktır.\r\n\r\n\r\ndbDisconnect(myDB)\r\n# myDB <- dbConnect(SQLite(), \"marketDB.sqlite\")\r\n\r\n\r\n\r\nGüncel veriler ile bir önceki zamana ait verilerin\r\nkarşılaştırılması ve karşılaştırmanın kaydedilmesi\r\nVeri tabanından alınan tablonun time sütunu character\r\nformatında olacağı için aşağıdaki gibi zaman formatına\r\ndönüştürülmelidir.\r\n\r\n\r\nmastertbl$Time <- as.POSIXct(mastertbl$Time)\r\n\r\n\r\n\r\nBizim bu tabloda iki zamana ihtiyacımız olacak: En güncel zaman ve\r\nbir önceki.\r\n\r\n\r\n# En güncel zaman\r\nmaxTime <- mastertbl[ymd_hms(mastertbl$Time)==max(ymd_hms(mastertbl$Time)),]\r\n# Bir önceki zaman\r\nmaxTime2 <- mastertbl[ymd_hms(mastertbl$Time)!=max(ymd_hms(mastertbl$Time)),]\r\nmaxTime2 <- maxTime2[ymd_hms(maxTime2$Time)==max(ymd_hms(maxTime2$Time)),]\r\n\r\n\r\n\r\nİstediğimiz verileri alıp birleştirdikten sonra bazı düzenlemeler\r\nyapacağız. Ama öncesinde şunu not düşmekte fayda var: Bu süreci ilk defa\r\nbaşlattığımız zaman elimizde bir tane zaman olacak. Yani, herhangi bir\r\nkarşılaştırma olmayacak. Hata almamak ve süreci devam ettirebilmek için\r\nbazı koşullara ihtiyaç olacaktır.\r\nAşağıdaki kodu çalıştırmadan önce maxTime2 değişkeninin\r\nsıfırdan büyük olup olmadığına bakmamız gerekir. Bakmaz isek hata\r\nalırız.\r\n\r\n\r\nif(nrow(maxTime2) > 0){\r\n  \r\n  comparetbl <- maxTime %>% \r\n  bind_rows(maxTime2) %>% \r\n  group_by(Time) %>% \r\n  mutate(ID = cur_group_id(),\r\n         ID  = if_else(ID == 1, \"Before\", \"After\")) %>% \r\n  pivot_wider(!Time, names_from = \"ID\", values_from = \"Price\") %>% \r\n  mutate(\r\n    # Virgülün nokta yapılması ve TL'nin kaldırılması; numeric'e dönüştürülmesi\r\n    After = as.numeric(gsub(\"\\\\,\",\".\",gsub(\" TL\",\"\",After))),\r\n    Before = as.numeric(gsub(\"\\\\,\",\".\",gsub(\" TL\",\"\",Before))),\r\n    # Öncesi ve sonrası arasındaki fiyat farkı\r\n    Diff = After - Before\r\n  ) %>% \r\n  filter(Diff != 0) # farkı sıfır olmayanlar filtrelendi\r\n  \r\n}\r\n\r\n\r\n\r\nEğer son filtreden sonra veri çerçevesi 0’dan büyük ise bir excel\r\ndosyası ile kaydedeceğiz. Öncesinde ise bir koşul koymak gerekiyor. Eğer\r\ncomparetbl tablosu var ise sıfırdan büyük olup olmadığı\r\nkoşuluna geçmelidir.\r\n\r\n\r\nif(exists(\"comparetbl\")){\r\n  \r\n  if(nrow(comparetbl) > 0){\r\n    \r\n    write.xlsx(comparetbl, \"PriceTracker.xlsx\")\r\n    \r\n  }\r\n  \r\n}\r\n\r\n\r\n\r\nGörev zamanlayıcının ayarlanması ve script’in belli bir\r\nfrekansta çalıştırılması\r\nGörev zamanlayıcı PC’den ayarlanabileceği gibi taskscheduleR\r\npaketi ile de yapılabilir.\r\n\r\n\r\ntaskscheduler_create(\r\n  \r\n  taskname = \"OrnekScript\", # görev adı\r\n  rscript = \"OrnekScript.R\", # çalıştırılacak olan script;\r\n  # dosya yolu belirtilmeli (\"C:/.../OrnekScript.R\")\r\n  schedule = \"HOURLY\", # saatlik\r\n  starttime = \"18:00\", # manuel belirlenen saat ile başlayacak; Ör: 18:00\r\n  modifier = 1 # 1 saat ara ile\r\n  \r\n)\r\n\r\n\r\n\r\nAyarlar kod ile yapılabileceği gibi Addins ile de yapılabilir. R\r\nStudio’da Addins’e tıkladıktan sonra Schedule R Scripts on\r\nWindows’a tıklanır. Açılan ekran ile ayarlar daha kolay bir şekilde\r\nyapılabilir.\r\nGörev zamanlayıcı çalıştıktan sonra taskname’e verdiğiniz\r\nisim ile birlikte bir log dosyası atabilir. Burada olası uyarı ve\r\nhataları görebilirsiniz.\r\nSürecin otomatize edilmesi bitti. Ürünlere ait fiyatlar veri\r\ntabanında kayıtlı ve 1 adet de karşılaştırma dosyası bulunmaktadır. Buna\r\nbir de görsel ekleyebilirsiniz. Bu kısmı size bırakıyorum :)\r\nBONUS\r\nİş hayatında Outlook kullanıldığı ve bu tip süreçleri birçok konuda\r\nentegre ettiğim için Outlook ile nasıl mail atılır bunu göstermek\r\nistiyorum.\r\n\r\n\r\nlibrary(RDCOMClient) # outlook mail\r\nlibrary(xtable) # html ile tablo oluşturmak için\r\n\r\nif(nrow(comparetbl) > 0){\r\n  \r\n  x <- head(comparetbl)\r\n  y <- print(xtable(x), type=\"html\", print.results=FALSE)\r\n  body <- paste0(\"<html>\", y, \"<\/html>\")\r\n  \r\n  OutApp <- COMCreate(\"Outlook.Application\")\r\n  outMail = OutApp$CreateItem(0)\r\n  outMail[[\"To\"]] = \"mailingidecegiadres@sirket.com\"\r\n  outMail[[\"subject\"]] = paste0(nrow(comparetbl),\" Adet Ürünün Fiyatında Değişiklik Var!\")\r\n  outMail[[\"Attachments\"]]$Add(paste0(\"PriceTracker.xlsx\"))\r\n  outMail[[\"HTMLbody\"]] = body\r\n  outMail$Send()\r\n}\r\n\r\n\r\n\r\nGitHub\r\nhesabımdan örnek bir script’e ulaşabilirsiniz.\r\nOrnekScript.R:\r\n\r\n\r\nlibrary(rvest)\r\nlibrary(DBI); library(RSQLite)\r\nlibrary(openxlsx)\r\nlibrary(tidyverse)\r\nlibrary(lubridate)\r\n\r\nurl <- \"https://www.cimri.com/market/temel-gida?page=1\"\r\n\r\nlastPage <- read_html(url) %>% \r\n  html_nodes(\"div.Pagination_pagination__6kvLO li\") %>% \r\n  html_text()\r\nlastPage <- as.numeric(tail(lastPage[lastPage != \"\"], 1))\r\n\r\nurls <- str_c(\r\n  \"https://www.cimri.com/market/temel-gida?page=\",\r\n  seq(1,lastPage,1)\r\n)\r\n\r\nmaster <- data.frame()\r\ntime <- format(Sys.time(), \"%Y-%m-%d %H:%M:%S\")\r\n\r\nfor(i in 1:2){\r\n  \r\n  thepage <- read_html(urls[i])\r\n  \r\n  item <- thepage %>% \r\n    html_nodes(\"div.ProductCard_productName__35zi5\") %>% \r\n    html_text()\r\n  \r\n  price <- thepage %>% \r\n    html_nodes(\"div.ProductCard_footer__Fc9OL span.ProductCard_price__10UHp\") %>% \r\n    html_text()\r\n  \r\n  tbl <- data.frame(\r\n    Item = item,\r\n    Price = price\r\n  )\r\n  \r\n  master <- master %>% \r\n    bind_rows(tbl)\r\n  \r\n  Sys.sleep(time = 3)\r\n  \r\n  if(i == 2){\r\n    \r\n    master$Time <- time\r\n    \r\n  }\r\n  \r\n}\r\n\r\nmyDB <- dbConnect(SQLite(), \"marketDB.sqlite\")\r\ndbWriteTable(myDB, \"master\", master, append = TRUE)\r\nmastertbl <- dbGetQuery(myDB, \"SELECT * FROM master\")\r\ndbDisconnect(myDB)\r\n\r\nmastertbl$Time <- as.POSIXct(mastertbl$Time)\r\nmaxTime <- mastertbl[ymd_hms(mastertbl$Time)==max(ymd_hms(mastertbl$Time)),]\r\nmaxTime2 <- mastertbl[ymd_hms(mastertbl$Time)!=max(ymd_hms(mastertbl$Time)),]\r\nmaxTime2 <- maxTime2[ymd_hms(maxTime2$Time)==max(ymd_hms(maxTime2$Time)),]\r\n\r\nif(nrow(maxTime2) > 0){\r\n  \r\n  comparetbl <- maxTime %>% \r\n    bind_rows(maxTime2) %>% \r\n    group_by(Time) %>% \r\n    mutate(ID = cur_group_id(),\r\n           ID  = if_else(ID == 1, \"Before\", \"After\")) %>% \r\n    pivot_wider(!Time, names_from = \"ID\", values_from = \"Price\") %>% \r\n    mutate(\r\n      After = as.numeric(gsub(\"\\\\,\",\".\",gsub(\" TL\",\"\",After))),\r\n      Before = as.numeric(gsub(\"\\\\,\",\".\",gsub(\" TL\",\"\",Before))),\r\n      Diff = After - Before\r\n    ) %>% \r\n    filter(Diff != 0)\r\n  \r\n}\r\n\r\nif(exists(\"comparetbl\")){\r\n  \r\n  if(nrow(comparetbl) > 0){\r\n    \r\n    write.xlsx(comparetbl, \"PriceTracker.xlsx\")\r\n    \r\n  }\r\n  \r\n}\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-06-05-post9/img1.png",
    "last_modified": "2022-06-05T18:40:48+03:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-06-01-post8/",
    "title": "Doğrusal Zaman Serisi Modelleri ile Enflasyon Öngörüsü",
    "description": "2022 için yıl sonu enflasyon öngörüsü.",
    "author": [
      {
        "name": "A. Uraz Akgül",
        "url": {}
      }
    ],
    "date": "2022-06-01",
    "categories": [
      "Economics"
    ],
    "contents": "\r\nBu çalışmada ele alacağımız zaman serisi modelleri tek değişkenlidir\r\nve kendi geçmiş değerleri ve hatalarına (kalıntılar) göre kurulan\r\nmodellerdir.\r\n\\(Y_t\\) gibi bir serimiz olsun. Bu\r\ndurumda sözel olarak ifade edilen şey aşağıdaki gibi olur:\r\n\\(Y_t = f(Y_{t-1},Y_{t-2},\r\n...,\\epsilon_t,\\epsilon_{t-1},\\epsilon_{t-2},...)\\)\r\nOtoregresif Süreç: AR(p)\r\nAR modeller, bir zaman serisinin herhangi bir dönemdeki gözlem\r\ndeğerini, aynı serinin ondan önceki belirli sayıda dönemin gözlem\r\ndeğerinin ve hata teriminin doğrusal bir bileşimi olarak ifade eden\r\nmodellerdir.\r\nAR(p) süreci şöyle ifade edilir:\r\n\\(Y_t = \\delta + \\phi_1Y_{t-1} +\r\n\\phi_2Y_{t-2} + ... + \\phi_pY_{t-p} + \\epsilon_t\\)\r\np = 1 olduğunu varsayalım. Bu, AR(1) sürecidir. Buna aynı zamanda\r\nbirinci derece otoregresif zaman serisi modeli de diyebiliriz. Çünkü,\r\n\\(Y_t\\) sadece kendi bir önceki dönemi\r\nolan \\(Y_{t-1}\\)’e ve bir rassal\r\nkalıntı olan \\(\\epsilon_t\\)’ye\r\nbağlıdır.\r\n\\(Y_t = \\delta + \\phi_1Y_{t-1} +\r\n\\epsilon_t; t = 1,2,...,T\\)\r\n\\(\\delta:\\) Sabit parametre (\\(Y_t\\)’nin ortalaması),\r\n\\(\\phi_1:\\) -1 ile 1 arasında değer\r\naldığı varsayılan bilinmeyen parametre,\r\n\\(\\epsilon_t:\\) Ortalaması sıfır ve\r\nsabit varyanslı \\(\\sigma_{\\epsilon}^2\\)\r\nkorelasyonsuz bir hata terimidir: \\(\\epsilon_t\r\n\\sim IID (0,\\sigma^2)\\)\r\nAR(1) süreci için simülasyon ile alternatif yapılar oluşturalım.\r\n\\(\\phi_1 = 0\\) için \\(Y_t = \\epsilon_t\\) olur.\r\n\r\n\r\n\r\n\\(\\phi_1 = 0.3\\) için \\(Y_t = 0.3Y_{t-1} + \\epsilon_t\\) olur.\r\n\r\n\r\n\r\n\\(\\phi_1 = 0.5\\) için \\(Y_t = 0.5Y_{t-1} + \\epsilon_t\\) olur.\r\n\r\n\r\n\r\n\\(\\phi_1 = 0.7\\) için \\(Y_t = 0.7Y_{t-1} + \\epsilon_t\\) olur.\r\n\r\n\r\n\r\n\\(\\phi_1 = 0.9\\) için \\(Y_t = 0.9Y_{t-1} + \\epsilon_t\\) olur.\r\n\r\n\r\n\r\n\r\n\r\n\r\nYukarıda, ilk simüle ettiğimiz \\(\\phi_{1} =\r\n0\\)’ın çok sık bir biçimde ortalamayı kestiğini ve \\(\\phi_1\\)’in değeri arttıkça ortalamayı daha\r\naz kestiğini görüyoruz. \\(\\phi_1\\) için\r\n-1 ile 1 arasında değer aldığı varsayılan bilinmeyen parametre demiştik.\r\nYani, -1 < \\(\\phi_1\\) < 1 ya da\r\n\\(|\\phi_1| < 1\\) şeklinde de\r\nyazabiliriz. Eğer \\(\\phi_1 = 1\\) olursa\r\nki bu grafikte yok; sürecin durağan olmadığı yorumunu yapabiliriz.\r\nHareketli Ortalama Süreci: MA(q)\r\nMA modeller, bir zaman serisinin herhangi bir dönemindeki gözlem\r\ndeğerinin, aynı döneminin hata terimi ve belirli sayıda geçmiş dönemin\r\nhata terimlerinin doğrusal bir bileşimi olarak ifade edildiği\r\nmodellerdir.\r\nMA(q) süreci şöyle ifade edilir:\r\n\\(Y_t = \\mu + \\epsilon_t +\r\n\\theta_1\\epsilon_{t-1} + \\theta_2\\epsilon_{t-2} + ... +\r\n\\theta_q\\epsilon_{t-q}\\)\r\nq = 1 olduğunu varsayalım. Bu, MA(1) sürecidir. Buna aynı zamanda\r\nbirinci derece hareketli ortalama süreci de diyebiliriz. MA(1) sürecini\r\nşöyle ifade edebiliriz:\r\n\\(Y_t = \\mu + \\epsilon_t +\r\n\\theta_1\\epsilon_{t-1}\\)\r\nFinans üzerinden örneklendirelim. t günündeki hisse senedinin fiyatı\r\n\\(P_t\\) olsun. Bu durumda bir günden\r\ndiğerine fiyattaki değişmeyi şöyle ifade edebiliriz: \\(Y_t = P_t - P_{t-1} = \\epsilon_t; t =\r\n1,2,...,T\\)\r\nBeklenmeyen haberlerin olduğu bir ortamda etkisi bir gün içinde\r\ngeçmeyebilir. Yani, fiyat değişmeleri sonraki günlerde de etkilenebilir.\r\nÖrneğin, \\(Y_{t+1} = \\epsilon_{t+1} +\r\n\\theta_1\\epsilon_t\\) olabilir. Burada, \\(\\epsilon_{t+1}\\), t+1 günündeki alınan yeni\r\nhaberlerin/bilgilerin etkisini gösterir. \\(\\theta_1\\epsilon_t\\) ise bir gün öncesinin\r\nhaber etkisinin devamıdır. Şunu diyebiliriz: \\(Y_{t+1}\\)’in değeri cari ve geçmiş rassal\r\nbir kalıntının ağırlıklı ortalamasıdır.\r\nMA sürecinde \\(Y_t\\)’nin k sayıda\r\ngecikmesi ile olan kovaryansı sıfırdır. Yani, \\(Cov(Y_t,Y_{t-k}) = \\gamma_k = 0\\)’dır.\r\nÖrneğin, MA(1) sürecinde k > 1 olduğu bütün durumlarda kovaryanslar\r\nsıfırdır. Bu da MA(1) sürecinin yalnızca bir dönemlik bir belleğe sahip\r\nolduğunu ifade eder. Yani, \\(Y_t\\),\r\nsadece \\(Y_{t-1}\\) ve \\(Y_{t+1}\\) ile korelasyonludur, diğer\r\ngecikmelerle arasında herhangi bir korelasyon yoktur.\r\nMA(1) süreci için simülasyon ile alternatif yapılar oluşturalım.\r\n\\(\\theta_1 = 0\\) için \\(Y_t = \\epsilon_t\\) olur.\r\n\r\n\r\n\r\n\\(\\theta_1 = -0.3\\) için \\(Y_t = \\epsilon_t - 0.3\\epsilon_{t-1}\\)\r\nolur.\r\n\r\n\r\n\r\n\\(\\theta_1 = -0.5\\) için \\(Y_t = \\epsilon_t - 0.5\\epsilon_{t-1}\\)\r\nolur.\r\n\r\n\r\n\r\n\\(\\theta_1 = -0.7\\) için \\(Y_t = \\epsilon_t - 0.7\\epsilon_{t-1}\\)\r\nolur.\r\n\r\n\r\n\r\n\\(\\theta_1 = -0.9\\) için \\(Y_t = \\epsilon_t - 0.9\\epsilon_{t-1}\\)\r\nolur.\r\n\r\n\r\n\r\n\r\n\r\n\r\nAR(1) sürecindeki simülasyondan farklı olarak MA(1) sürecinde \\(\\phi_1\\) değerleri değiştikçe ortalamayı\r\nkesme sıklığında bir değişiklik görülmemektedir.\r\nOtoregresif Hareketli Ortalama Süreci: ARMA(p,q)\r\nBazen bir zaman serisi hem AR hem de MA sürecini içerebilir. Bu\r\nmodellerde bir zaman serisinin herhangi bir dönemine ait gözlem değeri,\r\nondan önceki belirli sayıda gözlem değerinin ve hata teriminin doğrusal\r\nbir bileşimi olarak ifade edilir.\r\n\\(Y_t = \\delta + \\phi_1Y_{t-1} +\r\n\\phi_2Y_{t-2} + ... + \\phi_pY_{t-p} + \\epsilon_t + \\mu + \\epsilon_t +\r\n\\theta_1\\epsilon_{t-1} + \\theta_2\\epsilon_{t-2} + ... +\r\n\\theta_q\\epsilon_{t-q}\\)\r\nKesme terimi \\(\\delta\\), \\(Y_t\\)’nin ortalamasıdır. \\(\\epsilon_t\\) hatalar, \\(E(\\epsilon_t) = 0\\) ve \\(Var(\\epsilon_t) = \\sigma_\\epsilon^2\\) ile\r\nkorelasyonsuz rassal değişkenlerdir. Süreç durağan ise ki bunun koşulu\r\naşağıdadır, tüm dönemler için sabir bir ortalama, \\(\\mu\\), söz konusudur.\r\nDurağanlık koşulu: \\(\\phi_1 + \\phi_2 + ...\r\n+ \\phi_p < 1\\)’dir.\r\np = 1 ve q = 1 olsun. Bu bir ARMA(1,1) sürecidir.\r\n\\(Y_t = \\delta + \\phi_1Y_{t-1} + \\epsilon_t\r\n+ \\theta_1\\epsilon_{t-1}\\)\r\nARMA(1,1) süreci için simülasyon ile alternatif yapılar\r\noluşturalım.\r\n\\(\\phi_1 = 0.3\\) ve \\(\\theta_1 = -0.3\\) için \\(Y_t = 0.3Y_{t-1} + \\epsilon_t -\r\n0.3\\epsilon_{t-1}\\) olur.\r\n\r\n\r\n\r\n\\(\\phi_1 = 0.8\\) ve \\(\\theta_1 = -0.3\\) için \\(Y_t = 0.8Y_{t-1} + \\epsilon_t -\r\n0.3\\epsilon_{t-1}\\) olur.\r\n\r\n\r\n\r\n\\(\\phi_1 = 0.3\\) ve \\(\\theta_1 = -0.8\\) için \\(Y_t = 0.3Y_{t-1} + \\epsilon_t -\r\n0.8\\epsilon_{t-1}\\) olur.\r\n\r\n\r\n\r\n\\(\\phi_1 = -0.8\\) ve \\(\\theta_1 = -0.8\\) için \\(Y_t = -0.8Y_{t-1} + \\epsilon_t -\r\n0.8\\epsilon_{t-1}\\) olur.\r\n\r\n\r\n\r\n\\(\\phi_1 = 0.8\\) ve \\(\\theta_1 = 0.8\\) için \\(Y_t = 0.8Y_{t-1} + \\epsilon_t +\r\n0.8\\epsilon_{t-1}\\) olur.\r\n\r\n\r\n\r\n\\(\\phi_1 = -0.9\\) ve \\(\\theta_1 = 0.8\\) için \\(Y_t = -0.9Y_{t-1} + \\epsilon_t +\r\n0.8\\epsilon_{t-1}\\) olur.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\\(\\theta_1\\) parametresinin değil de\r\n\\(\\phi_1\\) parametresinin ki bu\r\notoregresif parametredir, etkili olduğunu görüyoruz. Çünkü, bu\r\nparametrenin değeri arttıkça ortalama daha az sayıda kesiliyor.\r\nHomojen Durağan-Dışı Süreçler: ARIMA(p,d,q)\r\nBu başlığa kadar süreçlerin hep durağan olduğunu varsaydık. Zaman\r\nserisi durağan-dışı bir yapıda ise bir ya da birden fazla farkını alarak\r\ndönüştürme işlemi gerçekleştiriyoruz. Bu bir entegre süreçtir ve d\r\nharfine karşılık gelir. d’nin değeri alınan fark sayıdır.\r\n\\(W_t = \\Delta^dY_t\\) durağan bir\r\nseri ise \\(Y_t\\) serisi d. dereceden\r\nhomojen durağan-dışıdır.\r\nSüreci iki farklı simülasyon örneği ile görelim.\r\n\\(\\phi_1=0.5\\), \\(\\theta_1=-0.3\\) ve \\(\\delta = 1\\) ise ARIMA(1,1,1)’dir. Yani,\r\nARIMA(p = 1, d = 1, q = 1).\r\n\r\n\r\n\r\n\\(\\phi_1=0.5\\), \\(\\theta_1=-0.3\\) ve \\(\\delta = 2\\) ise ARIMA(1,2,1)’dir. Yani,\r\nARIMA(p = 1, d = 2, q = 1).\r\n\r\n\r\n\r\nARIMA Model Kurma Süreci: Box-Jenkins Yaklaşımı\r\nBox-Jenkins yaklaşımının temel fikri cimrilik prensibidir. Yani,\r\nzaman serisi verilerinin özelliklerini ortaya koyan optimal (minimum\r\nsayıda parametre veya serbestlik derecesini göz önünde tutan) bir model\r\nkurmayı öngörür.\r\nYaklaşımı 3 aşamaya ayırabiliriz:\r\nTanımlama\r\n1.1. Veri Hazırlama\r\nVaryansı sabitleştirmek için verilere dönüştürme işlemi\r\nuygulanır.\r\nDurağan seriyi bulmak için verilerin farkı alınır.\r\n1.2. Model Seçimi\r\nPotansiyel modelleri teşhis edebilmek için ACF ve PACF\r\nhesaplanır.\r\nTahmin ve Test Yapma\r\n2.1. Tahmin\r\nPotansiyel modellerdeki parametrelerin tahminleri\r\nyapılır.\r\nUygun kriterler kullanılarak en iyi model seçilir.\r\n2.2. Tanı Kontrol\r\nKalıntıların ACF/PACF’leri kontrol edilir.\r\nKalıntıların Portmanteau testleri yapılır.\r\nKalıntıların temiz-dizi olup olmadığı kontrol edilir.\r\nModelin yeterliliğine bakılır.\r\nUygulama\r\n3.1. Önraporlama\r\nÖnraporlama ve kontrol amacıyla model kullanılır.\r\nUygulama\r\n2022 için yıl sonu enflasyon öngörüsü yapacağız.\r\nTCMB/EVDS’den aldığım verilere (post8.xlsx) GitHub\r\nhesabımdan ulaşabilirsiniz.\r\n\r\n\r\n\r\n\r\n\r\n\r\nZaman serisi analizlerinin geleneksel yaklaşımında ilk yaptığımız\r\nişlem serinin zaman yolu grafiğini çizmektir. Amaç, serinin bir trende\r\nsahip olup olmadığını, konjonktürel dalgalanmaların şiddetini, mevsimsel\r\nhareketleri incelemek; düzensiz hareketleri gidermektir.\r\n\r\n\r\n\r\nYazının ilerleyen kısımlarında ayrıştırma işlemi yapmadan R’da bunu\r\ndikkate alan fonksiyonu çalıştıracağız.\r\nBiz bu çalışmada model seçimi için Akaike Bilgi Kriteri’ni\r\nkullanacağız. Akaike Bilgi Kriteri (AIC) modeldeki terimlerin sayısını\r\ndikkate alarak modelin uyumunun iyiliğini ölçen bir kriterdir. AIC =\r\n-2LogL + 2m olarak tanımlanır. Burada, L olabilirlik iken; m = p +\r\nq’dur. Alternatif tanımlar da mevcuttur.\r\nAlternatif modeller arasında en küçük değerler veren AIC en uygun\r\nmodel ki bu da p ve q, olarak seçilir.\r\nR’da bulunan auto.arima() fonksiyonunu bunun için\r\nkullanabiliriz.\r\n\r\nSeries: enflasyon \r\nARIMA(2,1,1)(0,0,1)[12] with drift \r\n\r\nCoefficients:\r\n          ar1     ar2     ma1     sma1   drift\r\n      -0.4312  0.1966  0.8280  -0.6551  0.0054\r\ns.e.   0.2036  0.1179  0.1852   0.0606  0.0036\r\n\r\nsigma^2 = 0.008768:  log likelihood = 206.92\r\nAIC=-401.83   AICc=-401.43   BIC=-381.5\r\n\r\n\r\n    Ljung-Box test\r\n\r\ndata:  Residuals from ARIMA(2,1,1)(0,0,1)[12] with drift\r\nQ* = 10.033, df = 19, p-value = 0.9521\r\n\r\nModel df: 5.   Total lags used: 24\r\n\r\nACF grafiğinde tüm çubukların sınırlar içinde olduğunu görüyoruz.\r\nKalıntılar için beyaz gürültüye uyumlu ya da buna çok yakın diyebiliriz.\r\nKalıntı otokorelasyonlarının baştan sona kabul edilebilirliğinin\r\nkontrolü için Ljung-Box testine baktığımızda p değerinin 0.9521 olduğunu\r\ngörüyoruz. Bu da otokorelasyon olmadığı yönünde bize destek vermektedir.\r\nVarsayımlar sağlandı.\r\nEn uygun modelin ARIMA(2,1,1)(0,0,1)[12] olduğunu görüyoruz. ARIMA(p\r\n= 2, d = 1, q = 1) kısmını biliyoruz. Peki, (0,0,1)[12] nedir? Serimizde\r\nmevsimsellik olduğu için bunu dikkate alan bir model kurduk.\r\nMevsimselliği dikkate aldığımız zaman buna SARIMA diyebiliriz.\r\n\\(SARIMA(p,d,q)(P,D,Q)_m\\)\r\np,P: Sırasıyla, mevsimsel olmayan ve olan AR\r\nq,Q: Sırasıyla, mevsimsel olmayan ve olan MA\r\nd,D: Sırasıyla, seri durağan yapılana kadar kaç fark alındı ve\r\nseriden mevsimsel etkiyi kaldırmak için kaç fark alındının\r\nkarşılığıdır.\r\nm: Frekans. Aylık veriler ile çalıştığımız için 12’dir.\r\n\r\n\r\n\r\nModele göre enflasyonun Mayıs ayında 74.79% (Alt: 62.25%, Üst:\r\n89.86%); 2022 yılının sonunda ise 46.07% (Alt: 22.14, Üst: 95.85%)\r\nolması beklenmektedir. Evet, güven aralığı oldukça geniş ki bu da\r\nistenen bir durum olmamalı aslında.\r\nModele göre enflasyonda beklenen yıl sonu düşüşü beni pek şaşırtmadı.\r\nÇünkü, Türkiye’de yarın bile ne olacağını bilemiyoruz (o zaman neden\r\nmodel kuruyoruzun cevabı öngörü heyecanı ve modeli nasıl\r\niyileştirebilirizin sorgulanması). 2021 yılının sonlarına doğru\r\nUSDTRY’nin yükselişi durdurulamazken bir anda KKM çıktı ve sert bir\r\ndüşüş gördük. Bakalım enflasyon tarafında ne yaşayacağız.\r\nSon olarak, bu tür çalışmalarda modelin yeni veri geldikçe gözden\r\ngeçirilmesi ve alternatif yöntemlerle desteklenmesi önemlidir\r\ndiyebilirim.\r\n\r\n\r\ntarih\r\n\r\n\r\nÖngörü\r\n\r\n\r\nAlt95\r\n\r\n\r\nÜst95\r\n\r\n\r\n2022-05-01\r\n\r\n\r\n74.79428\r\n\r\n\r\n62.25376\r\n\r\n\r\n89.86098\r\n\r\n\r\n2022-06-01\r\n\r\n\r\n70.54543\r\n\r\n\r\n51.46939\r\n\r\n\r\n96.69160\r\n\r\n\r\n2022-07-01\r\n\r\n\r\n69.65640\r\n\r\n\r\n46.25963\r\n\r\n\r\n104.88657\r\n\r\n\r\n2022-08-01\r\n\r\n\r\n69.76207\r\n\r\n\r\n42.64513\r\n\r\n\r\n114.12196\r\n\r\n\r\n2022-09-01\r\n\r\n\r\n76.10810\r\n\r\n\r\n43.43605\r\n\r\n\r\n133.35566\r\n\r\n\r\n2022-10-01\r\n\r\n\r\n77.00459\r\n\r\n\r\n41.26345\r\n\r\n\r\n143.70364\r\n\r\n\r\n2022-11-01\r\n\r\n\r\n66.99449\r\n\r\n\r\n33.93994\r\n\r\n\r\n132.24130\r\n\r\n\r\n2022-12-01\r\n\r\n\r\n46.06953\r\n\r\n\r\n22.14357\r\n\r\n\r\n95.84728\r\n\r\n\r\n\r\n\r\n\r\nÇalışmanın R kodlarına aşağıdan ulaşılabilir.\r\n\r\n\r\noptions(scipen = 999)\r\n\r\nar1_s1 <- arima.sim(model = list(ar = 0), n = 100)\r\nar1_s2 <- arima.sim(model = list(ar = 0.3), n = 100)\r\nar1_s3 <- arima.sim(model = list(ar = 0.5), n = 100)\r\nar1_s4 <- arima.sim(model = list(ar = 0.7), n = 100)\r\nar1_s5 <- arima.sim(model = list(ar = 0.9), n = 100)\r\n\r\nplot.ts(cbind(ar1_s1,ar1_s2,ar1_s3,ar1_s4,ar1_s5),\r\n        nc = 1,\r\n        main = \"AR(1) Süreci için Alternatif Yapılar\")\r\n\r\nma1_s1 <- arima.sim(model = list(ma = 0), n = 100)\r\nma1_s2 <- arima.sim(model = list(ma = -0.3), n = 100)\r\nma1_s3 <- arima.sim(model = list(ma = -0.5), n = 100)\r\nma1_s4 <- arima.sim(model = list(ma = -0.7), n = 100)\r\nma1_s5 <- arima.sim(model = list(ma = -0.9), n = 100)\r\n\r\nplot.ts(cbind(ma1_s1,ma1_s2,ma1_s3,ma1_s4,ma1_s5),\r\n        nc = 1,\r\n        main = \"MA(1) Süreci için Alternatif Yapılar\")\r\n\r\narma11_s1 <- arima.sim(model = list(ar = 0.3, ma = -0.3), n = 100)\r\narma11_s2 <- arima.sim(model = list(ar = 0.8, ma = -0.3), n = 100)\r\narma11_s3 <- arima.sim(model = list(ar = 0.3, ma = -0.8), n = 100)\r\narma11_s4 <- arima.sim(model = list(ar = -0.8, ma = -0.8), n = 100)\r\narma11_s5 <- arima.sim(model = list(ar = 0.8, ma = 0.8), n = 100)\r\narma11_s6 <- arima.sim(model = list(ar = -0.9, ma = 0.8), n = 100)\r\n\r\nplot.ts(cbind(arma11_s1,arma11_s2,arma11_s3,arma11_s4,arma11_s5,arma11_s6),\r\n        nc = 1,\r\n        main = \"ARMA(1,1) Süreci için Alternatif Yapılar\")\r\n\r\narima111_s1 <- arima.sim(model = list(order = c(1,1,1), ar = 0.5, ma = -0.3), n = 100)\r\nplot.ts(arima111_s1, main = \"ARIMA(1,1,1)\")\r\n\r\narima121_s1 <- arima.sim(model = list(order = c(1,2,1), ar = 0.5, ma = -0.3), n = 100)\r\nplot.ts(arima121_s1, main = \"ARIMA(1,2,1)\")\r\n\r\nlibrary(tidyverse)\r\nlibrary(forecast)\r\n\r\ndf <- readxl::read_excel(\"data.xlsx\") %>% \r\n  mutate(tarih = as.Date(paste0(tarih,\"-\",1)),\r\n         lenf = log(enf))\r\n\r\nenflasyon <- ts(data = df$lenf,\r\n                start = c(2004,1),\r\n                end = c(2022,4),\r\n                frequency = 12)\r\n\r\nggplot(df, aes(x = tarih, y = lenf)) +\r\n  geom_line() +\r\n  theme_minimal() +\r\n  theme(axis.title = element_blank(),\r\n        plot.title = element_text(face = \"bold\"),\r\n        plot.subtitle = element_text(face = \"italic\"),\r\n        plot.caption = element_text(face = \"italic\")) +\r\n  labs(\r\n    title = \"Enflasyon*, 2004/Ocak-2022/Nisan\",\r\n    subtitle = \"*Bir önceki yılın aynı ayına göre değişim\",\r\n    caption = \"Serinin logaritması alınmıştır.\"\r\n  )\r\n\r\nplot(decompose(enflasyon))\r\n\r\nmodel <- auto.arima(y = enflasyon, ic = \"aic\") # trace = TRUE olursa tüm modeller izlenebilir\r\nmodel\r\ncheckresiduals(model)\r\n\r\nongoru <- forecast(model, h = 8) # Mayıs-Aralık = 8 ay\r\n\r\nmodel_output <- data.frame(\r\n  tarih = df$tarih,\r\n  Fit = model$fitted,\r\n  Enflasyon = model$x\r\n) %>% \r\n  bind_rows(\r\n    data.frame(\r\n      tarih = seq(as.Date(\"2022-05-01\"),as.Date(\"2022-12-01\"),by = \"months\"),\r\n      Fit = rep(NA,8),\r\n      Enflasyon = rep(NA,8)\r\n    )\r\n  ) %>% \r\n  mutate(`Öngörü` = c(rep(NA,220),ongoru$mean),\r\n         `Alt95` = c(rep(NA,220),ongoru$lower[,2]),\r\n         `Üst95` = c(rep(NA,220),ongoru$upper[,2])) %>% \r\n  pivot_longer(!tarih, names_to = \"var\", values_to = \"val\")\r\n\r\nggplot(model_output %>% filter(!(var %in% c(\"Alt95\",\"Üst95\"))),\r\n       aes(x = tarih, y = exp(val), color = var)) +\r\n  geom_line() +\r\n  theme_minimal() +\r\n  theme(axis.title = element_blank(),\r\n        legend.title = element_blank(),\r\n        legend.position = \"top\",\r\n        plot.title = element_text(face = \"bold\")) +\r\n  scale_color_manual(values = c(\"red\",\"gray40\",\"blue\",\"gray\",\"gray\")) +\r\n  labs(title = \"Fit Değerler ve Gerçek Enflasyon + Öngörü\")\r\n\r\nautoplot(forecast(model, h = 8))\r\n\r\n\r\n\r\nYararlandığım Kaynaklar:\r\nEkonometrik Zaman Serileri Analizi; M.Sevüktekin,\r\nM.Çınar\r\nZAMAN\r\nSERİLERİ ANALİZİNDE ARIMA MODELLERİ VE BİR UYGULAMA; Ö.Duru\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-06-01-post8/post8_files/figure-html5/unnamed-chunk-28-1.png",
    "last_modified": "2022-06-08T18:48:04+03:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-05-30-post7/",
    "title": "Cumhurbaşkanlığı Hükümet Sistemi Yapısal Kırılmaya Yol Açtı mı?",
    "description": "Ekonomik ve finansal değişkenlerin yapısal kırılma analizi.",
    "author": [
      {
        "name": "A. Uraz Akgül",
        "url": {}
      }
    ],
    "date": "2022-05-30",
    "categories": [
      "Economics",
      "Finance"
    ],
    "contents": "\r\nCumhurbaşkanlığı hükümet sistemi, 16 Nisan 2017 referandumu ile kabul\r\nedildi ve 9 Temmuz 2018 tarihinden itibaren de uygulanmaya başlandı. 24\r\nHaziran 2018’de de genel seçimler ile birlikte Cumhurbaşkanlığı seçimi\r\nyapıldı ve sistemin ilk lideri Erdoğan oldu.\r\nBu yazıda inceleyeceğimiz konu, Cumhurbaşkanlığı Hükümet Sistemi’nin\r\nekonomik ve finansal değişkenlerde yapısal kırılmaya yol açıp\r\naçmadığıdır.\r\nYapısal kırılma analizi için enflasyon, kur ve faiz verilerini\r\nseçtim. TCMB/EVDS’den aldığım verilere (post7.xlsx) GitHub\r\nhesabımdan ulaşabilirsiniz.\r\nFrekansları aylık ve çeyreklik bazda alıp yapısal kırılmayı bu\r\nşekilde inceleyeceğiz. Veri aralığı ise 2015-2022 yılları arasını\r\nkapsayacak.\r\nAylık olan frekansları çeyreklik olarak da incelemek isteme sebebim,\r\nbir olayın etkisi hemen o ay değil; içinde bulunduğu çeyrek içinde\r\nbaşladıysa kaçırmamaktır. 2015-2022 yıllarını seçme sebebim ise 2018\r\nyılının öncesi ve sonrasına zamanı yaklaşık olarak eşit dağıtmaktır.\r\nÇalışmada, serilerde birden fazla yapısal kırılma olduğu zaman\r\nkullanılabilen ve bu kırılmaları endojen (içsel) olarak kabul eden\r\nBai-Perron yöntemi kullanılmıştır. Bai-Perron (1998,2003), en küçük\r\nkareler yöntemi ile tahmin edilen regresyon modelinde bilinmeyen kırılma\r\nzamanlarını belirlemek amacıyla çoklu kırılmaların tespitine yönelik\r\ntest geliştirmişlerdir.\r\nBilinmeyen tarihlerdeki m adet kırılma (m+1 rejim) için oluşturulan\r\nçoklu regresyon modeli şöyledir:\r\n\\(y_t = x_t'\\beta + z_t'\\delta_1 +\r\n\\epsilon_t; t = 1,...,T_1\\)\r\n\\(y_t = x_t'\\beta + z_t'\\delta_2 +\r\n\\epsilon_t; t = T_1 + 1,...,T_2\\)\r\n. . .\r\n\\(y_t = x_t'\\beta + z_t'\\delta_j +\r\n\\epsilon_t; t = T_{j-1} + 1,...T_j; j = 1,...,m\\)\r\nSon yazdığımız denklemde;\r\n\\(x_t'\\) ve \\(z_t'\\) sırasıyla px1 ve qx1 boyutlu\r\nbağımsız değişkenler vektörü,\r\n\\(\\beta\\) ve \\(\\delta_j\\) katsayılar vektörü,\r\n\\(\\epsilon_t\\) saf hata terimi,\r\n\\(T_0 = 0\\) ve \\(T_{m+1} = T\\) olmak üzere, \\(T_1,T_2,...,T_m\\) bilinmeyen kırılma\r\nnoktalarıdır.\r\nBai-Perron testinin temel amacı, T sayıda gözlem, \\(y_t\\), \\(x_t'\\) ve \\(z_t'\\)’nin değerlerinin bilindiği\r\nvarsayımı altında, bilinmeyen regresyon parametreleri (\\(\\beta,\\delta_1,...,\\delta_m\\)) ve kırılma\r\ntarihlerinin (\\(T_1,T_2,...,T_m\\))\r\nbirlikte tahmin edilmesidir.\r\n\r\n\r\n\r\nEnflasyon\r\n\r\n\r\n\r\n=============================\r\n                  (Intercept)\r\n-----------------------------\r\n2015(1) - 2018(5)    9.094   \r\n2018(6) - 2021(3)   15.302   \r\n2021(4) - 2022(4)   32.352   \r\n-----------------------------\r\n\r\nAylık frekansta 2 tane kırılmanın olduğunu görüyoruz. İlk kırılma\r\n2018 yılının Mayıs ayına denk geliyor. Haziran ve Temmuz 2018’in\r\nsırasıyla seçimin yapıldığı ve sistemin uygulanmaya başlandığı aylar\r\nolduğunu biliyoruz. Sistem içerisinde bir kırılma da Mart 2021’de\r\nyaşanmış. Bu, piyasalara verdiği güven ile bilinen TCMB eski başkanı\r\nNaci Ağbal’ın görevden alındığı dönemdir. Yazının ilerleyen bölümlerinde\r\nfaizi de inceleyeceğiz. Şimdi o faizi alıp enflasyon ile bir araya\r\ngetirip bir bakalım. Çünkü, faiz sebep enflasyon sonuçtur\r\nısrarının bedelini ödemeye devam ettiğimizin resmi olacak.\r\n\r\n\r\n\r\n\r\n\r\n\r\n=============================\r\n                  (Intercept)\r\n-----------------------------\r\n2015(1) - 2021(2)   12.107   \r\n2021(3) - 2022(2)   42.438   \r\n-----------------------------\r\n\r\nÇeyreklik frekansta sadece 1 kırılma verdi o da 2021 yılının 2.\r\nçeyreğine ait.\r\nÜzerinde durduğumuz enflasyon-faiz serilerindeki kırılmayı çeyreklik\r\nolarak biraz daha fazla görebiliriz.\r\n\r\n\r\n\r\nFaiz\r\n\r\n\r\n\r\n===============================\r\n                    (Intercept)\r\n-------------------------------\r\n2015(1) - 2017(2)      8.488   \r\n2017(3) - 2018(6)     12.598   \r\n2018(7) - 2019(9)     22.383   \r\n2019(10) - 2020(10)   10.650   \r\n2020(11) - 2022(4)    16.628   \r\n-------------------------------\r\n\r\nAylık frekansta 4 tane kırılmanın olduğunu görüyoruz. Sistemle\r\nberaber ilk kırılma 2018 yılının Haziran ayında yaşanmış ve bu\r\nkırılmadan sonra 2 kırılma daha yaşanmış. Erdoğan’ın 24 Haziran öncesi\r\nsöylediği, 24’ünde siz bu kardeşinize yetkiyi verin, ondan sonra bu\r\nfaizle, şunla bunla nasıl uğraşılır göreceksiniz sözünün tabloya\r\nyansımış halini görüyoruz adeta.\r\n\r\n\r\n\r\n=============================\r\n                  (Intercept)\r\n-----------------------------\r\n2015(1) - 2017(1)    8.580   \r\n2017(2) - 2018(2)   12.706   \r\n2018(3) - 2019(3)   22.383   \r\n2019(4) - 2020(3)   10.522   \r\n2020(4) - 2022(2)   16.166   \r\n-----------------------------\r\n\r\nÇeyreklik frekansta sistem ile beraber 3 kırılma görüyoruz. İlki\r\nseçimin de içinde bulunduğu 2. çeyrek.\r\nKur\r\n\r\n\r\n\r\n=============================\r\n                  (Intercept)\r\n-----------------------------\r\n2015(1) - 2018(4)    3.202   \r\n2018(5) - 2020(2)    5.565   \r\n2020(3) - 2021(3)    7.246   \r\n2021(4) - 2022(4)   10.790   \r\n-----------------------------\r\n\r\nAylık frekansta 3 tane kırılmanın olduğunu görüyoruz. Sistemden önce\r\nkırılma 2018 yılının Nisan ayında yaşanmış ve bu kırılmadan sonra sistem\r\niçerisinde 2 kırılma daha yaşanmış. Kırılmalardan biri yine daha önce\r\nbahsettiğim eski başkan Ağbal’ın görevden alındığı tarihte yaşanmış.\r\n\r\n\r\n\r\n=============================\r\n                  (Intercept)\r\n-----------------------------\r\n2015(1) - 2018(2)    3.265   \r\n2018(3) - 2020(1)    5.700   \r\n2020(2) - 2021(2)    7.532   \r\n2021(3) - 2022(2)   12.046   \r\n-----------------------------\r\n\r\nÇeyreklik frekansta sistemin de içinde bulunduğu 2018 yılının 2.\r\nçeyreğindeki kırılma ile beraber toplamda 3 kırılma yaşanmış.\r\nSon olarak enflasyon, faiz ve kur değişkenlerini endeks\r\nhaline getirelim. Soru: 2011 yılı itibarıyla kaç kırılma\r\nyaşandı?\r\nSerileri önce aşağıdaki gibi normalize edeceğiz.\r\n\\(X_{nor} = \\frac{X - X_{min}}{X_{max} -\r\nX_{min}}\\)\r\nArdından da değişken sayısına bölerek bir endeks oluşturacağız.\r\n\\(Endeks =\r\n\\frac{Nor(Enflasyon)+Nor(Faiz)+Nor(Kur)}{3}\\)\r\nFaiz için Ağırlıklı Ortalama Fonlama Maliyeti’ni kullandığımız ve bu\r\nveri de 2011 yılı itibarıyla ulaşılabilir olduğu için 2011/Ocak -\r\n2022/Nisan verilerini kullanacağız.\r\n\r\n\r\n\r\n\r\n\r\n\r\n==============================\r\n                   (Intercept)\r\n------------------------------\r\n2011(1) - 2016(9)     0.082   \r\n2016(10) - 2018(5)    0.198   \r\n2018(6) - 2020(8)     0.382   \r\n2020(9) - 2022(4)     0.508   \r\n------------------------------\r\n\r\n2011 yılı itbarıyla aylık (ve çeyreklik) frekansta 3 kırılmanın\r\nyaşandığını görüyoruz. Bunların ilki sistem öncesi, biri sisteme çok\r\nyakın ve biri sistem sonrası. Sistem ile beraber oluşturulan endeksin\r\nyukarı doğru tırmanışı dikkat çekici. Her ne kadar sonrasında bir\r\ntoparlanma sürecine girse de atılan yanlış adımların etkisi çok ciddi\r\nolmuş ki olmaya da devam ediyor.\r\n\r\n\r\n\r\n=============================\r\n                  (Intercept)\r\n-----------------------------\r\n2011(1) - 2016(4)    0.084   \r\n2017(1) - 2018(2)    0.220   \r\n2018(3) - 2020(4)    0.378   \r\n2021(1) - 2022(2)    0.580   \r\n-----------------------------\r\n\r\nÇalışmanın R kodlarına aşağıdan ulaşılabilir.\r\n\r\n\r\noptions(scipen = 999)\r\n\r\nlibrary(strucchange) # yapısal kırılma\r\nlibrary(tidyverse)\r\nlibrary(lubridate)\r\nlibrary(stargazer)\r\n\r\ndf_aylik <- readxl::read_excel(\"data.xlsx\") %>% \r\n  mutate(tarih = as.Date(paste0(tarih,\"-\",1))) %>% \r\n  na.omit() %>% \r\n  filter(tarih >= as.Date(\"2015-01-01\"))\r\n\r\n# enflasyon\r\n\r\nenf_aylik <- df_aylik %>% \r\n  select(tarih,enflasyon) %>% \r\n  mutate(t = seq(1,nrow(.),1))\r\n\r\nenf_ceyreklik <- enf_aylik %>% \r\n  mutate(tarih = as.Date(paste0(year(tarih),\"-\",quarter(tarih),\"-\",1))) %>% \r\n  group_by(tarih) %>% \r\n  summarise(enflasyon = mean(enflasyon)) %>% \r\n  ungroup() %>% \r\n  mutate(t = seq(1,nrow(.),1))\r\n\r\n# faiz\r\n\r\nfaiz_aylik <- df_aylik %>% \r\n  select(tarih,faiz) %>% \r\n  mutate(t = seq(1,nrow(.),1))\r\n\r\nfaiz_ceyreklik <- faiz_aylik %>% \r\n  mutate(tarih = as.Date(paste0(year(tarih),\"-\",quarter(tarih),\"-\",1))) %>% \r\n  group_by(tarih) %>% \r\n  summarise(faiz = mean(faiz)) %>% \r\n  ungroup() %>% \r\n  mutate(t = seq(1,nrow(.),1))\r\n\r\n# kur\r\n\r\nkur_aylik <- df_aylik %>% \r\n  select(tarih,kur) %>% \r\n  mutate(t = seq(1,nrow(.),1))\r\n\r\nkur_ceyreklik <- kur_aylik %>% \r\n  mutate(tarih = as.Date(paste0(year(tarih),\"-\",quarter(tarih),\"-\",1))) %>% \r\n  group_by(tarih) %>% \r\n  summarise(kur = mean(kur)) %>% \r\n  ungroup() %>% \r\n  mutate(t = seq(1,nrow(.),1))\r\n\r\nenf_aylik_ts <- ts(data = enf_aylik$enflasyon,\r\n                   start = c(2015,1),\r\n                   frequency = 12)\r\n\r\nenf_aylik_bps <- breakpoints(enf_aylik_ts~1)\r\n\r\nenf_aylik <- enf_aylik %>% \r\n  mutate(bp = ifelse(t %in% enf_aylik_bps$breakpoints, t, NA))\r\n\r\nggplot(enf_aylik, aes(x = tarih, y = enflasyon)) +\r\n  geom_line() +\r\n  geom_vline(data = enf_aylik %>% na.omit(),\r\n             aes(xintercept = tarih), linetype = \"dashed\") +\r\n  theme_minimal() +\r\n  theme(plot.title = element_text(face = \"bold\", size = 10),\r\n        plot.subtitle = element_text(face = \"italic\", size = 7),\r\n        axis.title = element_blank()) +\r\n  scale_x_date(date_breaks = \"year\", date_labels = \"%Y\") +\r\n  labs(title = \"Enflasyon, Aylık\",\r\n       subtitle = \"2015/Ocak-2022/Nisan\")\r\n\r\nstargazer(coef(enf_aylik_bps), type = \"text\")\r\n\r\nenf_faiz_aylik <- merge(enf_aylik[,c(1,2)], faiz_aylik[,c(1,2)]) %>% \r\n  mutate(t = seq(1,nrow(.),1)) %>% \r\n  pivot_longer(!c(t,tarih), names_to = \"vars\", values_to = \"vals\")\r\n\r\nenf_faiz_tarih <- enf_faiz_aylik %>% \r\n  filter(t == enf_aylik_bps$breakpoints[2]) %>% \r\n  pull(tarih)\r\n\r\nggplot(enf_faiz_aylik, aes(x = tarih, y = vals, group = vars, color = vars)) +\r\n  geom_line() +\r\n  geom_vline(xintercept = enf_faiz_tarih, linetype = \"dashed\") +\r\n  theme_minimal() +\r\n  theme(axis.title = element_blank(),\r\n        plot.title = element_text(face = \"bold\"),\r\n        legend.position = \"top\",\r\n        legend.title = element_blank()) +\r\n  scale_color_manual(values = c(\"red\",\"blue\")) +\r\n  scale_x_date(date_breaks = \"year\", date_labels = \"%Y\") +\r\n  labs(title = \"Aylık\")\r\n\r\nenf_ceyreklik_ts <- ts(data = enf_ceyreklik$enflasyon,\r\n                       start = c(2015,1),\r\n                       frequency = 4)\r\n\r\nenf_ceyreklik_bps <- breakpoints(enf_ceyreklik_ts~1)\r\n\r\nenf_ceyreklik <- enf_ceyreklik %>% \r\n  mutate(bp = ifelse(t %in% enf_ceyreklik_bps$breakpoints, t, NA))\r\n\r\nggplot(enf_ceyreklik, aes(x = tarih, y = enflasyon)) +\r\n  geom_line() +\r\n  geom_vline(data = enf_ceyreklik %>% na.omit(),\r\n             aes(xintercept = tarih), linetype = \"dashed\") +\r\n  theme_minimal() +\r\n  theme(plot.title = element_text(face = \"bold\", size = 10),\r\n        plot.subtitle = element_text(face = \"italic\", size = 7),\r\n        plot.caption = element_text(face = \"italic\", size = 7),\r\n        axis.title = element_blank()) +\r\n  scale_x_date(date_breaks = \"year\", date_labels = \"%Y\") +\r\n  labs(title = \"Enflasyon, Çeyreklik*\",\r\n       subtitle = \"2015/Q1-2022/Q2**\",\r\n       caption = \"*Üçer aylık ortalamalardır.\\n**Henüz tamamlanmamıştır.\")\r\n\r\nstargazer(coef(enf_ceyreklik_bps), type = \"text\")\r\n\r\nenf_faiz_ceyreklik <- merge(enf_ceyreklik[,c(1,2)], faiz_ceyreklik[,c(1,2)]) %>% \r\n  mutate(t = seq(1,nrow(.),1)) %>% \r\n  pivot_longer(!c(t,tarih), names_to = \"vars\", values_to = \"vals\")\r\n\r\nenf_faiz_tarih2 <- enf_faiz_ceyreklik %>% \r\n  filter(t == enf_ceyreklik_bps$breakpoints[1]) %>% \r\n  pull(tarih)\r\n\r\nggplot(enf_faiz_ceyreklik, aes(x = tarih, y = vals, group = vars, color = vars)) +\r\n  geom_line() +\r\n  geom_vline(xintercept = enf_faiz_tarih2, linetype = \"dashed\") +\r\n  theme_minimal() +\r\n  theme(axis.title = element_blank(),\r\n        plot.title = element_text(face = \"bold\"),\r\n        legend.position = \"top\",\r\n        legend.title = element_blank()) +\r\n  scale_color_manual(values = c(\"red\",\"blue\")) +\r\n  scale_x_date(date_breaks = \"year\", date_labels = \"%Y\") +\r\n  labs(title = \"Çeyreklik\")\r\n\r\nfaiz_aylik_ts <- ts(data = faiz_aylik$faiz,\r\n                    start = c(2015,1),\r\n                    frequency = 12)\r\n\r\nfaiz_aylik_bps <- breakpoints(faiz_aylik_ts~1)\r\n\r\nfaiz_aylik <- faiz_aylik %>% \r\n  mutate(bp = ifelse(t %in% faiz_aylik_bps$breakpoints, t, NA))\r\n\r\nggplot(faiz_aylik, aes(x = tarih, y = faiz)) +\r\n  geom_line() +\r\n  geom_vline(data = faiz_aylik %>% na.omit(),\r\n             aes(xintercept = tarih), linetype = \"dashed\") +\r\n  theme_minimal() +\r\n  theme(plot.title = element_text(face = \"bold\", size = 10),\r\n        plot.subtitle = element_text(face = \"italic\", size = 7),\r\n        axis.title = element_blank()) +\r\n  scale_x_date(date_breaks = \"year\", date_labels = \"%Y\") +\r\n  labs(title = \"Faiz, Aylık\",\r\n       subtitle = \"2015/Ocak-2022/Nisan\")\r\n\r\nstargazer(coef(faiz_aylik_bps), type = \"text\")\r\n\r\nfaiz_ceyreklik_ts <- ts(data = faiz_ceyreklik$faiz,\r\n                        start = c(2015,1),\r\n                        frequency = 4)\r\n\r\nfaiz_ceyreklik_bps <- breakpoints(faiz_ceyreklik_ts~1)\r\n\r\nfaiz_ceyreklik <- faiz_ceyreklik %>% \r\n  mutate(bp = ifelse(t %in% faiz_ceyreklik_bps$breakpoints, t, NA))\r\n\r\nggplot(faiz_ceyreklik, aes(x = tarih, y = faiz)) +\r\n  geom_line() +\r\n  geom_vline(data = faiz_ceyreklik %>% na.omit(),\r\n             aes(xintercept = tarih), linetype = \"dashed\") +\r\n  theme_minimal() +\r\n  theme(plot.title = element_text(face = \"bold\", size = 10),\r\n        plot.subtitle = element_text(face = \"italic\", size = 7),\r\n        plot.caption = element_text(face = \"italic\", size = 7),\r\n        axis.title = element_blank()) +\r\n  scale_x_date(date_breaks = \"year\", date_labels = \"%Y\") +\r\n  labs(title = \"Faiz, Çeyreklik*\",\r\n       subtitle = \"2015/Q1-2022/Q2**\",\r\n       caption = \"*Üçer aylık ortalamalardır.\\n**Henüz tamamlanmamıştır.\")\r\n\r\nstargazer(coef(faiz_ceyreklik_bps), type = \"text\")\r\n\r\nkur_aylik_ts <- ts(data = kur_aylik$kur,\r\n                   start = c(2015,1),\r\n                   frequency = 12)\r\n\r\nkur_aylik_bps <- breakpoints(kur_aylik_ts~1)\r\n\r\nkur_aylik <- kur_aylik %>% \r\n  mutate(bp = ifelse(t %in% kur_aylik_bps$breakpoints, t, NA))\r\n\r\nggplot(kur_aylik, aes(x = tarih, y = kur)) +\r\n  geom_line() +\r\n  geom_vline(data = kur_aylik %>% na.omit(),\r\n             aes(xintercept = tarih), linetype = \"dashed\") +\r\n  theme_minimal() +\r\n  theme(plot.title = element_text(face = \"bold\", size = 10),\r\n        plot.subtitle = element_text(face = \"italic\", size = 7),\r\n        axis.title = element_blank()) +\r\n  scale_x_date(date_breaks = \"year\", date_labels = \"%Y\") +\r\n  labs(title = \"Kur, Aylık\",\r\n       subtitle = \"2015/Ocak-2022/Nisan\")\r\n\r\nstargazer(coef(kur_aylik_bps), type = \"text\")\r\n\r\nkur_ceyreklik_ts <- ts(data = kur_ceyreklik$kur,\r\n                       start = c(2015,1),\r\n                       frequency = 4)\r\n\r\nkur_ceyreklik_bps <- breakpoints(kur_ceyreklik_ts~1)\r\n\r\nkur_ceyreklik <- kur_ceyreklik %>% \r\n  mutate(bp = ifelse(t %in% kur_ceyreklik_bps$breakpoints, t, NA))\r\n\r\nggplot(kur_ceyreklik, aes(x = tarih, y = kur)) +\r\n  geom_line() +\r\n  geom_vline(data = kur_ceyreklik %>% na.omit(),\r\n             aes(xintercept = tarih), linetype = \"dashed\") +\r\n  theme_minimal() +\r\n  theme(plot.title = element_text(face = \"bold\", size = 10),\r\n        plot.subtitle = element_text(face = \"italic\", size = 7),\r\n        plot.caption = element_text(face = \"italic\", size = 7),\r\n        axis.title = element_blank()) +\r\n  scale_x_date(date_breaks = \"year\", date_labels = \"%Y\") +\r\n  labs(title = \"Kur, Çeyreklik*\",\r\n       subtitle = \"2015/Q1-2022/Q2**\",\r\n       caption = \"*Üçer aylık ortalamalardır.\\n**Henüz tamamlanmamıştır.\")\r\n\r\nstargazer(coef(kur_ceyreklik_bps), type = \"text\")\r\n\r\ndf_aylik2 <- readxl::read_excel(\"data.xlsx\") %>% \r\n  mutate(tarih = as.Date(paste0(tarih,\"-\",1))) %>% \r\n  na.omit()\r\n\r\nnormalize <- function(x){\r\n  \r\n  return((x - min(x)) / (max(x) - min(x)))\r\n  \r\n}\r\n\r\ndf_endeks <- df_aylik2 %>% \r\n  mutate_at(vars(-tarih), function(x) normalize(x)) %>% \r\n  mutate(endeks = rowSums(.[,-1])/ncol(.[,-1]),\r\n         t = seq(1,nrow(.),1))\r\n\r\nendeks_aylik_ts <- ts(data = df_endeks$endeks,\r\n                      start = c(2011,1),\r\n                      frequency = 12)\r\n\r\nendeks_aylik_bps <- breakpoints(endeks_aylik_ts~1)\r\n\r\ndf_endeks <- df_endeks %>% \r\n  mutate(bp = ifelse(t %in% endeks_aylik_bps$breakpoints, t, NA))\r\n\r\nggplot(df_endeks, aes(x = tarih, y = endeks)) +\r\n  geom_line() +\r\n  geom_vline(data = df_endeks %>% na.omit(),\r\n             aes(xintercept = tarih), linetype = \"dashed\") +\r\n  theme_minimal() +\r\n  theme(plot.title = element_text(face = \"bold\", size = 10),\r\n        plot.subtitle = element_text(face = \"italic\", size = 7),\r\n        plot.caption = element_text(face = \"italic\"),\r\n        axis.title = element_blank()) +\r\n  scale_x_date(date_breaks = \"year\", date_labels = \"%Y\") +\r\n  labs(title = \"2011 Yılı İtibarıyla Yapısal Kırılmalar\",\r\n       subtitle = \"2011/Ocak-2022/Nisan\",\r\n       caption = \"*Normalize edilen (Enflasyon+Faiz+Kur)/3\")\r\n\r\nstargazer(coef(endeks_aylik_bps), type = \"text\")\r\n\r\ndf_endeks2 <- df_endeks %>% \r\n  select(1:5) %>% \r\n  mutate(tarih = as.Date(paste0(year(tarih),\"-\",quarter(tarih),\"-\",1))) %>% \r\n  group_by(tarih) %>% \r\n  summarise(endeks = mean(endeks)) %>% \r\n  ungroup() %>% \r\n  mutate(t = seq(1,nrow(.),1))\r\n\r\nendeks_ceyreklik_ts <- ts(data = df_endeks2$endeks,\r\n                          start = c(2011,1),\r\n                          frequency = 4)\r\n\r\nendeks_ceyreklik_bps <- breakpoints(endeks_ceyreklik_ts~1)\r\n\r\ndf_endeks2 <- df_endeks2 %>% \r\n  mutate(bp = ifelse(t %in% endeks_ceyreklik_bps$breakpoints, t, NA))\r\n\r\nggplot(df_endeks2, aes(x = tarih, y = endeks)) +\r\n  geom_line() +\r\n  geom_vline(data = df_endeks2 %>% na.omit(),\r\n             aes(xintercept = tarih), linetype = \"dashed\") +\r\n  theme_minimal() +\r\n  theme(plot.title = element_text(face = \"bold\", size = 10),\r\n        plot.subtitle = element_text(face = \"italic\", size = 7),\r\n        plot.caption = element_text(face = \"italic\"),\r\n        axis.title = element_blank()) +\r\n  scale_x_date(date_breaks = \"year\", date_labels = \"%Y\") +\r\n  labs(title = \"2011 Yılı İtibarıyla Yapısal Kırılmalar\",\r\n       subtitle = \"2011/Q1-2022/Q2*\",\r\n       caption = \"*Normalize edilen (Enflasyon+Faiz+Kur)/3\\nHenüz tamamlanmamıştır.\")\r\n\r\nstargazer(coef(endeks_ceyreklik_bps), type = \"text\")\r\n\r\n\r\n\r\nYararlandığım Kaynaklar:\r\nFinansal Ekonometri; N.Ç.Yavuz\r\nCumhurbaşkanlığı\r\nHükûmet Sistemi\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-05-30-post7/post7_files/figure-html5/unnamed-chunk-11-1.png",
    "last_modified": "2022-05-30T18:29:06+03:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-05-24-post6/",
    "title": "İllerin Seçim Zamanı Tüketici Güven Endeksi ve Enflasyona Olan Duyarlılığı",
    "description": "Tüketici Güven Endeksi ve Enflasyon ile illerin oy oranları arasındaki korelasyon.",
    "author": [
      {
        "name": "A. Uraz Akgül",
        "url": {}
      }
    ],
    "date": "2022-05-24",
    "categories": [
      "Election"
    ],
    "contents": "\r\nOy vermede seçmenleri etkileyen birçok faktör bulunmaktadır ve\r\nliteratürde birçok oy verme modeli bulunmaktadır. Bunlardan biri de\r\nekonomik oy verme modelidir. Siyaset biliminde ekonomik oylama, seçmen\r\ndavranışının seçim anında ülkelerindeki ekonomik koşullardan büyük\r\nölçüde etkilendiğini savunan teorik bir bakış açısıdır.\r\nEkonomik oy verme modelinden yola çıkarak iki tane değişken\r\nbelirledim: Tüketici Güven Endeksi ve Enflasyon. İllerin oy oranlarını\r\nise AKP ile sınırlayıp 2007, 2011, 2015-I, 2015-II ve 2018 genel\r\nseçimlerini dikkate aldım. Gözlem sayısının az oluşundan (N = 5) dolayı\r\nanalizime temkinli yaklaşılması konusunda uyarmalıyım ancak bir miktar\r\nfikir verebileceğini de eklemek istiyorum. Özellikle Tüketici Güven\r\nEndeksi tarafı yakından takip edilmelidir.\r\nTÜİK’ten aldığım verilere (post6_1.xls, post6_2.xls,\r\npost6_3.xls) GitHub\r\nhesabımdan ulaşabilirsiniz. Oy oranları verileri Vikipedi’den web\r\nkazıma yöntemi ile alınmıştır.\r\n\r\n\r\n\r\nTüketici Güven Endeksi\r\n\r\n\r\n\r\nTüketici Güven Endeksi, 2004 yılının Ocak ayından 2022 yılının Mayıs\r\nayına kadar toparlama sürecine girse de aşağı yönlü bir eğilim\r\nsergilemiştir. 0 ile 200 arasında yer alan endeksin 100 üzerinde\r\nolmasının iyimser durumu gösterdiğini baz alırsak 221 ayın sadece\r\n7’sinde iyimser bölgede kalabilmiş. Bu 7 değerin 6’sı 2004, 1’i ise 2006\r\nyılına aittir.\r\nVerilerin olduğu aralıkta ilk genel seçim 22 Temmuz 2007’de oldu ve\r\nendeks Haziran ve Temmuz 2007’de sırasıyla 96.5 ve 97.2 idi. Bugün\r\nendeks 67.6 olmuştur.\r\n\r\n\r\n\r\nEnflasyon\r\n\r\n\r\n\r\nEnflasyon, 2004 yılının Ocak ayından 2022 yılının Nisan ayına kadar\r\nlogaritmik olarak baktığımız zaman eksponansiyel bir artış sergilemiş\r\ndiyebiliriz. İlk ciddi bozulmasını ise 2018 yılında göstermiş. 20% üstü\r\nenflasyonun olduğu 11 ayın 4’ü 2018’e, 1’i 2019’a, 2’si 2021’e ve kalan\r\n4’ü 2022’ye aittir.\r\nVerilerin olduğu aralıkta ilk genel seçim 22 Temmuz 2007’de oldu ve\r\nenflasyon Haziran ve Temmuz 2007’de sırasıyla 8.6% ve 6.9% idi. Bugün\r\nenflasyon 70% olmuştur.\r\n\r\n\r\n\r\nGenel Seçimler AKP Oy Oranı\r\nTürkiye, AKP’nin olduğu 6 genel seçim görmüştür.\r\n3 Kasım 2002\r\n22 Temmuz 2007\r\n12 Haziran 2011\r\n7 Haziran 2015\r\n1 Kasım 2015\r\n24 Haziran 2018\r\nGenel seçimlerde (2002 analizde olmayacağı için hariç) aldıkları oy\r\noranlarının zaman serisi aşağıdaki gibidir.\r\n\r\n\r\n\r\nDeğişkenler ile Oy Oranları Arasındaki İlişki\r\n\r\n\r\n\r\nTüketici Güven Endeksi ve Türkiye Geneli Oy Oranları\r\nKorelasyonu\r\nTürkiye genelinde, Tüketici Güven Endeksi’nin artışı oy oranlarına\r\npozitif; düşüşü ise negatif yansımıştır. Aradaki korelasyon: 90.5%\r\n\r\n\r\n\r\nEnflasyon ve Türkiye Geneli Oy Oranları\r\nKorelasyonu\r\nTürkiye genelinde, Enflasyon artışı oy oranlarına negatif; düşüşü ise\r\npozitif yansımıştır ifadesini aslında tam olarak söyleyemiyoruz. 2018\r\nyılına ait enflasyon oranı bir uç değer olmuştur ve eğimi aşağıya\r\nçekmektedir. Aradaki korelasyon: -45.8%\r\n\r\n\r\n\r\nYukarıdaki inceleme geneli yansıtmaktaydı. İller bazında ilişkiler ve\r\nkorelasyon katsayıları değişkenlik gösterebilir ki konumuz da bununla\r\nilgilidir.\r\nTüketici Güven Endeksi ve İllere Ait Oy Oranları\r\nKorelasyonu\r\n\r\n\r\n\r\nTüketici Güven Endeksi’ne en hassas ilin Erzurum olduğunu\r\nsöyleyebiliriz. Bu ili sırasıyla Eskişehir ve Bingöl takip ediyor. Zayıf\r\nda olsa negatif korelasyon gösteren 3 il ise Rize, Kırklareli ve Edirne.\r\n81 ilin 58’i 50%’den fazla pozitif korelasyonu olan yerlerdir.\r\n\r\n\r\n\r\n90% üstü pozitif korelasyona sahip iller\r\n\r\n\r\n\r\nEnflasyon ve İllere Ait Oy Oranları Korelasyonu\r\n\r\n\r\n\r\nEnflasyona en hassas ilin Yozgat olduğunu söyleyebiliriz. Bu ili\r\nsırasıyla Erzincan ve Kırıkkale takip ediyor. Ne zayıf ne güçlü\r\ndiyebileceğimiz pozitif korelasyon gösteren en yüksek 3 il ise Sinop,\r\nTunceli ve Kırklareli’dir. 81 ilin 19’u 50%’den fazla negatif\r\nkorelasyonu olan yerlerdir.\r\n\r\n\r\n\r\n60% üstü negatif korelasyona sahip iller\r\n\r\n\r\n\r\nTüketici Güven Endeksi ve Enflasyon Oranı Değişkenlerinin Oy\r\nOranlarıyla Olan Korelasyonu\r\n\r\n\r\n\r\n\r\n\r\n\r\nYukarıdaki grafiği, dikey eksenden (enflasyon) aşağıya doğru\r\ngittikçe ve yatay eksenden (tüketici güven endeksi) sağa doğru gittikçe\r\nbu değişkenlere olan hassasiyet artmaktadır şeklinde\r\nokuyabiliriz.\r\nÇalışmanın R kodlarına aşağıdan ulaşılabilir.\r\n\r\n\r\noptions(scipen = 999)\r\n\r\nlibrary(tidyverse)\r\nlibrary(rvest) # web kazıma\r\n\r\ntge1 <- readxl::read_excel(\"tge1.xls\") %>% \r\n  select(1,2) %>% \r\n  slice(5:100) %>% \r\n  rename(\"donem\"=1,\"tge\"=2) %>% \r\n  mutate(donem = seq(as.Date(\"2004-01-01\"),as.Date(\"2011-12-01\"), \"months\"),\r\n         tge = as.numeric(tge))\r\n\r\ntge2 <- readxl::read_excel(\"tge2.xls\") %>% \r\n  select(1,4) %>% \r\n  slice(5:129) %>% \r\n  rename(\"donem\"=1,\"tge\"=2) %>% \r\n  mutate(donem = seq(as.Date(\"2012-01-01\"),as.Date(\"2022-05-01\"), \"months\"),\r\n         tge = as.numeric(tge))\r\n\r\ntge <- rbind(tge1,tge2) %>% \r\n  mutate(tgeLag = lag(tge))\r\n\r\nggplot(tge, aes(x = donem, y = tge)) +\r\n  geom_line() +\r\n  geom_smooth(method = \"loess\", color = \"red\") +\r\n  geom_hline(yintercept = 100) +\r\n  theme_minimal() +\r\n  theme(axis.title = element_blank(),\r\n        plot.title = element_text(face = \"bold\"),\r\n        plot.subtitle = element_text(face = \"italic\"),\r\n        plot.caption = element_text(face = \"italic\")) +\r\n  labs(title = \"Tüketici Güven Endeksi\",\r\n       subtitle = \"Ocak/2004 - Mayıs/2022\",\r\n       caption = \"Veri: TÜİK\")\r\n\r\nenf <- readxl::read_excel(\"enflasyon.xls\") %>% \r\n  slice(80:98) %>% \r\n  `colnames<-`(c(\"yil\",paste0(\"C\",seq(1,18,1)))) %>% \r\n  pivot_longer(!yil, names_to = \"ay\", values_to = \"enf\") %>% \r\n  na.omit() %>% \r\n  mutate(donem = seq(as.Date(\"2004-01-01\"),as.Date(\"2022-04-01\"), \"months\"),\r\n         enf = as.numeric(enf),\r\n         enfLag = lag(enf)) %>% \r\n  select(donem,enf,enfLag)\r\n\r\nggplot(enf, aes(x = donem, y = log(enf))) +\r\n  geom_line() +\r\n  geom_smooth(method = \"loess\", color = \"red\") +\r\n  theme_minimal() +\r\n  theme(axis.title = element_blank(),\r\n        plot.title = element_text(face = \"bold\"),\r\n        plot.subtitle = element_text(face = \"italic\"),\r\n        plot.caption = element_text(face = \"italic\")) +\r\n  labs(title = \"Enflasyon (Bir Önceki Yılın Aynı Ayına Göre)\",\r\n       subtitle = \"Ocak/2004 - Nisan/2022\",\r\n       caption = \"Veriler logaritmiktir.\\n Veri: TÜİK\")\r\n\r\nakp <- data.frame(\r\n  t = seq(1,5,1),\r\n  tarih = c(\r\n    as.Date(\"2007-07-22\"),\r\n    as.Date(\"2011-06-12\"),\r\n    as.Date(\"2015-06-07\"),\r\n    as.Date(\"2015-11-01\"),\r\n    as.Date(\"2018-06-24\")\r\n  ),\r\n  oran = c(46.58,49.83,40.87,49.50,42.56)\r\n)\r\n\r\nggplot(akp, aes(x = t, y = oran)) +\r\n  geom_line() +\r\n  geom_point(size = 5) +\r\n  geom_text(aes(label = paste0(tarih,\"\\n\",oran,\"%\")), vjust = -0.5, size = 2) +\r\n  theme_minimal() +\r\n  theme(axis.title = element_blank(),\r\n        plot.title = element_text(face = \"bold\"),\r\n        plot.caption = element_text(face = \"italic\")) +\r\n  scale_y_continuous(limits = c(0,100)) +\r\n  labs(title = \"Genel Seçimler AKP Oy Oranları\",\r\n       caption = \"Veri: Vikipedi\")\r\n\r\ne1 <- read_html(\"https://tr.wikipedia.org/wiki/2007_T%C3%BCrkiye_genel_se%C3%A7imleri\") %>% \r\n  html_table() %>% \r\n  .[[7]] %>% \r\n  select(1,4) %>% \r\n  rename(\"il\"=1,\"akp\"=2) %>% \r\n  mutate(akp = gsub(\",\",\".\",akp),\r\n         donem = as.Date(\"2007-07-01\"))\r\n\r\ne2 <- read_html(\"https://tr.wikipedia.org/wiki/2011_T%C3%BCrkiye_genel_se%C3%A7imleri\") %>% \r\n  html_table() %>% \r\n  .[[9]] %>% \r\n  select(1,4) %>% \r\n  rename(\"il\"=1,\"akp\"=2) %>% \r\n  mutate(akp = gsub(\",\",\".\",akp),\r\n         donem = as.Date(\"2011-06-01\"))\r\n\r\ne3 <- read_html(\"https://tr.wikipedia.org/wiki/Haziran_2015_T%C3%BCrkiye_genel_se%C3%A7imleri\") %>% \r\n  html_table() %>% \r\n  .[[23]] %>% \r\n  select(1,4) %>% \r\n  rename(\"il\"=1,\"akp\"=2) %>% \r\n  mutate(akp = gsub(\",\",\".\",akp),\r\n         donem = as.Date(\"2015-06-01\"))\r\n\r\ne4 <- read_html(\"https://tr.wikipedia.org/wiki/Kas%C4%B1m_2015_T%C3%BCrkiye_genel_se%C3%A7imleri\") %>% \r\n  html_table() %>% \r\n  .[[19]] %>% \r\n  select(1,5) %>% \r\n  rename(\"il\"=1,\"akp\"=2) %>% \r\n  mutate(akp = gsub(\",\",\".\",akp),\r\n         il = gsub(\"\\\\(toplam)\",\"\",il),\r\n         donem = as.Date(\"2015-11-01\")) %>% \r\n  filter(!grepl(\"\\\\(I)|\\\\(II)|\\\\(III)\",il))\r\n    \r\ne5 <- read_html(\"https://tr.wikipedia.org/wiki/2018_T%C3%BCrkiye_genel_se%C3%A7imleri\") %>% \r\n  html_table() %>% \r\n  .[[21]] %>% \r\n  select(1,5) %>% \r\n  rename(\"il\"=1,\"akp\"=2) %>% \r\n  mutate(akp = gsub(\",\",\".\",akp)) %>% \r\n  mutate(akp = gsub(\",\",\".\",akp),\r\n         il = gsub(\"\\\\(toplam)\",\"\",il),\r\n         donem = as.Date(\"2018-06-01\")) %>% \r\n  filter(!grepl(\"\\\\(I)|\\\\(II)|\\\\(III)\",il))\r\n\r\ne <- rbind(e1,e2,e3,e4,e5) %>% \r\n  mutate(il = str_trim(il),\r\n         akp = as.numeric(akp),\r\n         il = gsub(\" Toplamı| toplamı\",\"\",il))\r\n\r\nmaster <- e %>% \r\n  left_join(tge[,c(1,2)], by = \"donem\") %>% \r\n  left_join(enf[,c(1,2)], by = \"donem\") %>% \r\n  arrange(donem)\r\n\r\nmaster %>% \r\n  filter(il == \"Türkiye\") %>% \r\n  ggplot(aes(x = tge, y = akp)) +\r\n  geom_smooth(method = \"lm\") +\r\n  geom_point(size = 5) +\r\n  geom_text(aes(label = paste0(donem,\"\\n\",akp,\"%\")), vjust = -0.7, size = 2) +\r\n  geom_text(aes(label = round(tge, digits = 1)), vjust = 2.5, size = 2) +\r\n  theme_minimal() +\r\n  labs(x = \"Tüketici Güven Endeksi\",\r\n       y = \"Oy Oranı\")\r\n\r\nmaster %>% \r\n  filter(il == \"Türkiye\") %>% \r\n  ggplot(aes(x = enf, y = akp)) +\r\n  geom_smooth(method = \"lm\") +\r\n  geom_point(size = 5) +\r\n  geom_text(aes(label = paste0(donem,\"\\n\",akp,\"%\")), vjust = -0.7, size = 2) +\r\n  geom_text(aes(label = round(enf, digits = 1)), vjust = 2.5, size = 2) +\r\n  theme_minimal() +\r\n  labs(x = \"Enflasyon\",\r\n       y = \"Oy Oranı\")\r\n\r\ntge_corr_il <- master %>% \r\n  group_by(il) %>% \r\n  summarise(corr = cor(akp,tge)) %>% \r\n  arrange(corr) %>% \r\n  mutate(tip = \"TGE\",\r\n         grup = ifelse(corr > 0, \"pozitif\", \"negatif\"))\r\n\r\nggplot(tge_corr_il, aes(x = reorder(il, corr), y = corr, fill = grup)) +\r\n  geom_col() +\r\n  theme_minimal() +\r\n  theme(axis.title = element_blank(),\r\n        axis.text.y = element_text(size = 10),\r\n        axis.text.x = element_text(size = 10),\r\n        legend.position = \"none\") +\r\n  scale_fill_manual(values = c(\"red\",\"orange\")) +\r\n  scale_y_continuous(sec.axis = sec_axis(trans=~.*1)) +\r\n  coord_flip()\r\n\r\ncorr90_tge <- tge_corr_il %>% \r\n  arrange(desc(corr)) %>% \r\n  filter(corr > 0.9 & il != \"Türkiye\") %>% \r\n  pull(il)\r\n\r\nmaster %>% \r\n  filter(il %in% corr90_tge) %>% \r\n  ggplot(aes(x = tge, y = akp)) +\r\n  geom_smooth(method = \"lm\") +\r\n  geom_point(size = 2) +\r\n  theme_minimal() +\r\n  labs(x = \"Tüketici Güven Endeksi\",\r\n       y = \"Oy Oranı\") +\r\n  facet_wrap(~il, scales = \"free\")\r\n\r\nenf_corr_il <- master %>% \r\n  group_by(il) %>% \r\n  summarise(corr = cor(akp,enf)) %>% \r\n  arrange(corr) %>% \r\n  mutate(tip = \"ENF\",\r\n         grup = ifelse(corr > 0, \"pozitif\", \"negatif\"))\r\n\r\nggplot(enf_corr_il, aes(x = reorder(il, corr), y = corr, fill = grup)) +\r\n  geom_col() +\r\n  theme_minimal() +\r\n  theme(axis.title = element_blank(),\r\n        axis.text.y = element_text(size = 10),\r\n        axis.text.x = element_text(size = 10),\r\n        legend.position = \"none\") +\r\n  scale_fill_manual(values = c(\"red\",\"orange\")) +\r\n  scale_y_continuous(sec.axis = sec_axis(trans=~.*1)) +\r\n  coord_flip()\r\n\r\ncorr60_enf <- enf_corr_il %>% \r\n  arrange(desc(corr)) %>% \r\n  filter(corr < -0.6 & il != \"Türkiye\") %>% \r\n  pull(il)\r\n\r\nmaster %>% \r\n  filter(il %in% corr60_enf) %>% \r\n  ggplot(aes(x = enf, y = akp)) +\r\n  geom_smooth(method = \"lm\") +\r\n  geom_point(size = 2) +\r\n  theme_minimal() +\r\n  labs(x = \"Enflasyon\",\r\n       y = \"Oy Oranı\") +\r\n  facet_wrap(~il, scales = \"free\")\r\n\r\ncorr_il <- rbind(tge_corr_il,enf_corr_il) %>% \r\n  select(-grup) %>% \r\n  pivot_wider(names_from = \"tip\", values_from = \"corr\")\r\n\r\ntr_enf <- corr_il %>% \r\n  filter(il == \"Türkiye\") %>% \r\n  pull(ENF)\r\n\r\ntr_tge <- corr_il %>% \r\n  filter(il == \"Türkiye\") %>% \r\n  pull(TGE)\r\n\r\nggplot(corr_il, aes(x = TGE, y = ENF)) +\r\n  geom_point(alpha = .1) +\r\n  geom_vline(xintercept = tr_tge, linetype = \"dashed\") +\r\n  geom_hline(yintercept = tr_enf, linetype = \"dashed\") +\r\n  geom_vline(xintercept = 0) +\r\n  geom_hline(yintercept = 0) +\r\n  ggrepel::geom_text_repel(data = corr_il %>% filter(il != \"Türkiye\"),\r\n                           aes(label = il), size = 5) +\r\n  ggrepel::geom_label_repel(data = corr_il %>% filter(il == \"Türkiye\"),\r\n                            aes(label = il), fill = \"red\",\r\n                            size = 5, alpha = .5, color = \"white\") +\r\n  theme_minimal() +\r\n  theme(plot.title = element_text(face = \"bold\", hjust = 0.5),\r\n        plot.subtitle = element_text(face = \"italic\"),\r\n        plot.caption = element_text(face = \"italic\"),\r\n        axis.text = element_text(size = 15),\r\n        axis.title = element_text(size = 15)) +\r\n  labs(title = \"İllere Göre Enflasyon ile Tüketici Güven Endeksinin \r\n       AKP Oy Oranıyla Korelasyonu\",\r\n       caption = \"Türkiye Genel Seçimleri \r\n       (2007,2011,2015-I,2015-II,2018) oy oranlarıdır.\\n\r\n       Veriler seçimden önceki aya aittir.\",\r\n       x = \"Tüketici Güven Endeksi Korelasyonu\",\r\n       y = \"Enflasyon Korelasyonu\")\r\n\r\n\r\n\r\nYararlandığım Kaynaklar:\r\nEconomic\r\nvoting\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-05-24-post6/post6_files/figure-html5/unnamed-chunk-17-1.png",
    "last_modified": "2022-05-24T22:22:22+03:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-05-22-post4/",
    "title": "Geometric Brownian Motion'ı Kümeleme Algoritmasıyla Kullanmak: USDTRY Simülasyon Uygulaması",
    "description": "Simüle edilmiş değerleri kümeleme algoritmasıyla kullanmak performansı iyileştirebilir.",
    "author": [
      {
        "name": "A. Uraz Akgül",
        "url": {}
      }
    ],
    "date": "2022-05-22",
    "categories": [
      "Finance",
      "Machine Learning"
    ],
    "contents": "\r\nBrownian hareketi (Brownian motion), toz parçacıkları gibi büyük\r\nparçacıkların akışkan içerisinde sıvı ya da gaz molekülleri gibi daha\r\nküçük parçacıklarla çarpışması sonucu oluşan rastlantısal harekettir.\r\nBrown hareketi literatürde Wiener süreci olarak da bilinmektedir.\r\nBrownian hareket sürecinin en basit özel durumu standart Brownian\r\nhareket sürecidir.\r\nYukarıda bahsedilen standart Brownian hareket süreci negatif değerler\r\nde alabildiği için uygulamamıza uymayacaktır. Bu noktada devreye\r\ngeometrik Brownian hareket süreci giriyor. Bu süreçte getiriler\r\nlog-normal dağılımlı olduğu için negatif durum ortadan kaldırılıyor.\r\nBunun yanında log-normal dağılım ile kalın kuyruk durumu ve çarpıklık\r\ndikkate alınmış oluyor.\r\nGeometrik Brownian hareket sürecine uyan bir finansal varlığın\r\nfiyatındaki değişim özellikle finansal modellemede \\(dS_t = \\mu S_t d_t + \\sigma S_t dW_t\\)\r\n(stokastik diferansiyel denklem) olarak ifade edilmektedir. Bu denklem\r\naşağıdaki gibi yeniden yazılabilir:\r\n\\(dlogS_t = (\\mu - \\frac{\\sigma^2}{2})dt +\r\n\\sigma dW_t\\)\r\n\\(dt\\) zaman aralığında değişim olan\r\n\\(dW_t = \\epsilon \\sqrt{dt}\\)’dir.\r\nBurada, \\(W_t\\)’deki değişme miktarı\r\nepsilon ile geçen sürenin karekökünün çarpımı; \\(\\epsilon \\sim N(0,1)\\)’dir ve \\(\\epsilon\\) ortalaması 0, standart sapması 1\r\nolan standart normal dağılım tablosundan üretilen rastsal rakamdır.\r\nSonuç olarak kullanacağımız denklem (Ito Lemma):\r\n\\(S_t = S_0 exp((\\mu -\r\n\\frac{1}{2}\\sigma^2)t + \\sigma W_t)\\) olacaktır.\r\n\\(S_0:\\) başlangıç fiyatı,\r\n\\(\\mu:\\) beklenen getiri (drift,\r\nsürüklenme),\r\n\\(\\sigma:\\) volatilitedir.\r\nUygulamayı şöyle tasarladım:\r\n\\(\\mu\\) ve \\(\\sigma\\)’nın her bir yıl için hesaplanması\r\nve test edilecek yıl için regresyon modeli kullanılarak uygun \\(\\mu\\) ve \\(\\sigma\\) değerinin kullanılması.\r\nHer bir yıl için 1000 adet simüle değerin oluşturulması ve\r\nortalamasının alınması.\r\n2018, 2019, 2020 ve 2021 için test edilmesi.\r\nAlternatif yöntem ile karşılaştırmanın yapılması.\r\nInvesting’ten aldığım verilere (post4.xlsx) GitHub\r\nhesabımdan ulaşabilirsiniz.\r\n\r\n\r\nset.seed(1) # simüle değerlerin her çalıştırmada aynı üretmesi için\r\n\r\nlibrary(tidyverse)\r\nlibrary(lubridate)\r\n\r\n\r\n\r\n\r\n\r\ndf <- readxl::read_excel(\"data.xlsx\")\r\ndf <- df %>% \r\n  mutate(\r\n    logReturn = lag(log(lead(close)/close)),\r\n    date = as.Date(date),\r\n    year = year(date)\r\n  ) %>% \r\n  na.omit()\r\n\r\n\r\n\r\n\r\n\r\n\r\nGeometrik Brownian motion’a ait fonksiyonu oluşturalım.\r\n\r\n\r\ngbmFunc <- function(nsim,t,mu,sigma,S0,nwd){ # kullanıcı tarafından girilecek değerler \r\n  \r\n  gbm <- matrix(ncol = nsim, nrow = t)\r\n  # simülasyon sayısı kadar sütun; gün sayısı kadar satır\r\n  \r\n  for(i in 1:nsim){ # i: initial (başlangıç)\r\n    \r\n    gbm[1,i] <- S0 # gbm matrisinin ilk satırları girilen başlangıç fiyatı olacak\r\n    \r\n    for(d in 2:t){ # d: day (gün); ilk satır belli olduğu için ikinci satırdan başlayacak\r\n      \r\n      epsilon <- rnorm(1) # epsilon ortalaması sıfır ve standart sapması 1 idi\r\n      \r\n      dt <- 1 / nwd # nwd: number of working days (iş günleri sayısı)\r\n      \r\n      gbm[d,i] <- gbm[(d-1),i] * exp((mu - sigma**2 / 2) * dt + sigma * epsilon * sqrt(dt))\r\n      \r\n    }\r\n    \r\n  }\r\n  \r\n  return(gbm)\r\n  \r\n}\r\n\r\n\r\n\r\nTarihsel olarak \\(\\mu\\) ve \\(\\sigma\\) değerlerine bakalım.\r\n\r\n\r\nmusigma_hist <- df %>% \r\n  group_by(year) %>% \r\n  summarise(\r\n    n = n(),\r\n    mu = mean(logReturn),\r\n    sigma = sd(logReturn)*sqrt(n) # iş günü sayısının karekökü ile yıllıklandırma\r\n  ) %>% \r\n  ungroup() %>% \r\n  mutate(t = seq(1,nrow(.),1), .before = n) %>% \r\n  select(-n)\r\n\r\n\r\n\r\n\r\n\r\n\r\nHer yıl için geçmiş 5 seneyi dikkate alarak \\(\\mu\\) ve \\(\\sigma\\) değerlerini tahmin edelim.\r\n\r\n\r\nmusigma_hist$pred_mu <- NA\r\nmusigma_hist$pred_sigma <- NA\r\n\r\ntarget <- 2018:2021\r\n\r\nfor(j in 1:length(target)) {\r\n  m_df <- musigma_hist %>%\r\n    filter(year < target[j] & year >= target[j] - 5) %>%\r\n    mutate(t = seq(1, nrow(.), 1))\r\n  \r\n  model_mu <- lm(mu ~ 0 + t, data = m_df)\r\n  model_sigma <- lm(sigma ~ 0 + t, data = m_df)\r\n  # t: trend\r\n  \r\n  pred_mu <- predict(model_mu, newdata = data.frame(t = nrow(m_df) + 1))\r\n  pred_sigma <- predict(model_sigma, newdata = data.frame(t = nrow(m_df) + 1))\r\n  \r\n  musigma_hist[which(musigma_hist$year == target[j]), ][5] <- as.numeric(pred_mu)\r\n  musigma_hist[which(musigma_hist$year == target[j]), ][6] <- as.numeric(pred_sigma)\r\n  \r\n}\r\n\r\n\r\n\r\nTahmin ettiğimiz değerlerin başarısını RMSE metriği ile ölçeceğiz.\r\nKök Ortalama Kare Hata da diyebileceğimiz Root Mean Square Error, tahmin\r\nhatalarının (residuals) standart sapmasıdır. RMSE için regresyon\r\nçizgisinin etrafındaki yoğunlaşmayı ölçüyor da diyebiliriz.\r\n\\(RMSE =\r\n\\sqrt{\\frac{\\sum_{i=1}^{N}(Tahmin_i-Gerçek_i)^2}{N}}\\)\r\n\r\n\r\nrmse <- function(predicted,actual){\r\n  \r\n  rmse_value <- sqrt(mean((predicted-actual)^2))\r\n  return(rmse_value)\r\n  \r\n}\r\n\r\nrmse_df <- data.frame(\r\n  \"year\" = 2018:2021,\r\n  \"rmse_avg\" = NA\r\n)\r\n\r\n\r\n\r\nTüm yılları hesaplayalım.\r\n\r\n\r\nsim_master <- data.frame()\r\ngbmdf_master <- data.frame()\r\n\r\nfor(k in 1:length(target)){\r\n  \r\n  df_target <- df %>% \r\n    filter(year == target[k])\r\n  \r\n  sim_target <- gbmFunc(nsim = 1000,\r\n                        t = nrow(df_target),\r\n                        mu = as.numeric(musigma_hist[(j+15),5]),\r\n                        sigma = as.numeric(musigma_hist[(j+15),6]),\r\n                        S0 = as.numeric(df_target[1,2]),\r\n                        nwd = nrow(df_target)) %>% \r\n    as.data.frame() %>% \r\n    mutate(\"simAvg\" = rowMeans(.)) %>% \r\n    mutate(rn = 1:nrow(.), .before = V1) %>% \r\n    cbind(df_target[,2]) %>% \r\n    mutate(\"year\" = target[k], .after = rn)\r\n  \r\n  sim_master <- sim_master %>% bind_rows(sim_target)\r\n  \r\n  gbmdf_target <- as.data.frame(sim_target) %>% \r\n    pivot_longer(!c(rn,year), names_to = \"sim\", values_to = \"value\")\r\n  \r\n  gbmdf_master <- gbmdf_master %>% bind_rows(gbmdf_target)\r\n  \r\n  rmse_df$rmse_avg[k] <- rmse(predicted = sim_target$simAvg, actual = sim_target$close)\r\n  \r\n}\r\n\r\n\r\n\r\n\r\n\r\n\r\n2018-2021 yılları için yaptığımız simülasyonlardan elde ettiğimiz\r\nRMSE değerleri aşağıdaki gibidir.\r\n\r\n\r\nyear\r\n\r\n\r\nrmse_avg\r\n\r\n\r\n2018\r\n\r\n\r\n1.3661915\r\n\r\n\r\n2019\r\n\r\n\r\n0.4085097\r\n\r\n\r\n2020\r\n\r\n\r\n1.3118926\r\n\r\n\r\n2021\r\n\r\n\r\n2.1905821\r\n\r\n\r\nBundan sonra asıl konumuza giriş yapacağız.\r\nTüm simüle değerlerin ortalamasını almak yerine kümeleme\r\nalgoritması kullanıp aynı kümeye düşen simüle değerlerin ortalamasını\r\nalsaydık performans ne olurdu?\r\nKümeleme için aşağıdakileri kullanacağız:\r\nKümeleme algoritması Hierarchical\r\nBağlantı yöntemi Ward\r\nUzaklık ölçütü Euclidean\r\nKümeleme Algoritması Hierarchical\r\n(Hiyerarşik)\r\nHiyerarşik kümeleme yöntemleri, ayrı ayrı ele alınan kümelerin\r\naşamalı olarak birleştirilmesi veya bir ana küme olarak ele alınarak\r\naşamalı olarak alt kümelere ayrılması esasına dayanır.\r\nBağlantı Yöntemi Ward\r\nWard, agglomerative nesting denilen birleştirici kümeleme yöntemidir.\r\nBu yöntemde amaç, bir küme oluşturabilmek için toplam varyanstaki\r\nartışların minimize edilmesidir. Bunun için küme içindeki kareli\r\ntoplamlar kullanılır ve bu değerin minimize edilmesi esas alınır.\r\n\\(ESS = \\sum_{g=1}^{G} \\sum_{i=1}^{n_g}\r\n\\sum_{j=1}^{p} (x_{ij}^g - \\overline{x}_j^g)^2\\)\r\n\\(G:\\) Kümelerin sayısı\r\n\\(n_g:\\) g. küme içindeki gözlem\r\nsayısı\r\n\\(x_{ij}^g:\\) g. küme içindeki i.\r\ngözlemin j. niteliğinin değeri\r\n\\(\\overline{x}_j^g:\\) g. küme\r\niçindeki j niteliğinin ortalaması\r\nUzaklık ölçütü Euclidean (Öklid)\r\nBu uzaklık boyutlu uzayda Pisagor teoreminin bir uygulamasıdır. A\r\nnoktası \\((x_1,y_1)\\); B noktası \\((x_2,y_2)\\) olsun.\r\nA ve B noktaları arasındaki Öklid uzaklığı:\r\n\\(d(A,B) = \\sqrt{(x_1 - x_2)^2 + (y_1 -\r\ny_2)^2}\\)\r\nAlgoritmayı uygulama aşamasına geçebiliriz. Önceki testlerimizden\r\nfarklı olarak burada simülasyonu biraz daha geriden başlatacağız. Çünkü\r\ntest edilecek yıla ait değerlerin henüz gerçekleşmediğini varsayarsak,\r\nönceki yılın bir bölümünü alıp burada gerçekleşen değerleri bir kümeye\r\ndahil etmemiz gerekir. Bunun için de test edilecek yıldan bir önceki\r\nyılın son 100 iş gününü alabiliriz. Bu durumda aşamaları şöyle\r\nsıralayabiliriz: Test edilecek yıldan bir önceki yılın son 100 iş gününü\r\nal, burayı simüle et, simüle değerleri kümelere ayır, 100 gün için\r\ngerçekleşen değerleri bir kümeye dahil et ve buradan seçilen kümeleri\r\nkullanarak test edilecek yılın değerlerini bul. 100 iş gününü seçme\r\nnedenim vade uzadıkça tahmin gücünün zayıflayabileceğini dikkate alarak\r\nçok geriden başlatıp tahmini kötüleştirmemek; bununla beraber, çok da\r\nerken başlatıp zaman serilerini kümelerken kaliteyi düşürmemek.\r\n\r\n\r\nfilter_min <- df %>% \r\n  filter(year %in% 2017:2020) %>% \r\n  group_by(year) %>% \r\n  slice(tail(row_number(), 100)) %>% \r\n  filter(date == min(date)) %>% \r\n  pull(date)\r\n\r\nfilter_max <- df %>% \r\n  filter(year %in% 2018:2021) %>% \r\n  group_by(year) %>% \r\n  filter(date == max(date)) %>% \r\n  pull(date)\r\n\r\nmlsim_master <- data.frame()\r\njoined_clusters_master <- data.frame()\r\n\r\nfor(m in 1:length(target)){\r\n  \r\n  mldf <- df %>% \r\n    filter(date >= filter_min[m] & date <= filter_max[m])\r\n  \r\n  mlsim <- gbmFunc(nsim = 1000,\r\n                   t = nrow(mldf),\r\n                   mu = as.numeric(musigma_hist[(m+15),5]),\r\n                   sigma = as.numeric(musigma_hist[(m+15),6]),\r\n                   S0 = as.numeric(mldf[1,2]),\r\n                   nwd = nrow(mldf)) %>% \r\n    as.data.frame() %>% \r\n    mutate(rn = 1:nrow(.), .before = V1) %>% \r\n    cbind(mldf[,2]) %>% \r\n    mutate(\"year\" = target[m], .after = rn)\r\n  \r\n  mlsim_master <- mlsim_master %>% bind_rows(mlsim)\r\n  \r\n  mlgbm <- t(scale(mlsim[1:100,-c(1,2)])) # son 100 günü kümelemeye dahil etmeliyiz\r\n  mlgbm_dist <- dist(mlgbm, method=\"euclidean\")\r\n  fit <- hclust(mlgbm_dist, method=\"ward.D\")\r\n  \r\n  clustered <- cutree(fit, k=50) # 50 adet küme oluşturulmuştur\r\n  clustered_tidy <- as.data.frame(as.table(clustered)) %>% \r\n    rename(\"sim\"=1,\"cluster\"=2) %>% \r\n    mutate(sim = as.character(sim))\r\n  cluster_x <- clustered_tidy %>% \r\n    filter(sim == \"close\") %>% \r\n    pull(cluster) %>% \r\n    .[[1]]\r\n  \r\n  mldf_2 <- mlsim %>% \r\n    pivot_longer(!c(rn,year), names_to = \"sim\", values_to = \"value\")\r\n  \r\n  joined_clusters <- mldf_2 %>%\r\n    inner_join(clustered_tidy, by = \"sim\") %>% \r\n    filter(cluster == cluster_x) %>% \r\n    select(-cluster) %>% \r\n    pivot_wider(names_from = \"sim\", values_from = \"value\") %>% \r\n    mutate(\"simAvg\" = apply(.[,-c(1,2,ncol(.))], 1, function(x) median(x))) %>% \r\n    # uç değerlerden etkilenmemek için ortalama yerine medyan (ortanca) kullanıldı\r\n    pivot_longer(!c(rn,year), names_to = \"sim\", values_to = \"value\")\r\n  \r\n  joined_clusters_master <- joined_clusters_master %>% bind_rows(joined_clusters)\r\n  \r\n  if(m == length(target)){\r\n    \r\n    rmse_df2 <- joined_clusters_master %>% \r\n      filter(sim %in% c(\"simAvg\",\"close\")) %>% \r\n      pivot_wider(names_from = \"sim\", values_from = \"value\") %>% \r\n      group_by(year) %>% \r\n      summarise(\"rmse_avg_cluster\" = rmse(simAvg,close)) %>% \r\n      ungroup() %>% \r\n      left_join(rmse_df, by = \"year\")\r\n    \r\n  }\r\n  \r\n}\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nyear\r\n\r\n\r\nrmse_avg_cluster\r\n\r\n\r\nrmse_avg\r\n\r\n\r\n2018\r\n\r\n\r\n1.1098768\r\n\r\n\r\n1.3661915\r\n\r\n\r\n2019\r\n\r\n\r\n0.6884056\r\n\r\n\r\n0.4085097\r\n\r\n\r\n2020\r\n\r\n\r\n1.0413089\r\n\r\n\r\n1.3118926\r\n\r\n\r\n2021\r\n\r\n\r\n1.8937623\r\n\r\n\r\n2.1905821\r\n\r\n\r\nYukarıdaki tablodan ve aşağıdaki grafikten görüleceği üzere RMSE\r\ndeğerlerini tek bir yıl hariç düşürdük ki sıfıra yaklaşması hiç tahmin\r\nhatası yapılmadığı anlamına geliyor.\r\n\r\n\r\n\r\nÇalışmada kümeleme algoritmasının simülasyonda faydalı olabileceğine\r\nodaklanmak istedim. Burada \\(\\mu\\) ve\r\n\\(\\sigma\\) değerlerinin, iş günü\r\nsayısının, küme sayısının en uygun şekilde seçimi önem kazanacaktır.\r\nGörsellere ait kodlara aşağıdan ulaşılabilir.\r\n\r\n\r\nggplot(df, aes(x = date, y = log(close), group = 1)) +\r\n  geom_line() +\r\n  theme_minimal() +\r\n  theme(axis.title = element_blank(),\r\n        plot.title = element_text(face = \"bold\"),\r\n        plot.subtitle = element_text(face = \"italic\", size = 7),\r\n        plot.caption = element_text(face = \"italic\", size = 7)) +\r\n  labs(title = \"USDTRY Günlük Değerler\",\r\n       subtitle = \"01.01.2003 - 31.12.2021\",\r\n       caption = \"Değerler logaritmiktir.\\nVeriler Investing'ten alınmıştır.\")\r\n\r\nggplot(df, aes(x = date, y = logReturn, group = 1)) +\r\n  geom_line() +\r\n  theme_minimal() +\r\n  theme(axis.title = element_blank(),\r\n        plot.title = element_text(face = \"bold\"),\r\n        plot.subtitle = element_text(face = \"italic\", size = 7),\r\n        plot.caption = element_text(face = \"italic\", size = 7)) +\r\n  labs(title = \"USDTRY Günlük Logaritmik Getiriler\",\r\n       subtitle = \"01.01.2003 - 31.12.2021\",\r\n       caption = \"Veriler Investing'ten alınmıştır.\")\r\n\r\nmusigma_hist %>% \r\n  select(-t) %>% \r\n  mutate(mu = scale(mu),\r\n         sigma = scale(sigma)) %>% \r\n  pivot_longer(!year, names_to = \"cons\", values_to = \"val\") %>% \r\n  ggplot(aes(x = factor(year), y = val, group = cons, color = cons)) +\r\n  geom_line() +\r\n  theme_minimal() +\r\n  theme(axis.title = element_blank(),\r\n        plot.subtitle = element_text(face = \"italic\", size = 7),\r\n        plot.caption = element_text(face = \"italic\", size = 7),\r\n        legend.title = element_blank(),\r\n        legend.position = \"top\") +\r\n  labs(title = expression(\"Tarihsel \"~mu~\"ve\"~sigma~\"Değerleri\"),\r\n       subtitle = \"2003-2021\",\r\n       caption = \"Veriler standardize edilmiştir.\") +\r\n  scale_color_manual(values = c(\"red\",\"blue\"))\r\n\r\nggplot(gbmdf_master, aes(x = rn, group = sim)) +\r\n  geom_line(data = gbmdf_master %>% filter(!(sim %in% c(\"simAvg\",\"close\"))),\r\n            aes(y = value), color = \"gray80\") +\r\n  geom_line(data = gbmdf_master %>% filter(sim == \"simAvg\"),\r\n            aes(y = value), color = \"red\") +\r\n  geom_line(data = gbmdf_master %>% filter(sim == \"close\"),\r\n            aes(y = value), color = \"blue\") +\r\n  facet_wrap(~year, scales = \"free\") +\r\n  theme_minimal() +\r\n  theme(axis.title = element_blank(),\r\n        plot.title = element_text(face = \"italic\", size = 7)) +\r\n  labs(title = \"Kırmızı: Ortalama Simüle, Mavi: Gerçek, Gri: Simüle\")\r\n\r\nggplot(joined_clusters_master, aes(x = rn, group = sim)) +\r\n  geom_line(data = joined_clusters_master %>% filter(!(sim %in% c(\"simAvg\",\"close\"))),\r\n            aes(y = value), color = \"gray80\") +\r\n  geom_line(data = joined_clusters_master %>% filter(sim == \"simAvg\"),\r\n            aes(y = value), color = \"red\") +\r\n  geom_line(data = joined_clusters_master %>% filter(sim == \"close\"),\r\n            aes(y = value), color = \"blue\") +\r\n  facet_wrap(~year, scales = \"free\") +\r\n  theme_minimal() +\r\n  theme(axis.title = element_blank(),\r\n        plot.title = element_text(face = \"italic\", size = 7)) +\r\n  labs(title = \"Kırmızı: Ortalama Simüle, Mavi: Gerçek, Gri: Simüle\")\r\n\r\nrmse_df2 %>% \r\n  pivot_longer(!year, names_to = \"rmse_test_type\", values_to = \"RMSE\") %>% \r\n  ggplot(aes(x = year, y = RMSE, fill = rmse_test_type)) +\r\n  geom_bar(stat = \"identity\", position = position_dodge()) +\r\n  geom_text(aes(label = round(RMSE, digits = 4)),\r\n            position = position_dodge(width = 0.9), vjust = -0.2) +\r\n  theme_minimal() +\r\n  theme(axis.title = element_blank(),\r\n        legend.title = element_blank(),\r\n        legend.position = \"top\") +\r\n  scale_fill_manual(values = c(\"gray30\",\"red\"))\r\n\r\n\r\n\r\nYararlandığım Kaynaklar:\r\nStokastik Süreçler ve R Uygulamaları; G.Ö.Kadılar\r\nBiyoenformatik DNA Mikrodizi Veri Madenciliği; Ç.S.Erol,\r\nY.Özkan\r\nRMSE:\r\nRoot Mean Square Error\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-05-22-post4/post4_files/figure-html5/unnamed-chunk-13-1.png",
    "last_modified": "2022-05-22T19:18:46+03:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-05-22-post5/",
    "title": "Twitter Verilerini Kullanarak Liderlerin Popülaritesini Ölçmek",
    "description": "Türkiye'deki bazı politik liderlerin Twitter'daki popülaritesi üzerine basit bir çalışma.",
    "author": [
      {
        "name": "A. Uraz Akgül",
        "url": {}
      }
    ],
    "date": "2022-05-22",
    "categories": [
      "Social Media"
    ],
    "contents": "\r\nTwitter geliştirici hesabı açmadan rtweet paketi ile belli\r\nsınırlarda veriler çekilebilir ve kısa vadeli analizler\r\ngerçekleştirilebilir. Bunun nasıl olabileceğini görelim.\r\nUygulamada, isim ve gündem olarak ön plana çıkmış bazı politik\r\nliderlerin tweetlerini çekip zamanla popülaritelerinde nasıl bir değişim\r\nolmuş bunu inceleyeceğiz.\r\nPopülariteyi ölçmek için birtakım indikatörleri bir araya getirmemiz\r\ngerekir ancak bu uygulamada tek bir indikatör olarak tweetlerine gelen\r\nbeğeni sayılarını alacağız.\r\n\r\n\r\n#install.packages(\"rtweet\")\r\nlibrary(rtweet)\r\nlibrary(tidyverse)\r\n\r\n\r\n\r\nBaşlamadan önce bir not: rtweet paketinin fonksiyonlarını kullanmaya\r\nbaşladığınız zaman karşınıza bir authentication (kimlik doğrulama)\r\nsayfası gelecektir. Bu noktada giriş yapmanız yeterli olacaktır. Giriş\r\nyaptıktan sonra authentication gerçekleşmiş olacaktır.\r\nAşağıdaki liderlerin tweetlerini çekerek başlayalım.\r\n\r\n\r\nleaders <- c(\r\n  \"RTErdogan\",\r\n  \"kilicdarogluk\",\r\n  \"meral_aksener\",\r\n  \"ekrem_imamoglu\",\r\n  \"mansuryavas06\",\r\n  \"umitozdag\",\r\n  \"alibabacan\",\r\n  \"Ahmet_Davutoglu\"\r\n)\r\n\r\n\r\n\r\nrtweet paketindeki get_timeline() fonksiyonu ile bir hesabın\r\ntweetlerini diğer tüm detayları ile birlikte çekebiliriz.\r\nDokümantasyonda yer alan bilgiye göre maksimum 3200 tweet\r\nçekebiliyoruz.\r\nhttps://www.rdocumentation.org/packages/rtweet/versions/0.7.0/topics/get_timeline\r\n\r\nmaster <- data.frame()\r\n\r\nfor(i in 1:length(leaders)){\r\n  \r\n  user_tl <- get_timeline(\r\n    user = leaders[i],\r\n    n = Inf, # alınabilecek maksimum tweet sayısıdır; girilmezse default 100 tane çeker\r\n  )\r\n  \r\n  # her bir hesabın çekilen tweetlerini (user_tl) master veri çerçevesinde birleştir\r\n  master <- master %>% \r\n    bind_rows(user_tl)\r\n  \r\n  Sys.sleep(time = 1) # döngü her çalıştığında 1 saniye bekletebiliriz\r\n  # sık gönderilen isteklerde problem olmaması için kullanılabilecek bir fonksiyon\r\n  \r\n}\r\n\r\n\r\n\r\nVeri çerçevesini biraz düzenleyelim.\r\n\r\n\r\ndf <- master %>% \r\n  filter(!is_retweet) %>% # retweetler kaldırıldı\r\n  select(created_at,name,favorite_count) %>% # 3 adet kolon seçildi\r\n  mutate(created_at = as.Date(created_at)) # tarih formatı düzeltildi (sadece yyyy-mm-dd)\r\n\r\n\r\n\r\nHer bir liderin 2022 yılında attığı toplam tweet ve beğeni sayısına\r\nbakalım.\r\n\r\n\r\ndf2022 <- df %>% \r\n  filter(created_at >= as.Date(\"2022-01-01\")) %>% \r\n  group_by(name) %>% \r\n  summarise(\r\n    n = n(), # toplam tweet sayısı\r\n    totalFav = sum(favorite_count) # toplam beğeni sayısı\r\n  )\r\n\r\n\r\n\r\n\r\n\r\n\r\nBir de 2022 yılına ait tweet başına düşen beğeni sayısına\r\nbakalım.\r\n\r\n\r\ndf2022 <- df2022 %>% \r\n  mutate(lpt = totalFav / n) # lpt: like per tweet (tweet başına beğeni)\r\n\r\n\r\n\r\n\r\n\r\nggplot(df2022, aes(x = reorder(name,lpt), y = lpt, fill = lpt)) +\r\n  geom_col() +\r\n  coord_flip() +\r\n  theme_minimal() +\r\n  theme(legend.position = \"none\") +\r\n  scale_y_continuous(labels = scales::comma) +\r\n  scale_fill_gradient(low = \"orange\", high = \"red\") +\r\n  labs(x = \"\", y = \"Tweet Başına Beğeni Sayısı\")\r\n\r\n\r\n\r\n\r\n2022 yılına ait günlük bazda popülaritenin zaman ile olan eğilimine\r\nbakabiliriz.\r\n\r\n\r\ndf_pop <- df %>% \r\n  filter(created_at >= as.Date(\"2022-01-01\")) %>% \r\n  group_by(name,created_at) %>% \r\n  summarise(\r\n    n = n(),\r\n    totalFav = sum(favorite_count)\r\n  ) %>% \r\n  mutate(lpt = totalFav / n) %>% \r\n  group_by(name) %>% \r\n  mutate(t = row_number()) # t: gözlem sayısı\r\n\r\n\r\n\r\n\r\n\r\nggplot(df_pop, aes(x = t, y = lpt)) +\r\n  geom_line(color = \"gray\") +\r\n  geom_point(color = \"gray40\") +\r\n  geom_smooth(method = \"loess\", color = \"red\") +\r\n  facet_wrap(~name, scales = \"free\") +\r\n  theme_minimal() +\r\n  theme(axis.title = element_blank(),\r\n        axis.text.y = element_text(size = 7),\r\n        axis.text.x = element_blank(),\r\n        plot.title = element_text(face = \"bold\"),\r\n        plot.subtitle = element_text(face = \"italic\")) +\r\n  scale_y_continuous(labels = scales::unit_format(unit = \"K\", scale = 1e-3, sep = \"\")) +\r\n  labs(title = \"Günlük Tweet Başına Beğeni Sayısı, 2022\",\r\n       subtitle = \"Günlük Toplam Beğeni / Günlük Toplam Tweet\")\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-05-22-post5/post5_files/figure-html5/unnamed-chunk-10-1.png",
    "last_modified": "2022-05-22T19:46:06+03:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-05-19-post3/",
    "title": "Faizdeki Değişimin Enflasyon Üzerindeki Etkisinin Markov Zinciri Modeli ile Analizi",
    "description": "Enflasyonun uzun dönemde yükselmesi olasılığı 70%'tir.",
    "author": [
      {
        "name": "A. Uraz Akgül",
        "url": {}
      }
    ],
    "date": "2022-05-19",
    "categories": [
      "Finance",
      "Economics"
    ],
    "contents": "\r\nMarkov süreci, bir stokastik sürecin şu anki değerleri bilindiğinde,\r\nsürecin gelecekteki değerlerinin geçmişteki değerlerinden koşullu olarak\r\nbağımsız olduğu süreçtir. Markov zinciri ise Markov sürecinin kesikli\r\ndurum uzayına sahip olduğu özel bir durumdur. Stokastik sistemlerin kısa\r\nveya uzun dönemdeki davranışlarının modellenmesinde Markov\r\nzincirlerinden yararlanılmaktadır.\r\nAylık frekansta olan ve Haziran 2018 (başkanlık sistemi) ile Nisan\r\n2022 (son veri) arasını kapsayan verileri TCMB/EVDS’den aldım. Faizi\r\ntemsilen Ağırlık Ortalama Fonlama Maliyeti’ni; enflasyonu temsilen de\r\nTÜFE’den elde edilen yıllık değişimleri kullandım. Verilere\r\n(post3.xlsx) GitHub\r\nhesabımdan ulaşabilirsiniz.\r\n\r\n\r\nlibrary(kableExtra) # zorunlu değil\r\nlibrary(expm) # %^% operatörü\r\nlibrary(markovchain) # uzun dönem denge hesaplaması\r\nlibrary(tidyverse)\r\nlibrary(ggalluvial)\r\n\r\ndf <- readxl::read_excel(\"data.xlsx\")\r\ndf <- df %>% \r\n  mutate(tarih = as.Date(paste0(tarih,\"-\",1))) %>% \r\n  filter(tarih >= as.Date(\"2018-06-01\")) %>% \r\n  na.omit() # Mayıs 2022 verisi çıkarıldı.\r\n\r\n\r\n\r\n\r\n\r\n\r\nElimizdeki durumların (State; S1, S2, …) neler olabileceğine\r\nbakalım:\r\nS1: Faiz Yükseldi - Enflasyon Yükseldi\r\nS2: Faiz Yükseldi - Enflasyon Düştü\r\nS3: Faiz Düştü - Enflasyon Yükseldi\r\nS4: Faiz Düştü - Enflasyon Düştü\r\nS5: Faiz Değişmedi - Enflasyon Yükseldi\r\nS6: Faiz Değişmedi - Enflasyon Düştü\r\n\r\n\r\ndf2 <- df %>% \r\n  mutate(\r\n    durum = case_when(\r\n      lead(faiz) > faiz & lead(enflasyon) > enflasyon ~ \"S1\",\r\n      lead(faiz) > faiz & lead(enflasyon) < enflasyon ~ \"S2\",\r\n      lead(faiz) < faiz & lead(enflasyon) > enflasyon ~ \"S3\",\r\n      lead(faiz) < faiz & lead(enflasyon) < enflasyon ~ \"S4\",\r\n      lead(faiz) == faiz & lead(enflasyon) > enflasyon ~ \"S5\",\r\n      lead(faiz) == faiz & lead(enflasyon) < enflasyon ~ \"S6\",\r\n    ) # lead() ---> bir sonraki durum; t + 1 ile t karşılaştırılıyor\r\n  ) %>% \r\n  na.omit() %>% # NA içeren değerler çıkarıldı\r\n  select(durum) # yeterli olacak sütun\r\n\r\n\r\n\r\nKesikli parametreli Markov zincirlerinde sistem belirli bir olasılık\r\ndağılımına bağlı olarak bulunduğu durumdan başka bir duruma geçebilir\r\nveya aynı durumda kalabilir. Bu nedenle, incelenen sistemin içinde\r\nbulunabileceği farklı durumların ve bu durumların birinden diğerine\r\ngeçiş olasılıklarını bilmemiz gerekir. Sistemin bu durum değişiklikleri\r\ngeçiş olarak isimlendirilir ve durum uzayındaki herhangi i ve j\r\ndurumları için, \\(p_{ij}(m,n) = P(X_{m+n} =\r\nj/X_m = i)\\) biçimindeki koşullu olasılığa geçiş olasılığı adı\r\nverilir. Ayrıca, homojen bir Markov zincirinde geçiş olasılığı yalnızca\r\nadım sayısının bir fonksiyonudur ve m zamanına bağlı değildir.\r\nYukarıdaki durumları baz alarak Markov geçiş matrisini\r\noluşturalım.\r\n\r\n\r\nmgm <- df2 %>% \r\n  mutate(\r\n    durum2 = lead(durum)\r\n  ) %>% # hangi durumdan hangi duruma geçti\r\n  na.omit() %>% \r\n  group_by(durum,durum2) %>% \r\n  summarise(n = n()) %>% \r\n  ungroup()\r\n\r\n\r\n\r\nTabloda olmayan bazı durumlar var. Bunları tüm durumları baz alarak\r\nekleyebiliriz.\r\n\r\n\r\ntumDurumlar <- data.frame(\r\n  durum = paste0(\"S\",seq(1,6,1)), # tüm durumlar\r\n  durum2 = paste0(\"S\",seq(1,6,1)) # tüm durumlar 2\r\n) %>% \r\n  expand(durum,durum2) %>% # tüm durumlar burada genişletiliyor\r\n  left_join(mgm, by = c(\"durum\",\"durum2\")) %>% \r\n  mutate(n = replace(n,is.na(n),0))\r\n\r\n\r\n\r\n\r\n\r\n\r\nMatrisi oluşturabiliriz. Her bir hücreyi o satırın toplamına bölerek\r\nolasılıkları hesaplayacağız.\r\n\r\n\r\nm <- tumDurumlar %>% \r\n  pivot_wider(names_from = \"durum2\", values_from = \"n\") %>% # sütun\r\n  column_to_rownames(var = \"durum\") %>% # satır\r\n  mutate_all(.funs = function(x) round(x / rowSums(.), digits = 2)) %>% \r\n  mutate_if(is.numeric, funs(ifelse(is.nan(.), 0, .))) %>% \r\n  as.matrix()\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nS1\r\n\r\n\r\nS2\r\n\r\n\r\nS3\r\n\r\n\r\nS4\r\n\r\n\r\nS5\r\n\r\n\r\nS6\r\n\r\n\r\nS1\r\n\r\n\r\n0.38\r\n\r\n\r\n0.12\r\n\r\n\r\n0.25\r\n\r\n\r\n0.12\r\n\r\n\r\n0.12\r\n\r\n\r\n0.00\r\n\r\n\r\nS2\r\n\r\n\r\n0.50\r\n\r\n\r\n0.00\r\n\r\n\r\n0.25\r\n\r\n\r\n0.25\r\n\r\n\r\n0.00\r\n\r\n\r\n0.00\r\n\r\n\r\nS3\r\n\r\n\r\n0.07\r\n\r\n\r\n0.07\r\n\r\n\r\n0.47\r\n\r\n\r\n0.13\r\n\r\n\r\n0.13\r\n\r\n\r\n0.13\r\n\r\n\r\nS4\r\n\r\n\r\n0.00\r\n\r\n\r\n0.14\r\n\r\n\r\n0.43\r\n\r\n\r\n0.43\r\n\r\n\r\n0.00\r\n\r\n\r\n0.00\r\n\r\n\r\nS5\r\n\r\n\r\n0.12\r\n\r\n\r\n0.00\r\n\r\n\r\n0.12\r\n\r\n\r\n0.00\r\n\r\n\r\n0.62\r\n\r\n\r\n0.12\r\n\r\n\r\nS6\r\n\r\n\r\n0.33\r\n\r\n\r\n0.33\r\n\r\n\r\n0.00\r\n\r\n\r\n0.00\r\n\r\n\r\n0.33\r\n\r\n\r\n0.00\r\n\r\n\r\nYukarıdaki matris bir-adım geçiş olasılığı matrisidir. Homojen bir\r\nMarkov zincirinde m. adımda i durumunda bulunan sürecin bir adım sonra j\r\ndurumunda bulunması olasılığı \\(p_{ij} =\r\np_{ij}(1) = p_{ij}^{(1)} = P(X_{m+1} = j/X_m = i)\\)’dir.\r\nYukarıdaki bir-adım geçiş matrisinde satırlar sistemin şu an\r\nbulunabileceği durumları; sütunlar ise bir adım sonra bulunabileceği\r\ndurumları göstermektedir.\r\nBaşlangıç durumu S5’tir. Yani, faiz değişmedi - enflasyon yükseldi.\r\nGelecek Mayıs ayına ait olasılıklara bakalım.\r\n\r\n\r\nbaslangicDurum <- matrix(\r\n  data = c(0,0,0,0,1,0), # S5 ---> 1; diğerleri 0\r\n  nrow = 1,\r\n  byrow = TRUE\r\n)\r\n\r\n\r\n\r\n\r\n\r\nbaslangicDurum %*% m\r\n\r\n\r\n\r\n\r\n\r\nS1\r\n\r\n\r\nS2\r\n\r\n\r\nS3\r\n\r\n\r\nS4\r\n\r\n\r\nS5\r\n\r\n\r\nS6\r\n\r\n\r\n0.12\r\n\r\n\r\n0\r\n\r\n\r\n0.12\r\n\r\n\r\n0\r\n\r\n\r\n0.62\r\n\r\n\r\n0.12\r\n\r\n\r\nFaizin değişmediği ve enflasyonun yükseldiği bir dönemden yine faizin\r\ndeğişmediği ve enflasyonun yükseldiği bir döneme geçiş olasılığı\r\n62%’dir.\r\nPeki, Haziran ayı için (iki-adım geçiş) olasılıklar ne olacaktır?\r\n\r\n\r\nm %^% 2 %>% as.matrix() %>% round(., digits = 2)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nS1\r\n\r\n\r\nS2\r\n\r\n\r\nS3\r\n\r\n\r\nS4\r\n\r\n\r\nS5\r\n\r\n\r\nS6\r\n\r\n\r\nS1\r\n\r\n\r\n0.24\r\n\r\n\r\n0.08\r\n\r\n\r\n0.31\r\n\r\n\r\n0.16\r\n\r\n\r\n0.15\r\n\r\n\r\n0.05\r\n\r\n\r\nS2\r\n\r\n\r\n0.21\r\n\r\n\r\n0.11\r\n\r\n\r\n0.35\r\n\r\n\r\n0.20\r\n\r\n\r\n0.09\r\n\r\n\r\n0.03\r\n\r\n\r\nS3\r\n\r\n\r\n0.15\r\n\r\n\r\n0.10\r\n\r\n\r\n0.33\r\n\r\n\r\n0.14\r\n\r\n\r\n0.19\r\n\r\n\r\n0.08\r\n\r\n\r\nS4\r\n\r\n\r\n0.10\r\n\r\n\r\n0.09\r\n\r\n\r\n0.42\r\n\r\n\r\n0.28\r\n\r\n\r\n0.06\r\n\r\n\r\n0.06\r\n\r\n\r\nS5\r\n\r\n\r\n0.17\r\n\r\n\r\n0.06\r\n\r\n\r\n0.16\r\n\r\n\r\n0.03\r\n\r\n\r\n0.45\r\n\r\n\r\n0.09\r\n\r\n\r\nS6\r\n\r\n\r\n0.33\r\n\r\n\r\n0.04\r\n\r\n\r\n0.20\r\n\r\n\r\n0.12\r\n\r\n\r\n0.24\r\n\r\n\r\n0.04\r\n\r\n\r\nMarkov zincirinde ilk olasılık dağılımı ve zincirin bir-adım geçiş\r\nmatrisini belirlediğimizde zincirin tüm adımlarındaki olasılıkları\r\nkolayca elde edebiliriz. n-adım olasılıklı vektörü \\(\\pi_{n} = \\pi_{n-1}P\\) olsun. Buradan\r\naşağıdaki eşitlikleri türetebiliriz.\r\n\\(\\pi_n = (\\pi_{n-2}P)P =\r\n\\pi_{n-2}P^2\\)\r\n.\r\n.\r\n.\r\n\\(\\pi_n = \\pi_0P^n\\)\r\nP geçiş matrisinin n. kuvvetini aldığımızda elde edeceğimiz n-adım\r\ngeçiş matrisi \\(P^n\\)’de n değeri\r\nbüyüdükçe \\(P_{ij}^{(n)}\\) olasılık\r\ndeğerleri sabit bir değere ya da limite yaklaşıyorsa, n-adım geçiş\r\nolasılıkları denge durumuna ulaşır. Denge durumuna ulaştığımızda geçiş\r\nmatrisinin satırlarında değişim olmadığını göreceğiz. Yani, her satırı\r\naynı olan bir geçiş matrisi elde edeceğiz.\r\n\\(\\pi_n = [\\pi_0 \\pi_1 ...\r\n\\pi_n]\\)\r\n\r\n\r\nm <- tumDurumlar %>% \r\n  pivot_wider(names_from = \"durum2\", values_from = \"n\") %>% # sütun\r\n  column_to_rownames(var = \"durum\") %>% # satır\r\n  mutate_all(.funs = function(x) x / rowSums(.)) %>% \r\n  # satır toplamı 1 olması için round() kaldırıldı\r\n  mutate_if(is.numeric, funs(ifelse(is.nan(.), 0, .))) %>% \r\n  as.matrix()\r\n\r\nn_adim <- new(\r\n  \"markovchain\",\r\n  transitionMatrix = matrix(as.numeric(m), nrow = 6, ncol = 6)\r\n)\r\n\r\n\r\n\r\n\r\n\r\nsteadyStates(n_adim) %>% round(. ,digits = 2)\r\n\r\n\r\n\r\n\r\n\r\n1\r\n\r\n\r\n2\r\n\r\n\r\n3\r\n\r\n\r\n4\r\n\r\n\r\n5\r\n\r\n\r\n6\r\n\r\n\r\n0.18\r\n\r\n\r\n0.09\r\n\r\n\r\n0.29\r\n\r\n\r\n0.15\r\n\r\n\r\n0.23\r\n\r\n\r\n0.07\r\n\r\n\r\nUzun dönemde;\r\nFaizin yükseldiği ve enflasyonun yükseldiği dönem olasılığı\r\n18%,\r\nFaizin yükseldiği ve enflasyonun düştüğü dönem olasılığı\r\n9%,\r\nFaizin düştüğü ve enflasyonun yükseldiği dönem olasılığı\r\n29%,\r\nFaizin düştüğü ve enflasyonun düştüğü dönem olasılığı\r\n15%,\r\nFaizin değişmediği ve enflasyonun yükseldiği dönem olasılığı\r\n23%,\r\nFaizin değişmediği ve enflasyonun düştüğü dönem olasılığı\r\n7%’dir.\r\nDeğerlerin toplamı yuvarlamadan dolayı 100% değildir.\r\nYukarıdaki olasılıkları baz aldığımızda uzun vadede enflasyonun\r\nyükselmesi olasılığı 70% iken; düşmesi olasılığı 30%’dur.\r\nGörsellere ait kodlara aşağıdan ulaşılabilir.\r\n\r\n\r\ndf %>% \r\n  rename(\"AOFM\"=2, \"Enflasyon\"=3) %>% \r\n  pivot_longer(!tarih, names_to = \"variables\", values_to = \"values\") %>% \r\n  ggplot(aes(x = tarih, y = log(values), group = variables, color = variables)) +\r\n  geom_line() +\r\n  theme_minimal() +\r\n  theme(axis.title = element_blank(),\r\n        legend.title = element_blank(),\r\n        legend.position = \"top\",\r\n        plot.title = element_text(face = \"bold\"),\r\n        plot.subtitle = element_text(face = \"italic\", size = 7),\r\n        plot.caption = element_text(face = \"italic\", size = 5)) +\r\n  scale_color_manual(values = c(\"blue\",\"red\")) +\r\n  labs(title = \"Ağırlıklı Ortalama Fonlama Faizi ve Enflasyon (YoY)\",\r\n       subtitle = \"Ocak 2011 - Nisan 2022\",\r\n       caption = \"Değerler logaritmiktir.\\nVeriler TCMB/EVDS'den alınmıştır.\")\r\n\r\nggplot(data = tumDurumlar, aes(axis1 = durum, axis2 = durum2, y = n)) +\r\n  geom_alluvium(aes(fill = durum2)) +\r\n  geom_stratum(fill = \"gray15\") +\r\n  geom_text(stat = \"stratum\", aes(label = after_stat(stratum)), color = \"white\") +\r\n  scale_x_discrete(limits = c(\"durum\", \"durum2\"), expand = c(0.15, 0.05)) +\r\n  theme_void() + \r\n  theme(legend.position = \"none\") +\r\n  scale_fill_viridis_d()\r\n\r\n\r\n\r\nYararlandığım Kaynaklar:\r\nStokastik Süreçler ve R Uygulamaları; G.Ö.Kadılar\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-05-19-post3/post3_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2022-05-29T01:32:02+03:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-05-04-post2/",
    "title": "Merkez Bankası Rezervleri Nasıl Hesaplanır?",
    "description": "Bir fonksiyon ile güncel rezerv değerine kolay erişim.",
    "author": [
      {
        "name": "A. Uraz Akgül",
        "url": {}
      }
    ],
    "date": "2022-05-04",
    "categories": [
      "Finance"
    ],
    "contents": "\r\nRezervlerin R’da nasıl hesaplanabileceğini göstermek için Mahfi\r\nEğilmez’in Rezerv\r\nMeselesi başlıklı bulabildiğim en güncel yazısından faydalandım. En\r\ngüncel diyorum çünkü diğer yazılarına göre farklılıklar gördüm ve en\r\ngüncelin en doğru olabileceğini düşündüm. Tabi ki hesaplama ile ilgili\r\ngörüş farklılıkları çıkacaktır ancak bu tartışma bu yazının konusu\r\ndeğildir.\r\nÜç ana konuyu öğretmeyi hedefliyorum: 1) TCMB’den veri nasıl çekilir?\r\n2) R’da bu hesaplamayı nasıl otomatik hale getirebiliriz? 3)\r\nRezervlerimiz nasıl hesaplanıyor?\r\nTCMB EVDS sistemine üye nasıl olunur ve sistemden API\r\nanahtarı nasıl alınır?\r\nTCMB’nin Elektronik Veri Dağıtım Sistemi olan EVDS’ye buradan ulaşabilirsiniz.\r\nDokümantasyon okumayı alışkanlık haline getirmeliyiz. Bunun için sayfada\r\nyer alan Kullanıcı\r\nDokümanları başlığına tıklayıp Web\r\nServis Kullanım Kılavuzu’na erişiyoruz.\r\nTCMB bu doküman ile web servis metotlarının parametrelerini\r\naçıklamış. Bu sayfa şimdilik kalsın.\r\nYine aynı sayfada yer alan Giriş Yap\r\nbölümüne giriyoruz. Daha önce kayıt olmadıysanız Kayıt Olun\r\nseçeneği ile kayıt işleminizi yapabilirsiniz. Ardından kullanıcı\r\nadı, parola ve doğrulama kodu ile giriş\r\nyapılabilir. Bundan sonra bir API anahtarına ihtiyacınız olacak. Bunun\r\niçin de isminiz ve soyisminizin yer aldığı yerden profilinize gidip\r\nAPI Anahtarı butonuna tıklayabilirsiniz. Tarayıcınızda çıkan\r\nbir mesaj ile API anahtarınız verilecek. Bunu kopyalayın ya da aşağıdaki\r\ngibi bir objeye kaydedin.\r\n\r\n\r\napi_key <- \"api_anahtariniz\"\r\n\r\n\r\n\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(jsonlite)\r\n\r\n\r\n\r\nBaşlamadan bir not: Önce detaylı bir şekilde açıklayacağım,\r\nsonrasında öğrendiklerimizi tek bir fonksiyonda toplayıp vereceğim.\r\nBaşlamadan diğer bir not: Önce yazıda hesaplanan değerlere ulaşıp bir\r\nnevi değerleri teyit edeceğiz; ardından güncel rezervi\r\nhesaplayacağız.\r\nRezervleri Hesaplamak İçin Denklemler:\r\nNet Rezervler = (Dış Varlıklar / O günkü TCMB USD Alış Kuru) - (Döviz\r\nYükümlülükleri / O günkü TCMB USD Alış Kuru)\r\nSwaplar = Döviz Swapları + Altın Swapları\r\nSwap Hariç Net Rezervler = Net Rezervler - Swaplar\r\nNet Rezervler\r\nNet Rezervler = (Dış Varlıklar / O günkü TCMB USD Alış Kuru) - (Döviz\r\nYükümlülükleri / O günkü TCMB USD Alış Kuru)\r\nEVDS’deki yeri: Tüm Seriler > TCMB Bilanço Verileri > Merkez\r\nBankası Analitik Bilanço (Bin TL)\r\nWeb Servis ile almak için aşağıdaki adımları takip edebiliriz.\r\nKategorileri görelim.\r\nDokümanda 4 no’lu EVDS Metaveri web servisleri başlığı\r\naltındaki 4.1 no’lu Konu Başlığı Servisi alt başlığında\r\nbulunmaktadır.\r\nTüm konu başlıklarını sunan bir servistir. Bu servisi temsilen\r\n“https://evds2.tcmb.gov.tr/service/evds/” tanımından\r\nsonra “categories” eklenmelidir.\r\nhttps://evds2.tcmb.gov.tr/service/evds/categories/key=XXXXXX&type=xml\r\nUzantısı xml olsa da biz bunu json ile alabiliriz (json için jsonlite\r\npaketinin fromJSON() fonksiyonundan faydalanacağız).\r\n\r\n\r\nurl_kategori <- paste0(\"https://evds2.tcmb.gov.tr/service/evds/categories/key=\",\r\n                       api_key,\r\n                       \"&type=json\")\r\n\r\ndf_kategori <- fromJSON(url_kategori) %>%\r\n  as.data.frame() %>%\r\n  select(CATEGORY_ID,TOPIC_TITLE_TR)\r\n\r\nkategori_id <- df_kategori %>% \r\n  filter(TOPIC_TITLE_TR == 'TCMB BİLANÇO VERİLERİ') %>% \r\n  pull(CATEGORY_ID)\r\n\r\n\r\n\r\n2 sütun ve 24 satırdan oluşan df_kategori objesini aldık. TCMB\r\nBilanço Verileri’nin kategori ID’si 13’tür.\r\nVeri gruplarını görelim.\r\nDokümanda 4 no’lu EVDS Metaveri web servisleri başlığı\r\naltındaki 4.2 no’lu Veri Grubu Servisi alt başlığında\r\nbulunmaktadır.\r\nİlgili Konu başlığı bazında ya da tek bir veri grubunun metaveri\r\nbilgilerini listeleyen bir servistir. Bu servisi temsilen “https://evds2.tcmb.gov.tr/service/evds/” tanımından\r\nsonra “datagroups” eklenmelidir.\r\nhttps://evds2.tcmb.gov.tr/service/evds/datagroups/key=XXXX&mode=1&code=bie_yssk&type=json\r\nhttps://evds2.tcmb.gov.tr/service/evds/datagroups/key=XXXX&mode=2&code=2&type=xml\r\nVerilen örneklerde mode=1 ve mode=2 olarak verilse de biz mode=0\r\nolarak kullanacağız. Çünkü tüm konu başlıkları altındaki tüm veri\r\ngruplarını göreceğiz.\r\n\r\n\r\nurl_verigrubu <- paste0(\"https://evds2.tcmb.gov.tr/service/evds/datagroups/key=\",\r\n                        api_key,\r\n                        \"&mode=0&type=json\")\r\n\r\ndf_verigrubu <- fromJSON(url_verigrubu) %>% \r\n  as.data.frame() %>% \r\n  filter(CATEGORY_ID == kategori_id) %>% \r\n  select(CATEGORY_ID,\r\n         DATAGROUP_CODE,\r\n         DATAGROUP_NAME,\r\n         FREQUENCY_STR,\r\n         NOTE,\r\n         START_DATE,\r\n         END_DATE)\r\n\r\nverigrubu_kodu <- df_verigrubu %>% \r\n  filter(DATAGROUP_NAME == 'Merkez Bankası Analitik Bilanço(Bin TL)') %>% \r\n  pull(DATAGROUP_CODE)\r\n\r\n\r\n\r\n7 sütun ve 6 satırdan oluşan df_verigrubu objesini aldık. Merkez\r\nBankası Analitik Bilanço(Bin TL)’nin veri grubu kodu\r\nbie_abanlbil’dir.\r\nSerileri görelim.\r\nDokümanda 4 no’lu EVDS Metaveri web servisleri başlığı\r\naltındaki 4.3 no’lu Seri Listesi Servisi alt başlığında\r\nbulunmaktadır.\r\nSeri listesini veri grubu ya da seri kodu bazında sunan bir\r\nservistir. Bu servisi temsilen “https://evds2.tcmb.gov.tr/service/evds/” tanımından\r\nsonra “serieList” eklenmelidir.\r\nhttps://evds2.tcmb.gov.tr/service/evds/serieList/key=XXXXX&type=xml&code=TP.DK.USD.A\r\nhttps://evds2.tcmb.gov.tr/service/evds/serieList/key=XXXXX&type=csv&code=bie_yssk\r\nBiz kodumuzu bildiğimiz için örnekte verilen code= kısmına kendi\r\nkodumuzu yazacağız.\r\n\r\n\r\nurl_seri <- paste0(\"https://evds2.tcmb.gov.tr/service/evds/serieList/key=\",\r\n                   api_key,\r\n                   \"&type=json&code=\",\r\n                   verigrubu_kodu)\r\n\r\ndf_seri <- fromJSON(url_seri) %>% \r\n  as.data.frame() %>% \r\n  select(SERIE_CODE,\r\n         DATAGROUP_CODE,\r\n         SERIE_NAME,\r\n         FREQUENCY_STR,\r\n         DEFAULT_AGG_METHOD_STR,\r\n         DEFAULT_AGG_METHOD,\r\n         START_DATE,\r\n         END_DATE)\r\n\r\nseri <- df_seri %>% \r\n  filter(SERIE_NAME %in% c('A.1-DIŞ VARLIKLAR(Bin TL)',\r\n                           'P.1-TOPLAM DÖVİZ YÜKÜMLÜLÜKLERİ(Bin TL)')) %>% \r\n  pull(SERIE_CODE)\r\n\r\n\r\n\r\n8 sütun ve 31 satırdan oluşan df_seri objesini aldık. A.1-DIŞ\r\nVARLIKLAR(Bin TL)’nin seri kodu TP.AB.A02; P.1-TOPLAM DÖVİZ\r\nYÜKÜMLÜLÜKLERİ(Bin TL)’nin seri kodu TP.AB.A10’dur.\r\nSon olarak verileri alabiliriz. URL olarak temelde https://evds2.tcmb.gov.tr/service/evds/series=\r\nkullanacağız. Sonrası ise parametrelerin eklenmesi olacak. Dokümanda\r\nbununla ilgili açıklayıcı örnekler bulunmaktadır.\r\nAşağıdaki örnekte url’den sonra seri kodlarının arasına “-” koyduk.\r\nYani, birden fazla seriyi alabiliyoruz. Devamında ise başlangıç ve bitiş\r\ntarihlerine sırasıyla &startDate= ve &endDate= ile adres\r\ngösterdik. Bunlara ek olarak url’e frekans tipini ekleyeceğiz çünkü bu\r\nseri default iş günü olarak geliyor ve bunu haftalık yapmamız gerekir.\r\nEklenecek olan parametre frequency ve haftalık olduğu için 3\r\ndiyeceğiz.\r\n\r\n\r\nhaftalikTarih <- \"26-02-2021\"\r\nfrekans <- 3\r\n\r\nurl_veri <- paste0(\"https://evds2.tcmb.gov.tr/service/evds/series=\",\r\n                   seri[1],\r\n                   \"-\",\r\n                   seri[2],\r\n                   \"&startDate=\",\r\n                   haftalikTarih,\r\n                   \"&endDate=\",\r\n                   haftalikTarih,\r\n                   \"frequency=\",\r\n                   frekans,\r\n                   \"&type=json&key=\",\r\n                   api_key)\r\n\r\ndf_veri <- fromJSON(url_veri) %>% \r\n  as.data.frame() %>% \r\n  select(2,3,4) %>% \r\n  rename(\r\n    \"Tarih\"=1,\r\n    \"A.1-DIŞ VARLIKLAR(Bin TL)\"=2,\r\n    \"P.1-TOPLAM DÖVİZ YÜKÜMLÜLÜKLERİ(Bin TL)\"=3\r\n  )\r\n\r\n\r\n\r\nDış varlıklardan toplam döviz yükümlülüklerini çıkaracağız ancak\r\nçıkacak olan fark TL cinsinden olduğu için bunu dolara çevirmemiz\r\ngerekiyor. Bunu da o güne ait kur üzerinden yapacağız (yazıda hesaplama\r\nyapılırken 15:30’da belirlenen gösterge niteliğindeki Merkez Bankası\r\nkuru kullanılıyor; biz ise EVDS sisteminden aldığımız kuru\r\nkullanacağız).\r\n26 Şubat 2021’deki USD/TL kur değerine ulaşalım. Kurlar’ın bulunduğu\r\nkategorinin ID’si 2’dir. Bunun yanında Kurlar-Döviz Kurları’nın veri\r\ngrubu kodu bie_dkdovytl; (USD) ABD Doları (Döviz Alış)’ın seri kodu\r\nTP.DK.USD.A.YTL’dir.\r\n\r\n\r\nkategori_id_2 <- df_kategori %>% \r\n  filter(TOPIC_TITLE_TR == 'KURLAR') %>% \r\n  pull(CATEGORY_ID)\r\n\r\ndf_verigrubu_2 <- fromJSON(url_verigrubu) %>% \r\n  as.data.frame() %>% \r\n  filter(CATEGORY_ID == kategori_id_2) %>% \r\n  select(CATEGORY_ID,\r\n         DATAGROUP_CODE,\r\n         DATAGROUP_NAME,\r\n         FREQUENCY_STR,\r\n         NOTE,\r\n         START_DATE,\r\n         END_DATE)\r\n\r\nverigrubu_kodu_2 <- df_verigrubu_2 %>% \r\n  filter(DATAGROUP_NAME == 'Kurlar-Döviz Kurları') %>% \r\n  pull(DATAGROUP_CODE)\r\n\r\nurl_seri_2 <- paste0(\"https://evds2.tcmb.gov.tr/service/evds/serieList/key=\",\r\n                     api_key,\r\n                     \"&type=json&code=\",\r\n                     verigrubu_kodu_2)\r\n\r\ndf_seri_2 <- fromJSON(url_seri_2) %>% \r\n  as.data.frame() %>% \r\n  select(SERIE_CODE,\r\n         DATAGROUP_CODE,\r\n         SERIE_NAME,\r\n         FREQUENCY_STR,\r\n         DEFAULT_AGG_METHOD_STR,\r\n         DEFAULT_AGG_METHOD,\r\n         START_DATE,\r\n         END_DATE)\r\n\r\nseri_2 <- df_seri_2 %>% \r\n  filter(SERIE_NAME == '(USD) ABD Doları (Döviz Alış)') %>% \r\n  pull(SERIE_CODE)\r\n\r\nurl_veri_2 <- paste0(\"https://evds2.tcmb.gov.tr/service/evds/series=\",\r\n                     seri_2,\r\n                     \"&startDate=\",\r\n                     haftalikTarih,\r\n                     \"&endDate=\",\r\n                     haftalikTarih,\r\n                     \"&type=json&key=\",\r\n                     api_key)\r\n\r\ndf_veri_2 <- fromJSON(url_veri_2) %>% \r\n  as.data.frame() %>% \r\n  select(2,3) %>% \r\n  rename(\r\n    \"Tarih\"=1,\r\n    \"(USD) ABD Doları (Döviz Alış)\"=2\r\n  )\r\n\r\n\r\n\r\nNet Rezerv aşağıdaki gibi hesaplanabilir.\r\n\r\n\r\n# Birimler Bin TL'dir.\r\n\r\ndis_varliklar <- as.numeric(df_veri$`A.1-DIŞ VARLIKLAR(Bin TL)`)\r\ndoviz_yukumluluk <- as.numeric(df_veri$`P.1-TOPLAM DÖVİZ YÜKÜMLÜLÜKLERİ(Bin TL)`)\r\nkur <- as.numeric(df_veri_2$`(USD) ABD Doları (Döviz Alış)`)\r\n\r\nnet_rezerv <- (dis_varliklar - doviz_yukumluluk) / kur\r\n\r\n\r\n\r\nNet Rezerv = -2247086 ya da -2,2 milyar dolar.\r\nSwaplar\r\n\r\n\r\n\r\nSwaplar = Döviz Swapları + Altın Swapları\r\nEVDS’deki yeri: Tüm Seriler > Ödemeler Dengesi, Uluslararası\r\nYatırım Pozisyonu > Uluslararası Rezervler ve Döviz Likiditesi\r\nTablosu (Milyon ABD Doları)\r\nWeb Servis ile almak için:\r\nÖDEMELER DENGESİ, ULUSLARARASI YATIRIM POZİSYONU’nun bulunduğu\r\nkategorinin ID’si 18’dir. Bunun yanında Uluslararası Rezervler ve Döviz\r\nLikiditesi Tablosu (Milyon ABD Doları)’nın veri grubu kodu\r\nbie_ulusdovlkd; II.2. Yurt içi para karşılığında döviz forward ve\r\nfuture’ların toplam kısa ve fazla pozisyon büyüklükleri (para\r\nswaplarının gelecekteki bacağını da kapsar.) (Milyon ABD Doları)’nın ve\r\nII.3. Diğer (Milyon ABD Doları)’nın seri kodları sırasıyla\r\nTP.DOVVARNC.K14 ve TP.DOVVARNC.K23’tür.\r\n\r\n\r\nkategori_id_3 <- df_kategori %>% \r\n  filter(TOPIC_TITLE_TR == 'ÖDEMELER DENGESİ, ULUSLARARASI YATIRIM POZİSYONU') %>% \r\n  pull(CATEGORY_ID)\r\n\r\ndf_verigrubu_3 <- fromJSON(url_verigrubu) %>% \r\n  as.data.frame() %>% \r\n  filter(CATEGORY_ID == kategori_id_3) %>% \r\n  select(CATEGORY_ID,\r\n         DATAGROUP_CODE,\r\n         DATAGROUP_NAME,\r\n         FREQUENCY_STR,\r\n         NOTE,\r\n         START_DATE,\r\n         END_DATE)\r\n\r\nverigrubu_kodu_3 <- df_verigrubu_3 %>% \r\n  filter(DATAGROUP_NAME == 'Uluslararası Rezervler ve Döviz Likiditesi Tablosu (Milyon ABD Doları)') %>% \r\n  pull(DATAGROUP_CODE)\r\n\r\nurl_seri_3 <- paste0(\"https://evds2.tcmb.gov.tr/service/evds/serieList/key=\",\r\n                     api_key,\r\n                     \"&type=json&code=\",\r\n                     verigrubu_kodu_3)\r\n\r\ndf_seri_3 <- fromJSON(url_seri_3) %>% \r\n  as.data.frame() %>% \r\n  select(SERIE_CODE,\r\n         DATAGROUP_CODE,\r\n         SERIE_NAME,\r\n         FREQUENCY_STR,\r\n         DEFAULT_AGG_METHOD_STR,\r\n         DEFAULT_AGG_METHOD,\r\n         START_DATE,\r\n         END_DATE)\r\n\r\nuzun_vektor <- paste0(\r\n  \"II.2. Yurt içi para karşılığında döviz forward ve future'ların \",\r\n  \"toplam kısa ve fazla pozisyon büyüklükleri \",\r\n  \"(para swaplarının gelecekteki bacağını da kapsar.) (Milyon ABD Doları)\"\r\n)\r\n\r\nseri_3 <- df_seri_3 %>% \r\n  filter(SERIE_NAME %in% c(uzun_vektor,\r\n                           'II.3. Diğer (Milyon ABD Doları)')) %>% \r\n  pull(SERIE_CODE)\r\n\r\naylikTarih <- \"01-02-2021\"\r\nfrekans_2 <- 5\r\n\r\nurl_veri_3 <- paste0(\"https://evds2.tcmb.gov.tr/service/evds/series=\",\r\n                     seri_3[1],\r\n                     \"-\",\r\n                     seri_3[2],\r\n                     \"&startDate=\",\r\n                     aylikTarih,\r\n                     \"&endDate=\",\r\n                     aylikTarih,\r\n                     \"&frequency=\",\r\n                     frekans_2,\r\n                     \"&type=json&key=\",\r\n                     api_key)\r\n\r\ndf_veri_3 <- fromJSON(url_veri_3) %>% \r\n  as.data.frame() %>% \r\n  select(2,3,4) %>% \r\n  rename(\r\n    \"Tarih\"=1,\r\n    \"II.2. Yurt içi para karşılığında...\"=2,\r\n    \"II.3. Diğer (Milyon ABD Doları)\"=3\r\n  )\r\n\r\n\r\n\r\nYukarıda veriyi aylık olarak çekeceğimiz için frekansı 5 ve tarihi\r\nbaşında 1 olacak şekilde yazdık.\r\nSwap tutarını aşağıdaki gibi hesaplayabiliriz.\r\n\r\n\r\n# Birimler Milyon ABD Dolarıdır.\r\n\r\ndoviz_swap <- as.numeric(df_veri_3$`II.2. Yurt içi para karşılığında...`)\r\naltin_swap <- as.numeric(df_veri_3$`II.3. Diğer (Milyon ABD Doları)`)\r\n\r\nswap <- doviz_swap + altin_swap\r\n\r\n\r\n\r\nSwap tutarı = -58024 ya da -58,1 milyar dolar.\r\nSwap Hariç Net Rezervler\r\nSwap Hariç Net Rezervler = Net Rezervler - Swaplar\r\n\r\n\r\nrezerv <- net_rezerv + swap*1000 #birim farklılığı giderildi.\r\n\r\n\r\n\r\nNet rezervler (-) olduğu ve swaplar ile yükümlülüğümüz arttığı için\r\niki değeri topluyoruz.\r\nSwap hariç net rezervler -60271086 ya da -60,3 milyar dolardır.\r\nSon olarak, yazdıklarımızı bir fonksiyonda toplayalım ve güncel\r\nrezerv miktarımızı hesaplayalım.\r\n\r\n\r\nrezerv_hesapla <- function(api_key = NULL, haftalikTarih = NULL, aylikTarih = NULL){\r\n  \r\n  # Net Rezerv\r\n  \r\n  url_veri <- paste0(\"https://evds2.tcmb.gov.tr/service/evds/series=\",\r\n                     \"TP.AB.A02-TP.AB.A10\",\r\n                     \"&startDate=\",\r\n                     haftalikTarih,\r\n                     \"&endDate=\",\r\n                     haftalikTarih,\r\n                     \"frequency=3\",\r\n                     \"&type=json&key=\",\r\n                     api_key)\r\n  \r\n  df_veri <- fromJSON(url_veri) %>% \r\n    as.data.frame() %>% \r\n    select(2,3,4) %>% \r\n    rename(\r\n      \"Tarih\"=1,\r\n      \"A.1-DIŞ VARLIKLAR(Bin TL)\"=2,\r\n      \"P.1-TOPLAM DÖVİZ YÜKÜMLÜLÜKLERİ(Bin TL)\"=3\r\n    )\r\n  \r\n  url_veri_2 <- paste0(\"https://evds2.tcmb.gov.tr/service/evds/series=\",\r\n                       \"TP.DK.USD.A.YTL\",\r\n                       \"&startDate=\",\r\n                       haftalikTarih,\r\n                       \"&endDate=\",\r\n                       haftalikTarih,\r\n                       \"&type=json&key=\",\r\n                       api_key)\r\n  \r\n  df_veri_2 <- fromJSON(url_veri_2) %>% \r\n    as.data.frame() %>% \r\n    select(2,3) %>% \r\n    rename(\r\n      \"Tarih\"=1,\r\n      \"(USD) ABD Doları (Döviz Alış)\"=2\r\n    )\r\n  \r\n  dis_varliklar <- as.numeric(df_veri$`A.1-DIŞ VARLIKLAR(Bin TL)`)\r\n  doviz_yukumluluk <- as.numeric(df_veri$`P.1-TOPLAM DÖVİZ YÜKÜMLÜLÜKLERİ(Bin TL)`)\r\n  kur <- as.numeric(df_veri_2$`(USD) ABD Doları (Döviz Alış)`)\r\n  \r\n  net_rezerv <- (dis_varliklar - doviz_yukumluluk) / kur\r\n  \r\n  # Swap\r\n  \r\n  url_veri_3 <- paste0(\"https://evds2.tcmb.gov.tr/service/evds/series=\",\r\n                       \"TP.DOVVARNC.K14-TP.DOVVARNC.K23\",\r\n                       \"&startDate=\",\r\n                       aylikTarih,\r\n                       \"&endDate=\",\r\n                       aylikTarih,\r\n                       \"&frequency=5\",\r\n                       \"&type=json&key=\",\r\n                       api_key)\r\n  \r\n  df_veri_3 <- fromJSON(url_veri_3) %>% \r\n    as.data.frame() %>% \r\n    select(2,3,4) %>% \r\n    rename(\r\n      \"Tarih\"=1,\r\n      \"II.2. Yurt içi para karşılığında...\"=2,\r\n      \"II.3. Diğer (Milyon ABD Doları)\"=3\r\n    )\r\n  \r\n  doviz_swap <- as.numeric(df_veri_3$`II.2. Yurt içi para karşılığında...`)\r\n  altin_swap <- as.numeric(df_veri_3$`II.3. Diğer (Milyon ABD Doları)`)\r\n  \r\n  swap <- doviz_swap + altin_swap\r\n  \r\n  # Swap Hariç Net Rezerv\r\n  \r\n  rezerv <- net_rezerv + swap*1000\r\n  \r\n  return(rezerv)\r\n  \r\n}\r\n\r\nrezerv_hesapla(api_key = \"api_anahtariniz\",\r\n               haftalikTarih = \"28-04-2022\",\r\n               aylikTarih = \"01-03-2022\")\r\n\r\n\r\n\r\nFonksiyonun sonucu her ne kadar önceki örnek ile bir çıksa da yine de\r\ngünceli kontrol edelim.\r\nElimizdeki son haftalık veriler 28 Nisan 2022’ye; çekebildiğimiz swap\r\nverileri ise Mart 2022’ye ait.\r\nNet Rezervler = (1639818009 / 14.7962) - (1548217098 / 14.7962) =\r\n(1639818009 - 1548217098) / 14.7962 = 6190840 ya da 6,2 milyar dolar\r\nSwaplar = (-58373) + (-3768) = -62141 ya da -62,1 milyar dolar\r\nSwap Hariç Net Rezervler = 6,2 + (-62,1) = -55,9 milyar dolar\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-05-04-post2/img1.PNG",
    "last_modified": "2022-05-22T19:21:56+03:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-05-02-post1/",
    "title": "Event Study (Olay Çalışması): FED ve Gezi'nin Kısa Vadede Kura Etkisi",
    "description": "#GeziyiSavunacağız",
    "author": [
      {
        "name": "A. Uraz Akgül",
        "url": {}
      }
    ],
    "date": "2022-05-02",
    "categories": [
      "Finance"
    ],
    "contents": "\r\n\r\n\r\n\r\n\r\n\r\n\r\nGezi’nin 9. yıldönümü yaklaşırken ‘kurun Gezi nedeniyle arttığı’\r\nsözleri duyulmaya başlandı ki bu da devam edecektir. Ben de blogumun ilk\r\nyazısında bu konuya bilimsel bir bakış açısı katmak\r\nistedim.\r\nNormal şartlarda bu konuyu incelemek kolay olabilirdi ancak Gezi’nin\r\nbaşladığı 28 Mayıs gününden önce 22 Mayıs’ta FED’in açıklamaları\r\ngelişmekte olan ülke piyasaları için kritikti. Dönemin FED Başkanı Ben\r\nBernanke Merkez Bankası’nın tahvil alımlarını azaltabileceğini\r\nsöylemişti. Bu da parasal genişlemeden (Quantitative Easing, QE) çıkışın\r\nilk adımıydı.\r\nGezi’nin FED’in açıklamasına yakın bir tarihte başlaması işleri biraz\r\nzorlaştırıyor. Bu tip durumlarda Türkiye ile benzer ülkeleri de\r\nçalışmaya katmak faydalı olabilir. Bunu göz önüne alarak iki grup ülkeyi\r\nçalışmaya dahil ettim: Kırılgan Beşli (Fragile Five) ve Zordaki Onlu\r\n(Troubled Ten).\r\nKırılgan Beşli ülkeleri (Morgan Stanley, 2013): Brezilya, Endonezya,\r\nGüney Afrika, Hindistan ve Türkiye.\r\nZordaki Onlu ülkeleri (Morgan Stanley, 2015): Brezilya, Güney Afrika,\r\nGüney Kore, Kolombiya, Peru, Rusya, Singapur, Şili, Tayland ve\r\nTayvan.\r\nİki grubu birleştirirsek çalışmaya toplamda 13 ülkeyi dahil etmiş\r\nolacağız.\r\nTartışmalar dolar üzerinden ilerlediği için Türkiye de dahil 13\r\nülkenin para birimlerinin dolar karşısındaki değeri baz alındı. Değerler\r\nReuters’tan alınmıştır. Verilere (post1.xlsx) GitHub\r\nhesabımdan ulaşabilirsiniz.\r\n\r\n\r\nÜlke\r\n\r\n\r\nDöviz Kodu\r\n\r\n\r\nBrezilya\r\n\r\n\r\nUSDBRL\r\n\r\n\r\nEndonezya\r\n\r\n\r\nUSDIDR\r\n\r\n\r\nGüney Afrika\r\n\r\n\r\nUSDZAR\r\n\r\n\r\nGüney Kore\r\n\r\n\r\nUSDKRW\r\n\r\n\r\nHindistan\r\n\r\n\r\nUSDINR\r\n\r\n\r\nKolombiya\r\n\r\n\r\nUSDCOP\r\n\r\n\r\nPeru\r\n\r\n\r\nUSDPEN\r\n\r\n\r\nRusya\r\n\r\n\r\nUSDRUB\r\n\r\n\r\nSingapur\r\n\r\n\r\nUSDSGD\r\n\r\n\r\nŞili\r\n\r\n\r\nUSDCLP\r\n\r\n\r\nTayland\r\n\r\n\r\nUSDTHB\r\n\r\n\r\nTayvan\r\n\r\n\r\nUSDTWD\r\n\r\n\r\nTürkiye\r\n\r\n\r\nUSDTRY\r\n\r\n\r\nÖncelikle ülkelerin performanslarına bakalım. 2013 yılının başından\r\nsonuna kadar bir aralık belirledim ve her bir ülke kurunu 100 ile\r\nbaşlatarak bir endeks oluşturdum. Tüm kurların yer aldığı görsel\r\naşağıdaki gibidir.\r\n\r\n\r\n\r\nYukarıdaki grafiğe bir de ayrı ayrı bakalım.\r\n\r\n\r\n\r\n22 Mayıs 2013 tarihi baz alındığında genel resimde bir\r\nhareketlenmenin yaşandığını söyleyebiliriz ancak tam olarak yeterli ve\r\ngüvenilir bir yorum olmayacaktır. Bu noktada son bir görsel ile olay\r\nçalışmasına geçebiliriz. Aşağıdaki görselde 13 ülkenin endeks\r\nortalamasının USDTRY ile olan ilişkisi zaman serisi olarak\r\nverilmiştir.\r\n\r\n\r\n\r\nYukarıdaki görsel, yoruma diğerlerine göre biraz daha kolaylık\r\nkatıyor. Hareketlenmenin 22 Mayıs 2013 öncesinde başladığını ve diğer\r\nEM’ler ile korele bir şekilde hareket ettiğimizi görebiliriz. Kur ile\r\nendeks arasındaki farkı (USDTRY - Endeks > 0, pozitif bölge) ise\r\naşağıdaki gibi verebiliriz.\r\n\r\n\r\n\r\nYazının bundan sonraki kısmında Event Study’e geçebiliriz.\r\nOlay çalışması da diyebileceğimiz Event Study için kısaca herhangi\r\nbir olayın finansal piyasalar üzerine etkisini tespit etmek için\r\nkullanılan istatistiksel bir yöntemdir tanımını yapabiliriz. Literatüre\r\nbakıldığında yapılan çalışmaların hisse/borsa ağırlıklı olduğunu\r\ngörebiliriz. Ancak bu çalışmada yazının başında da belirttiğim nedenden\r\ndolayı ülke kurları üzerinden gitmeyi tercih ettim.\r\nOlay çalışmasını üç parçada inceliyoruz.\r\nEvent Studies in Economics and Finance,\r\nA. Craig MacKinlayEstimation Window denilen Tahmin Penceresi.\r\nTahmin penceresinde ortada henüz bir olay yoktur. Bu adımda USDTRY\r\ngetirilerinin piyasa (13 ülke kurundan hesaplanan endeks) getirisine\r\nkıyasla gerçekleşen normal davranışı belirlenir. Tahmin penceresinde\r\npiyasaya göre düzeltilmiş getiri (market adjusted return) yöntemini\r\nkullanacağız. Aslında bu nedenle tahmin penceresi bölümüne ihtiyacımız\r\nolmayacak (diğer bazı yöntemlerde olabiliyor).\r\n\\(AR_{it} = R_{it} - R_{mt}\\)\r\n\\(AR_{it}:\\) t gününe ait anormal\r\ngetiri (AR: Abnormal Return)\r\n\\(R_{it}:\\) t gününe ait USDTRY\r\ngetirisi\r\n\\(R_{mt}:\\) t gününe ait endeks\r\ngetirisi\r\nTahmin penceresi için bir başlangıç ve bir bitiş tarihi vardır.\r\nEvent Window denilen Olay Penceresi.\r\nOlay penceresi olaydan sonraki aralığı ifade ediyor. İlgili olayın\r\nolduktan sonra kaç gün daha USDTRY üzerinde etkisinin olduğu\r\ngösterilir.\r\nOlay penceresinde tıpkı tahmin penceresinde olduğu gibi bir başlangıç\r\nve bir de bitiş tarihi olmak ile beraber bir de olay günü vardır.\r\nPostevent Window denilen Olay Sonrası Pencere.\r\nOlay sonrası pencerede ise USDTRY’nin uzun vadeli performansı\r\ngösterilir.\r\nOlay sonrası pencerede de diğer iki pencerede olduğu gibi bir\r\nbaşlangıç ve bir de bitiş tarihi vardır.\r\nOlay çalışması ile FED ve Gezi’nin herhangi bir anormal getiriye yol\r\naçıp açmadığı inceleyeceğiz.\r\nAdım adım neler yapacağız?\r\nUSDTRY ve EM endeksinin getirilerini hesaplayarak başlıyoruz. Bunun\r\niçin logaritmik getiriyi kullanacağız.\r\n\r\n\r\n\r\n\r\n\r\n\r\nUSDTRY getirilerinden hesaplanan EM endeks getirilerini çıkarıyoruz\r\nve anormal getirileri (abnormal return) elde ediyoruz.\r\n\r\n\r\n\r\nBir önceki adımda hesaplanan anormal getirilerden kümülatif anormal\r\ngetirileri (cumulative abnormal return) hesaplıyoruz. Anormal getiri\r\ndeğerlerini günlük olarak hesapladık ve bu da piyasanın tepkisini tek\r\nbir gün için ölçer. Uzun dönemdeki olay etkisine bakabilmek için olay\r\npenceresi süresindeki anormal getiriler toplanarak kümülatif anormal\r\ngetiri değerleri hesaplanır. Bu da piyasanın olay penceresi dönemindeki\r\ngenel fiyat eğilimini gösterir.\r\n\r\n\r\n\r\nÇalışmada, FED için olay penceresi, -5 gün olay öncesi, +5 gün\r\nolay sonrası ve olay günü olmak üzere 11 gün olarak; Gezi için (Gezi’nin\r\n31 Mayıs’ta aktif olduğunu göz önüne alırsak) olay penceresi, -5 gün\r\nolay öncesi, +5 gün olay sonrası ve olay günü olmak üzere 11 gün olarak\r\nbelirlenmiştir.\r\nHesapladığımız kümülatif anormal getiri değerleri istatistiksel\r\nolarak sıfırdan farklı ise ilgili olayın değişkenler üzerinde etkili\r\nolduğunu ve anormal değişimi ortaya çıkardığını söyleyeceğiz. Ancak,\r\nelde edilen kümülatif anormal getiri 0’a eşit ya da 0’a çok yakın\r\ndeğerler alıyorsa ilgili olayın değişkenler üzerinde etkili olmadığı\r\nifade edebileceğiz.\r\nTeste karar vermek için öncelikle getirilerin normal dağılıp\r\ndağılmadığını inceleyeceğiz. Normallik varsayımının kontrolü için\r\nShapiro-Wilk normallik testi kullanılmıştır. Çünkü örnek büyüklüğünün\r\n50’den küçük olması durumunda Shapiro-Wilk; büyük olması durumunda ise\r\nKolmogorov Smirnov kullanılır gibi bir kullanım algoritması vardır.\r\nOlay penceresi FED(-5,+5) için;\r\n\\(H_0:\\) Olay penceresi içerisinde,\r\nolay gününden 5 gün öncesi ve 5 gün sonrası kümülatif aşırı getiriler\r\nnormal dağılım göstermektedir.\r\n\\(H_1:\\) Olay penceresi içerisinde,\r\nolay gününden 5 gün öncesi ve 5 gün sonrası kümülatif aşırı getiriler\r\nnormal dağılım göstermemektedir.\r\n\r\n\r\n    Shapiro-Wilk normality test\r\n\r\ndata:  obs_fed\r\nW = 0.89606, p-value = 0.1654\r\n\r\nOlay penceresi Gezi(-5,+5) için;\r\n\\(H_0:\\) Olay penceresi içerisinde,\r\nolay gününden 5 gün öncesi ve 5 gün sonrası kümülatif aşırı getiriler\r\nnormal dağılım göstermektedir.\r\n\\(H_1:\\) Olay penceresi içerisinde,\r\nolay gününden 5 gün öncesi ve 5 gün sonrası kümülatif aşırı getiriler\r\nnormal dağılım göstermemektedir.\r\n\r\n\r\n    Shapiro-Wilk normality test\r\n\r\ndata:  obs_gezi\r\nW = 0.85286, p-value = 0.0465\r\n\r\nShapiro-Wilk testine göre p değerleri sırasıyla 0.1653741 ve 0.046503\r\n%5’ten büyük çıktığı için (ikincisi yaklaşık %5) normallik varsayımının\r\nsağlandığını söyleyebiliriz. Bu durumda parametrik olmayan testler\r\ngrubundaki bağımlı örneklem t testi (Paired Samples T-test) ile bir\r\ngrubun belirli bir olaydan önceki ve sonraki ortalamaları arasındaki\r\nfarkın anlamlılığı test edilebilir.\r\nOlay penceresi FED(-5,+5) için;\r\n\\(H_{0,(\\pm5)}: CAR_{-5} =\r\nCAR_{+5}\\)\r\nOlay günü tarihinden önceki 5 günlük kümülatif ortalama anormal\r\ngetirileriyle olay gününden sonraki 5 günlük kümülatif anormal\r\ngetirileri arasında fark yoktur.\r\n\\(H_{1,(\\pm5)}: CAR_{-5} \\neq\r\nCAR_{+5}\\)\r\nOlay günü tarihinden önceki 5 günlük kümülatif ortalama anormal\r\ngetirileriyle olay gününden sonraki 5 günlük kümülatif anormal\r\ngetirileri arasında fark vardır.\r\n\r\n\r\n    Paired t-test\r\n\r\ndata:  before_fed and after_fed\r\nt = 1.5718, df = 4, p-value = 0.1911\r\nalternative hypothesis: true difference in means is not equal to 0\r\n95 percent confidence interval:\r\n -0.002125328  0.007671368\r\nsample estimates:\r\nmean of the differences \r\n             0.00277302 \r\n\r\nOlay penceresi Gezi(-5,+5) için;\r\n\\(H_{0,(\\pm5)}: CAR_{-5} =\r\nCAR_{+5}\\)\r\nOlay günü tarihinden önceki 5 günlük kümülatif ortalama anormal\r\ngetirileriyle olay gününden sonraki 5 günlük kümülatif anormal\r\ngetirileri arasında fark yoktur.\r\n\\(H_{1,(\\pm5)}: CAR_{-5} \\neq\r\nCAR_{+5}\\)\r\nOlay günü tarihinden önceki 5 günlük kümülatif ortalama anormal\r\ngetirileriyle olay gününden sonraki 5 günlük kümülatif anormal\r\ngetirileri arasında fark vardır.\r\n\r\n\r\n    Paired t-test\r\n\r\ndata:  before_gezi and after_gezi\r\nt = -2.2407, df = 4, p-value = 0.08855\r\nalternative hypothesis: true difference in means is not equal to 0\r\n95 percent confidence interval:\r\n -0.012374046  0.001321203\r\nsample estimates:\r\nmean of the differences \r\n           -0.005526422 \r\n\r\np değerleri olay penceresi FED(-5,+5) için 0.1911003 ve Gezi(-5,+5)\r\niçin 0.0885484 %5’ten büyük olduğu için sıfır hipotezi reddedilemez; bu\r\nda olay öncesi ile sonrası ortalama getirilerinde fark olmadığını\r\ngösterir.\r\nOrtalama getiriler FED(-5,+5) için olay öncesi 0.0039657 iken; olay\r\nsonrası 0.0011927; Gezi(-5,+5) için olay öncesi -0.0013321 iken; olay\r\nsonrası 0.0041943 olmuştur.\r\nBir de kümülatif anormal getirilerin istatistiksel olarak anlamlı\r\nolup olmadıklarına bireysel olarak bakalım. Bu durumda t istatistiği\r\n\\(\\frac{CAR}{\\sigma(CAR)*\\sqrt{Gün\\\r\nSayısı}}\\) olacak.\r\nUSING DAILY STOCK RETURNS The Case of\r\nEvent Studies*, S.J.Brown, J.B.Warner\r\n\r\n\r\n\r\n\r\n\r\nFED için olay öncesinde 3 kümülatif anormal getiri istatistiksel\r\nolarak anlamlı çıkarken; Gezi için olay sonrasında 2 kümülatif anormal\r\ngetiri istatistiksel olarak anlamlı çıkmıştır.\r\nOlay penceresi için FED(-5,+5) ve Gezi(-5,+5) demiştik. Bunların\r\nistatistiksel anlamlılığına bakalım.\r\n\r\n\r\n\r\nOlay penceresinde FED(,-5,+5) ve Gezi(-5,+5) için |-0.0389315| >\r\n1.96 ve |-0.1593087| > 1.96 olmadığı için istatistiksel olarak\r\nanlamlı çıkmamıştır.\r\nSonuç olarak şunları söyleyebilirim: Bu çalışma, FED ve Gezi’nin kısa\r\nvadeli etkilerini göstermektedir. Kısa vadede her iki olayda da\r\nistatistiksel olarak anlamlı bir etki olmasa da Türkiye’nin diğer EM’ler\r\nile 22 Mayıs 2013’ten önce başlayarak korele bir şekilde hareket ettiği\r\ndikkate alınmalıdır. Belirlenen olay günlerinin farklı olay\r\npencerelerinde de değerlendirilmesini tavsiye ederim. Ayrıca tercih\r\nettiğim market adjusted returns dışında farklı yöntemlerin de\r\nkullanıldığını hatırlatmak isterim. Daha uzun vadedeki etkilere bakmak\r\niçin farklı istatistiksel yöntemlerin uygulanması yazının başında\r\nbelirtilen tartışmaya farklı bir açı katacaktır.\r\nÇalışmanın R kodlarına aşağıdan ulaşılabilir.\r\n\r\n\r\noptions(scipen = 999)\r\nlibrary(tidyverse)\r\n\r\ndf <- readxl::read_excel(\"data.xlsx\")\r\n\r\nmaster <- df %>% \r\n  mutate(Date = as.Date(Date)) %>% \r\n  filter(Date >= as.Date(\"2013-01-01\") & Date <= as.Date(\"2013-12-31\"))\r\n\r\nemergingMarket <- master %>% \r\n  pivot_longer(!Date, names_to = \"Currencies\", values_to = \"Values\") %>% \r\n  arrange(Currencies)\r\n\r\ninitialValues <- emergingMarket %>% \r\n  group_by(Currencies) %>% \r\n  slice_min(Date) %>% \r\n  rename(\"Initial\"=3)\r\n\r\nemergingMarketFinal <- emergingMarket %>% \r\n  left_join(initialValues, by = c(\"Date\",\"Currencies\")) %>% \r\n  mutate(Initial = zoo::na.locf(Initial)) %>% \r\n  mutate(\"SubIndex\" = Values / Initial * 100) %>% \r\n  mutate(\"ColGr\" = ifelse(Currencies == \"USDTRY\", \"USDTRY\", \"Diğer EM'ler\"))\r\n\r\nggplot(emergingMarketFinal,\r\n       aes(x = Date, y = SubIndex, group = Currencies, color = ColGr)) +\r\n  geom_line() +\r\n  geom_hline(yintercept = 100) +\r\n  geom_vline(xintercept = as.Date(\"2013-05-22\"), linetype = \"dashed\") +\r\n  theme_minimal() +\r\n  theme(legend.title = element_blank(),\r\n        legend.position = \"bottom\",\r\n        axis.title = element_blank(),\r\n        plot.caption = element_text(face = \"italic\")) +\r\n  scale_color_manual(values = c(\"gray80\",\"red\")) +\r\n  labs(caption = \"EM: Emerging Market\")\r\n\r\nggplot(emergingMarketFinal,\r\n       aes(x = Date, y = SubIndex, group = Currencies, color = ColGr)) +\r\n  geom_line() +\r\n  geom_hline(yintercept = 100) +\r\n  geom_vline(xintercept = as.Date(\"2013-05-22\"), linetype = \"dashed\") +\r\n  theme_minimal() +\r\n  theme(legend.title = element_blank(),\r\n        legend.position = \"none\",\r\n        axis.title = element_blank(),\r\n        strip.text = element_text(size = 20)) +\r\n  scale_color_manual(values = c(\"gray30\",\"red\")) +\r\n  scale_x_date(date_labels = \"%m-%Y\") +\r\n  facet_wrap(~Currencies, ncol = 3)\r\n\r\nemIndex <- emergingMarketFinal %>% \r\n  select(Date,Currencies,SubIndex) %>% \r\n  pivot_wider(names_from = \"Currencies\", values_from = \"SubIndex\") %>% \r\n  mutate(\"EMIndex\" = rowMeans(.[,-1], na.rm = TRUE))\r\n\r\nemIndex %>% \r\n  select(Date,USDTRY,EMIndex) %>% \r\n  pivot_longer(!Date, names_to = \"Currencies\", values_to = \"Index\") %>% \r\n  ggplot(aes(x = Date, y = Index, group = Currencies, color = Currencies)) +\r\n  geom_line() +\r\n  geom_hline(yintercept = 100) +\r\n  geom_vline(xintercept = as.Date(\"2013-05-22\"), linetype = \"dashed\") +\r\n  theme_minimal() +\r\n  theme(axis.title = element_blank(),\r\n        legend.title = element_blank(),\r\n        legend.position = \"bottom\",\r\n        plot.caption = element_text(face = \"italic\")) +\r\n  scale_color_manual(values = c(\"gray30\",\"red\")) +\r\n  labs(caption = \"EMIndex: 13 Emerging Market'ın Ortalaması\")\r\n\r\nemIndex %>% \r\n  select(Date,USDTRY,EMIndex) %>% \r\n  mutate(Diff = USDTRY - EMIndex) %>% \r\n  ggplot(aes(x = Date, y = Diff)) +\r\n  geom_area(fill = \"gray40\") +\r\n  geom_vline(xintercept = as.Date(\"2013-05-22\"), linetype = \"dashed\") +\r\n  geom_hline(yintercept = 0) +\r\n  theme_minimal() +\r\n  theme(axis.title = element_blank())\r\n\r\ndf_event <- emIndex %>% \r\n  select(Date,USDTRY,EMIndex) %>% \r\n  mutate(\r\n    \"USDTRY Return\" = lag(log(lead(USDTRY)/USDTRY)),\r\n    \"EMIndex Return\" = lag(log(lead(EMIndex)/EMIndex))\r\n  ) %>% \r\n  na.omit()\r\n\r\ndf_event %>% \r\n  select(Date,`USDTRY Return`,`EMIndex Return`) %>% \r\n  pivot_longer(!Date, names_to = \"Vars\", values_to = \"Return\") %>% \r\n  ggplot(aes(x = Date, y = Return, group = Vars, color = Vars)) +\r\n  geom_line() +\r\n  geom_vline(xintercept = as.Date(\"2013-05-22\"), linetype = \"dashed\") +\r\n  theme_minimal() +\r\n  theme(axis.title = element_blank(),\r\n        legend.position = \"none\") +\r\n  facet_wrap(~Vars, ncol = 1) +\r\n  scale_color_manual(values = c(\"blue\",\"red\"))\r\n\r\ndf_event <- df_event %>% \r\n  mutate(\"Abnormal Return\" = `USDTRY Return` - `EMIndex Return`)\r\n\r\n# -5,5 (FED için)\r\n\r\ndf_event_fed <- df_event %>% \r\n  filter(Date >= as.Date(\"2013-05-15\") & Date <= as.Date(\"2013-05-29\")) %>% \r\n  mutate(\"Cumulative Abnormal Return\" = cumsum(`Abnormal Return`))\r\n\r\n# -5,5 (Gezi için)\r\n\r\ndf_event_gezi <- df_event %>% \r\n  filter(Date >= as.Date(\"2013-05-24\") & Date <= as.Date(\"2013-06-07\")) %>% \r\n  mutate(\"Cumulative Abnormal Return\" = cumsum(`Abnormal Return`))\r\n\r\nobs_fed <- df_event_fed %>% \r\n  pull(`Cumulative Abnormal Return`)\r\n\r\nshapiro.test(obs_fed)\r\nshapiro.test(obs_fed)$p.value\r\n\r\nobs_gezi <- df_event_gezi %>% \r\n  pull(`Cumulative Abnormal Return`)\r\n\r\nshapiro.test(obs_gezi)\r\nshapiro.test(obs_gezi)$p.value\r\n\r\nbefore_fed <- df_event_fed %>% \r\n  filter(Date < as.Date(\"2013-05-22\")) %>% \r\n  pull(`Cumulative Abnormal Return`)\r\n\r\nafter_fed <- df_event_fed %>% \r\n  filter(Date > as.Date(\"2013-05-22\")) %>% \r\n  pull(`Cumulative Abnormal Return`)\r\n\r\nt.test(before_fed, after_fed, paired = TRUE)\r\nt.test(before_fed, after_fed, paired = TRUE)$p.value\r\n\r\nbefore_gezi <- df_event_gezi %>% \r\n  filter(Date < as.Date(\"2013-05-31\")) %>% \r\n  pull(`Cumulative Abnormal Return`)\r\n\r\nafter_gezi <- df_event_gezi %>% \r\n  filter(Date > as.Date(\"2013-05-31\")) %>% \r\n  pull(`Cumulative Abnormal Return`)\r\n\r\nt.test(before_gezi, after_gezi, paired = TRUE)\r\nt.test(before_gezi, after_gezi, paired = TRUE)$p.value\r\n\r\nmean(before_fed)\r\nmean(after_fed)\r\n\r\nmean(before_gezi)\r\nmean(after_gezi)\r\n\r\nsigma_fed <- df_event_fed %>% \r\n  summarise(sd = sd(`Abnormal Return`)) %>% \r\n  pull(sd)\r\n\r\ndf_event_fed <- df_event_fed %>% \r\n  mutate(t = seq(-5,5,1)) %>% \r\n  mutate(\"t_stat\" = `Cumulative Abnormal Return`/sigma_fed*sqrt(abs(t))) %>% \r\n  mutate(\"Result\" = ifelse(abs(t_stat) > 1.96, \"SIGNIFICANT\",\"INSIGNIFICANT\"))\r\n\r\ndf_event_fed %>% \r\n  ggplot(aes(x = factor(t), y = `Cumulative Abnormal Return`, group = 1)) +\r\n  geom_line() +\r\n  geom_point(aes(color = Result), size = 7) +\r\n  geom_vline(xintercept = factor(0)) +\r\n  geom_hline(yintercept = 0) +\r\n  theme_minimal() +\r\n  theme(axis.title = element_blank(),\r\n        legend.title = element_blank(),\r\n        plot.title = element_text(size = 20),\r\n        plot.subtitle = element_text(size = 15)) +\r\n  scale_color_manual(values = c(\"gray40\",\"red\")) +\r\n  labs(title = \"FED\",\r\n       subtitle = \"t0: 22 Mayıs 2013\")\r\n\r\nsigma_gezi <- df_event_gezi %>% \r\n  summarise(sd = sd(`Abnormal Return`)) %>% \r\n  pull(sd)\r\n\r\ndf_event_gezi <- df_event_gezi %>% \r\n  mutate(t = seq(-5,5,1)) %>% \r\n  mutate(\"t_stat\" = `Cumulative Abnormal Return`/sigma_gezi*sqrt(abs(t))) %>% \r\n  mutate(\"Result\" = ifelse(abs(t_stat) > 1.96, \"SIGNIFICANT\",\"INSIGNIFICANT\"))\r\n\r\ndf_event_gezi %>% \r\n  ggplot(aes(x = factor(t), y = `Cumulative Abnormal Return`, group = 1)) +\r\n  geom_line() +\r\n  geom_point(aes(color = Result), size = 7) +\r\n  geom_vline(xintercept = factor(0)) +\r\n  geom_hline(yintercept = 0) +\r\n  theme_minimal() +\r\n  theme(axis.title = element_blank(),\r\n        legend.title = element_blank(),\r\n        plot.title = element_text(size = 20),\r\n        plot.subtitle = element_text(size = 15)) +\r\n  scale_color_manual(values = c(\"gray40\",\"red\")) +\r\n  labs(title = \"GEZİ\",\r\n       subtitle = \"t0: 31 Mayıs 2013\")\r\n\r\new_sum_fed <- sum(df_event_fed$`Abnormal Return`)\r\new_tstat_fed <- ew_sum_fed / (sigma_fed * sqrt(11))\r\n\r\new_sum_gezi <- sum(df_event_gezi$`Abnormal Return`)\r\new_tstat_gezi <- ew_sum_gezi / (sigma_gezi * sqrt(11))\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-05-02-post1/post1_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2022-05-22T19:22:32+03:00",
    "input_file": {}
  }
]
